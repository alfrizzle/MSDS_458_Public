<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />

<title>MSDS 458 Assignment 1 (Experiment 3) April 11</title>

<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>



<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.7.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.7.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.7.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0') format('woff2'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.7.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.7.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.7.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.fa-pull-left {
  float: left;
}
.fa-pull-right {
  float: right;
}
.fa.fa-pull-left {
  margin-right: .3em;
}
.fa.fa-pull-right {
  margin-left: .3em;
}
/* Deprecated as of 4.4.0 */
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
.fa-pulse {
  -webkit-animation: fa-spin 1s infinite steps(8);
  animation: fa-spin 1s infinite steps(8);
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=1)";
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2)";
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=3)";
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1)";
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  -ms-filter: "progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1)";
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook-f:before,
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-feed:before,
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before,
.fa-gratipay:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper-pp:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-resistance:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-y-combinator-square:before,
.fa-yc-square:before,
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
.fa-buysellads:before {
  content: "\f20d";
}
.fa-connectdevelop:before {
  content: "\f20e";
}
.fa-dashcube:before {
  content: "\f210";
}
.fa-forumbee:before {
  content: "\f211";
}
.fa-leanpub:before {
  content: "\f212";
}
.fa-sellsy:before {
  content: "\f213";
}
.fa-shirtsinbulk:before {
  content: "\f214";
}
.fa-simplybuilt:before {
  content: "\f215";
}
.fa-skyatlas:before {
  content: "\f216";
}
.fa-cart-plus:before {
  content: "\f217";
}
.fa-cart-arrow-down:before {
  content: "\f218";
}
.fa-diamond:before {
  content: "\f219";
}
.fa-ship:before {
  content: "\f21a";
}
.fa-user-secret:before {
  content: "\f21b";
}
.fa-motorcycle:before {
  content: "\f21c";
}
.fa-street-view:before {
  content: "\f21d";
}
.fa-heartbeat:before {
  content: "\f21e";
}
.fa-venus:before {
  content: "\f221";
}
.fa-mars:before {
  content: "\f222";
}
.fa-mercury:before {
  content: "\f223";
}
.fa-intersex:before,
.fa-transgender:before {
  content: "\f224";
}
.fa-transgender-alt:before {
  content: "\f225";
}
.fa-venus-double:before {
  content: "\f226";
}
.fa-mars-double:before {
  content: "\f227";
}
.fa-venus-mars:before {
  content: "\f228";
}
.fa-mars-stroke:before {
  content: "\f229";
}
.fa-mars-stroke-v:before {
  content: "\f22a";
}
.fa-mars-stroke-h:before {
  content: "\f22b";
}
.fa-neuter:before {
  content: "\f22c";
}
.fa-genderless:before {
  content: "\f22d";
}
.fa-facebook-official:before {
  content: "\f230";
}
.fa-pinterest-p:before {
  content: "\f231";
}
.fa-whatsapp:before {
  content: "\f232";
}
.fa-server:before {
  content: "\f233";
}
.fa-user-plus:before {
  content: "\f234";
}
.fa-user-times:before {
  content: "\f235";
}
.fa-hotel:before,
.fa-bed:before {
  content: "\f236";
}
.fa-viacoin:before {
  content: "\f237";
}
.fa-train:before {
  content: "\f238";
}
.fa-subway:before {
  content: "\f239";
}
.fa-medium:before {
  content: "\f23a";
}
.fa-yc:before,
.fa-y-combinator:before {
  content: "\f23b";
}
.fa-optin-monster:before {
  content: "\f23c";
}
.fa-opencart:before {
  content: "\f23d";
}
.fa-expeditedssl:before {
  content: "\f23e";
}
.fa-battery-4:before,
.fa-battery:before,
.fa-battery-full:before {
  content: "\f240";
}
.fa-battery-3:before,
.fa-battery-three-quarters:before {
  content: "\f241";
}
.fa-battery-2:before,
.fa-battery-half:before {
  content: "\f242";
}
.fa-battery-1:before,
.fa-battery-quarter:before {
  content: "\f243";
}
.fa-battery-0:before,
.fa-battery-empty:before {
  content: "\f244";
}
.fa-mouse-pointer:before {
  content: "\f245";
}
.fa-i-cursor:before {
  content: "\f246";
}
.fa-object-group:before {
  content: "\f247";
}
.fa-object-ungroup:before {
  content: "\f248";
}
.fa-sticky-note:before {
  content: "\f249";
}
.fa-sticky-note-o:before {
  content: "\f24a";
}
.fa-cc-jcb:before {
  content: "\f24b";
}
.fa-cc-diners-club:before {
  content: "\f24c";
}
.fa-clone:before {
  content: "\f24d";
}
.fa-balance-scale:before {
  content: "\f24e";
}
.fa-hourglass-o:before {
  content: "\f250";
}
.fa-hourglass-1:before,
.fa-hourglass-start:before {
  content: "\f251";
}
.fa-hourglass-2:before,
.fa-hourglass-half:before {
  content: "\f252";
}
.fa-hourglass-3:before,
.fa-hourglass-end:before {
  content: "\f253";
}
.fa-hourglass:before {
  content: "\f254";
}
.fa-hand-grab-o:before,
.fa-hand-rock-o:before {
  content: "\f255";
}
.fa-hand-stop-o:before,
.fa-hand-paper-o:before {
  content: "\f256";
}
.fa-hand-scissors-o:before {
  content: "\f257";
}
.fa-hand-lizard-o:before {
  content: "\f258";
}
.fa-hand-spock-o:before {
  content: "\f259";
}
.fa-hand-pointer-o:before {
  content: "\f25a";
}
.fa-hand-peace-o:before {
  content: "\f25b";
}
.fa-trademark:before {
  content: "\f25c";
}
.fa-registered:before {
  content: "\f25d";
}
.fa-creative-commons:before {
  content: "\f25e";
}
.fa-gg:before {
  content: "\f260";
}
.fa-gg-circle:before {
  content: "\f261";
}
.fa-tripadvisor:before {
  content: "\f262";
}
.fa-odnoklassniki:before {
  content: "\f263";
}
.fa-odnoklassniki-square:before {
  content: "\f264";
}
.fa-get-pocket:before {
  content: "\f265";
}
.fa-wikipedia-w:before {
  content: "\f266";
}
.fa-safari:before {
  content: "\f267";
}
.fa-chrome:before {
  content: "\f268";
}
.fa-firefox:before {
  content: "\f269";
}
.fa-opera:before {
  content: "\f26a";
}
.fa-internet-explorer:before {
  content: "\f26b";
}
.fa-tv:before,
.fa-television:before {
  content: "\f26c";
}
.fa-contao:before {
  content: "\f26d";
}
.fa-500px:before {
  content: "\f26e";
}
.fa-amazon:before {
  content: "\f270";
}
.fa-calendar-plus-o:before {
  content: "\f271";
}
.fa-calendar-minus-o:before {
  content: "\f272";
}
.fa-calendar-times-o:before {
  content: "\f273";
}
.fa-calendar-check-o:before {
  content: "\f274";
}
.fa-industry:before {
  content: "\f275";
}
.fa-map-pin:before {
  content: "\f276";
}
.fa-map-signs:before {
  content: "\f277";
}
.fa-map-o:before {
  content: "\f278";
}
.fa-map:before {
  content: "\f279";
}
.fa-commenting:before {
  content: "\f27a";
}
.fa-commenting-o:before {
  content: "\f27b";
}
.fa-houzz:before {
  content: "\f27c";
}
.fa-vimeo:before {
  content: "\f27d";
}
.fa-black-tie:before {
  content: "\f27e";
}
.fa-fonticons:before {
  content: "\f280";
}
.fa-reddit-alien:before {
  content: "\f281";
}
.fa-edge:before {
  content: "\f282";
}
.fa-credit-card-alt:before {
  content: "\f283";
}
.fa-codiepie:before {
  content: "\f284";
}
.fa-modx:before {
  content: "\f285";
}
.fa-fort-awesome:before {
  content: "\f286";
}
.fa-usb:before {
  content: "\f287";
}
.fa-product-hunt:before {
  content: "\f288";
}
.fa-mixcloud:before {
  content: "\f289";
}
.fa-scribd:before {
  content: "\f28a";
}
.fa-pause-circle:before {
  content: "\f28b";
}
.fa-pause-circle-o:before {
  content: "\f28c";
}
.fa-stop-circle:before {
  content: "\f28d";
}
.fa-stop-circle-o:before {
  content: "\f28e";
}
.fa-shopping-bag:before {
  content: "\f290";
}
.fa-shopping-basket:before {
  content: "\f291";
}
.fa-hashtag:before {
  content: "\f292";
}
.fa-bluetooth:before {
  content: "\f293";
}
.fa-bluetooth-b:before {
  content: "\f294";
}
.fa-percent:before {
  content: "\f295";
}
.fa-gitlab:before {
  content: "\f296";
}
.fa-wpbeginner:before {
  content: "\f297";
}
.fa-wpforms:before {
  content: "\f298";
}
.fa-envira:before {
  content: "\f299";
}
.fa-universal-access:before {
  content: "\f29a";
}
.fa-wheelchair-alt:before {
  content: "\f29b";
}
.fa-question-circle-o:before {
  content: "\f29c";
}
.fa-blind:before {
  content: "\f29d";
}
.fa-audio-description:before {
  content: "\f29e";
}
.fa-volume-control-phone:before {
  content: "\f2a0";
}
.fa-braille:before {
  content: "\f2a1";
}
.fa-assistive-listening-systems:before {
  content: "\f2a2";
}
.fa-asl-interpreting:before,
.fa-american-sign-language-interpreting:before {
  content: "\f2a3";
}
.fa-deafness:before,
.fa-hard-of-hearing:before,
.fa-deaf:before {
  content: "\f2a4";
}
.fa-glide:before {
  content: "\f2a5";
}
.fa-glide-g:before {
  content: "\f2a6";
}
.fa-signing:before,
.fa-sign-language:before {
  content: "\f2a7";
}
.fa-low-vision:before {
  content: "\f2a8";
}
.fa-viadeo:before {
  content: "\f2a9";
}
.fa-viadeo-square:before {
  content: "\f2aa";
}
.fa-snapchat:before {
  content: "\f2ab";
}
.fa-snapchat-ghost:before {
  content: "\f2ac";
}
.fa-snapchat-square:before {
  content: "\f2ad";
}
.fa-pied-piper:before {
  content: "\f2ae";
}
.fa-first-order:before {
  content: "\f2b0";
}
.fa-yoast:before {
  content: "\f2b1";
}
.fa-themeisle:before {
  content: "\f2b2";
}
.fa-google-plus-circle:before,
.fa-google-plus-official:before {
  content: "\f2b3";
}
.fa-fa:before,
.fa-font-awesome:before {
  content: "\f2b4";
}
.fa-handshake-o:before {
  content: "\f2b5";
}
.fa-envelope-open:before {
  content: "\f2b6";
}
.fa-envelope-open-o:before {
  content: "\f2b7";
}
.fa-linode:before {
  content: "\f2b8";
}
.fa-address-book:before {
  content: "\f2b9";
}
.fa-address-book-o:before {
  content: "\f2ba";
}
.fa-vcard:before,
.fa-address-card:before {
  content: "\f2bb";
}
.fa-vcard-o:before,
.fa-address-card-o:before {
  content: "\f2bc";
}
.fa-user-circle:before {
  content: "\f2bd";
}
.fa-user-circle-o:before {
  content: "\f2be";
}
.fa-user-o:before {
  content: "\f2c0";
}
.fa-id-badge:before {
  content: "\f2c1";
}
.fa-drivers-license:before,
.fa-id-card:before {
  content: "\f2c2";
}
.fa-drivers-license-o:before,
.fa-id-card-o:before {
  content: "\f2c3";
}
.fa-quora:before {
  content: "\f2c4";
}
.fa-free-code-camp:before {
  content: "\f2c5";
}
.fa-telegram:before {
  content: "\f2c6";
}
.fa-thermometer-4:before,
.fa-thermometer:before,
.fa-thermometer-full:before {
  content: "\f2c7";
}
.fa-thermometer-3:before,
.fa-thermometer-three-quarters:before {
  content: "\f2c8";
}
.fa-thermometer-2:before,
.fa-thermometer-half:before {
  content: "\f2c9";
}
.fa-thermometer-1:before,
.fa-thermometer-quarter:before {
  content: "\f2ca";
}
.fa-thermometer-0:before,
.fa-thermometer-empty:before {
  content: "\f2cb";
}
.fa-shower:before {
  content: "\f2cc";
}
.fa-bathtub:before,
.fa-s15:before,
.fa-bath:before {
  content: "\f2cd";
}
.fa-podcast:before {
  content: "\f2ce";
}
.fa-window-maximize:before {
  content: "\f2d0";
}
.fa-window-minimize:before {
  content: "\f2d1";
}
.fa-window-restore:before {
  content: "\f2d2";
}
.fa-times-rectangle:before,
.fa-window-close:before {
  content: "\f2d3";
}
.fa-times-rectangle-o:before,
.fa-window-close-o:before {
  content: "\f2d4";
}
.fa-bandcamp:before {
  content: "\f2d5";
}
.fa-grav:before {
  content: "\f2d6";
}
.fa-etsy:before {
  content: "\f2d7";
}
.fa-imdb:before {
  content: "\f2d8";
}
.fa-ravelry:before {
  content: "\f2d9";
}
.fa-eercast:before {
  content: "\f2da";
}
.fa-microchip:before {
  content: "\f2db";
}
.fa-snowflake-o:before {
  content: "\f2dc";
}
.fa-superpowers:before {
  content: "\f2dd";
}
.fa-wpexplorer:before {
  content: "\f2de";
}
.fa-meetup:before {
  content: "\f2e0";
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
div.traceback-wrapper pre.traceback {
  max-height: 600px;
  overflow: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  display: flex;
  flex-direction: row;
  justify-content: space-between;
  padding: 5px;
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
[dir="rtl"] #ipython_notebook {
  margin-right: 10px;
  margin-left: 0;
}
[dir="rtl"] #ipython_notebook.pull-left {
  float: right !important;
  float: right;
}
.flex-spacer {
  flex: 1;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#kernel_logo_widget {
  margin: 0 10px;
}
span#login_widget {
  float: right;
}
[dir="rtl"] span#login_widget {
  float: left;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
.modal-header {
  cursor: move;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
[dir="rtl"] .center-nav form.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] .center-nav .navbar-text {
  float: right;
}
[dir="rtl"] .navbar-inner {
  text-align: right;
}
[dir="rtl"] div.text-left {
  text-align: right;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  position: absolute;
  display: block;
  width: 100%;
  height: 100%;
  overflow: hidden;
  cursor: pointer;
  opacity: 0;
  z-index: 2;
}
.alternate_upload .btn-xs > input.fileinput {
  margin: -1px -5px;
}
.alternate_upload .btn-upload {
  position: relative;
  height: 22px;
}
::-webkit-file-upload-button {
  cursor: pointer;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
ul#tabs {
  margin-bottom: 4px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
[dir="rtl"] ul#tabs.nav-tabs > li {
  float: right;
}
[dir="rtl"] ul#tabs.nav.nav-tabs {
  padding-right: 0;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons .pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .list_toolbar .col-sm-4,
[dir="rtl"] .list_toolbar .col-sm-8 {
  float: right;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: text-bottom;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
[dir="rtl"] .list_item > div input {
  margin-right: 0;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_modified {
  margin-right: 7px;
  margin-left: 7px;
}
[dir="rtl"] .item_modified.pull-right {
  float: left !important;
  float: left;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
[dir="rtl"] .item_buttons.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .item_buttons .kernel-name {
  margin-left: 7px;
  float: right;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
.sort_button {
  display: inline-block;
  padding-left: 7px;
}
[dir="rtl"] .sort_button.pull-right {
  float: left !important;
  float: left;
}
#tree-selector {
  padding-right: 0px;
}
#button-select-all {
  min-width: 50px;
}
[dir="rtl"] #button-select-all.btn {
  float: right ;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
  margin-top: 2px;
  height: 16px;
}
[dir="rtl"] #select-all.pull-left {
  float: right !important;
  float: right;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.fa-pull-left {
  margin-right: .3em;
}
.folder_icon:before.fa-pull-right {
  margin-left: .3em;
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.fa-pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.fa-pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.fa-pull-left {
  margin-right: .3em;
}
.file_icon:before.fa-pull-right {
  margin-left: .3em;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
#new-menu .dropdown-header {
  font-size: 10px;
  border-bottom: 1px solid #e5e5e5;
  padding: 0 0 3px;
  margin: -3px 20px 0;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.move-button {
  display: none;
}
.download-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.fa-pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.fa-pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
.CodeMirror-dialog {
  background-color: #fff;
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI escape sequences */
/* The color values are a mix of
   http://www.xcolors.net/dl/baskerville-ivorylight and
   http://www.xcolors.net/dl/euphrasia */
.ansi-black-fg {
  color: #3E424D;
}
.ansi-black-bg {
  background-color: #3E424D;
}
.ansi-black-intense-fg {
  color: #282C36;
}
.ansi-black-intense-bg {
  background-color: #282C36;
}
.ansi-red-fg {
  color: #E75C58;
}
.ansi-red-bg {
  background-color: #E75C58;
}
.ansi-red-intense-fg {
  color: #B22B31;
}
.ansi-red-intense-bg {
  background-color: #B22B31;
}
.ansi-green-fg {
  color: #00A250;
}
.ansi-green-bg {
  background-color: #00A250;
}
.ansi-green-intense-fg {
  color: #007427;
}
.ansi-green-intense-bg {
  background-color: #007427;
}
.ansi-yellow-fg {
  color: #DDB62B;
}
.ansi-yellow-bg {
  background-color: #DDB62B;
}
.ansi-yellow-intense-fg {
  color: #B27D12;
}
.ansi-yellow-intense-bg {
  background-color: #B27D12;
}
.ansi-blue-fg {
  color: #208FFB;
}
.ansi-blue-bg {
  background-color: #208FFB;
}
.ansi-blue-intense-fg {
  color: #0065CA;
}
.ansi-blue-intense-bg {
  background-color: #0065CA;
}
.ansi-magenta-fg {
  color: #D160C4;
}
.ansi-magenta-bg {
  background-color: #D160C4;
}
.ansi-magenta-intense-fg {
  color: #A03196;
}
.ansi-magenta-intense-bg {
  background-color: #A03196;
}
.ansi-cyan-fg {
  color: #60C6C8;
}
.ansi-cyan-bg {
  background-color: #60C6C8;
}
.ansi-cyan-intense-fg {
  color: #258F8F;
}
.ansi-cyan-intense-bg {
  background-color: #258F8F;
}
.ansi-white-fg {
  color: #C5C1B4;
}
.ansi-white-bg {
  background-color: #C5C1B4;
}
.ansi-white-intense-fg {
  color: #A1A6B2;
}
.ansi-white-intense-bg {
  background-color: #A1A6B2;
}
.ansi-default-inverse-fg {
  color: #FFFFFF;
}
.ansi-default-inverse-bg {
  background-color: #000000;
}
.ansi-bold {
  font-weight: bold;
}
.ansi-underline {
  text-decoration: underline;
}
/* The following styles are deprecated an will be removed in a future version */
.ansibold {
  font-weight: bold;
}
.ansi-inverse {
  outline: 0.5px dotted;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  position: relative;
  overflow: visible;
}
div.cell:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: transparent;
}
div.cell.jupyter-soft-selected {
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected,
div.cell.selected.jupyter-soft-selected {
  border-color: #ababab;
}
div.cell.selected:before,
div.cell.selected.jupyter-soft-selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #42A5F5;
}
@media print {
  div.cell.selected,
  div.cell.selected.jupyter-soft-selected {
    border-color: transparent;
  }
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
}
.edit_mode div.cell.selected:before {
  position: absolute;
  display: block;
  top: -1px;
  left: -1px;
  width: 5px;
  height: calc(100% +  2px);
  content: '';
  background: #66BB6A;
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  /* Note that this should set vertical padding only, since CodeMirror assumes
       that horizontal padding will be set on CodeMirror pre */
  padding: 0.4em 0;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only,
    use .CodeMirror-lines for vertical */
  padding: 0 0.4em;
  border: 0;
  border-radius: 0;
}
.CodeMirror-cursor {
  border-left: 1.4px solid black;
}
@media screen and (min-width: 2138px) and (max-width: 4319px) {
  .CodeMirror-cursor {
    border-left: 2px solid black;
  }
}
@media screen and (min-width: 4320px) {
  .CodeMirror-cursor {
    border-left: 4px solid black;
  }
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
div.output_area .mglyph > img {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 1px 0 1px 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul:not(.list-inline),
.rendered_html ol:not(.list-inline) {
  padding-left: 2em;
}
.rendered_html ul {
  list-style: disc;
}
.rendered_html ul ul {
  list-style: square;
  margin-top: 0;
}
.rendered_html ul ul ul {
  list-style: circle;
}
.rendered_html ol {
  list-style: decimal;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin-top: 0;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
  padding: 0px;
  background-color: #fff;
}
.rendered_html code {
  background-color: #eff0f1;
}
.rendered_html p code {
  padding: 1px 5px;
}
.rendered_html pre code {
  background-color: #fff;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  color: #000;
  font-size: 100%;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
.rendered_html .alert {
  margin-bottom: initial;
}
.rendered_html * + .alert {
  margin-top: 1em;
}
[dir="rtl"] .rendered_html p {
  text-align: right;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.rendered .rendered_html tr,
.text_cell.rendered .rendered_html th,
.text_cell.rendered .rendered_html td {
  max-width: none;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.text_cell .dropzone .input_area {
  border: 2px dashed #bababa;
  margin: -1px;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
.jupyter-keybindings {
  padding: 1px;
  line-height: 24px;
  border-bottom: 1px solid gray;
}
.jupyter-keybindings input {
  margin: 0;
  padding: 0;
  border: none;
}
.jupyter-keybindings i {
  padding: 6px;
}
.well code {
  background-color: #ffffff;
  border-color: #ababab;
  border-width: 1px;
  border-style: solid;
  padding: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.tags_button_container {
  width: 100%;
  display: flex;
}
.tag-container {
  display: flex;
  flex-direction: row;
  flex-grow: 1;
  overflow: hidden;
  position: relative;
}
.tag-container > * {
  margin: 0 4px;
}
.remove-tag-btn {
  margin-left: 4px;
}
.tags-input {
  display: flex;
}
.cell-tag:last-child:after {
  content: "";
  position: absolute;
  right: 0;
  width: 40px;
  height: 100%;
  /* Fade to background color of cell toolbar */
  background: linear-gradient(to right, rgba(0, 0, 0, 0), #EEE);
}
.tags-input > * {
  margin-left: 4px;
}
.cell-tag,
.tags-input input,
.tags-input button {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  box-shadow: none;
  width: inherit;
  font-size: inherit;
  height: 22px;
  line-height: 22px;
  padding: 0px 4px;
  display: inline-block;
}
.cell-tag:focus,
.tags-input input:focus,
.tags-input button:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.cell-tag::-moz-placeholder,
.tags-input input::-moz-placeholder,
.tags-input button::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.cell-tag:-ms-input-placeholder,
.tags-input input:-ms-input-placeholder,
.tags-input button:-ms-input-placeholder {
  color: #999;
}
.cell-tag::-webkit-input-placeholder,
.tags-input input::-webkit-input-placeholder,
.tags-input button::-webkit-input-placeholder {
  color: #999;
}
.cell-tag::-ms-expand,
.tags-input input::-ms-expand,
.tags-input button::-ms-expand {
  border: 0;
  background-color: transparent;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
.cell-tag[readonly],
.tags-input input[readonly],
.tags-input button[readonly],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  background-color: #eeeeee;
  opacity: 1;
}
.cell-tag[disabled],
.tags-input input[disabled],
.tags-input button[disabled],
fieldset[disabled] .cell-tag,
fieldset[disabled] .tags-input input,
fieldset[disabled] .tags-input button {
  cursor: not-allowed;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button {
  height: auto;
}
select.cell-tag,
select.tags-input input,
select.tags-input button {
  height: 30px;
  line-height: 30px;
}
textarea.cell-tag,
textarea.tags-input input,
textarea.tags-input button,
select[multiple].cell-tag,
select[multiple].tags-input input,
select[multiple].tags-input button {
  height: auto;
}
.cell-tag,
.tags-input button {
  padding: 0px 4px;
}
.cell-tag {
  background-color: #fff;
  white-space: nowrap;
}
.tags-input input[type=text]:focus {
  outline: none;
  box-shadow: none;
  border-color: #ccc;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
[dir="rtl"] #kernel_logo_widget {
  float: left !important;
  float: left;
}
.modal .modal-body .move-path {
  display: flex;
  flex-direction: row;
  justify-content: space;
  align-items: center;
}
.modal .modal-body .move-path .server-root {
  padding-right: 20px;
}
.modal .modal-body .move-path .path-input {
  flex: 1;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
[dir="rtl"] #menubar .navbar-toggle {
  float: right;
}
[dir="rtl"] #menubar .navbar-collapse {
  clear: right;
}
[dir="rtl"] #menubar .navbar-nav {
  float: right;
}
[dir="rtl"] #menubar .nav {
  padding-right: 0px;
}
[dir="rtl"] #menubar .navbar-nav > li {
  float: right;
}
[dir="rtl"] #menubar .navbar-right {
  float: left !important;
}
[dir="rtl"] ul.dropdown-menu {
  text-align: right;
  left: auto;
}
[dir="rtl"] ul#new-menu.dropdown-menu {
  right: auto;
  left: 0;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
[dir="rtl"] i.menu-icon.pull-right {
  float: left !important;
  float: left;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
[dir="rtl"] ul#help_menu li a {
  padding-left: 2.2em;
}
[dir="rtl"] ul#help_menu li a i {
  margin-right: 0;
  margin-left: -1.2em;
}
[dir="rtl"] ul#help_menu li a i.pull-right {
  float: left !important;
  float: left;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
[dir="rtl"] .dropdown-submenu > .dropdown-menu {
  right: 100%;
  margin-right: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.fa-pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.fa-pull-right {
  margin-left: .3em;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
[dir="rtl"] .dropdown-submenu > a:after {
  float: left;
  content: "\f0d9";
  margin-right: 0;
  margin-left: -10px;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
[dir="rtl"] #notification_area {
  float: left !important;
  float: left;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] .indicator_area {
  float: left !important;
  float: left;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
[dir="rtl"] #kernel_indicator {
  float: left !important;
  float: left;
  border-left: 0;
  border-right: 1px solid;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
[dir="rtl"] #modal_indicator {
  float: left !important;
  float: left;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.fa-pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.fa-pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.fa-pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.fa-pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  height: 30px;
  margin-top: 4px;
  display: flex;
  justify-content: flex-start;
  align-items: baseline;
  width: 50%;
  flex: 1;
}
span.save_widget span.filename {
  height: 100%;
  line-height: 1em;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
[dir="rtl"] span.save_widget.pull-left {
  float: right !important;
  float: right;
}
[dir="rtl"] span.save_widget span.filename {
  margin-left: 0;
  margin-right: 16px;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
  white-space: nowrap;
  padding: 0 5px;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
    padding: 0 0 0 5px;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
.toolbar-btn-label {
  margin-left: 6px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
[dir="rtl"] .btn-group > .btn,
.btn-group-vertical > .btn {
  float: right;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
[dir="rtl"] ul.typeahead-list i {
  margin-left: 0;
  margin-right: -10px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
ul.typeahead-list  > li > a.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .typeahead-list {
  text-align: right;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  min-width: 20px;
  color: transparent;
}
[dir="rtl"] .no-shortcut.pull-right {
  float: left !important;
  float: left;
}
[dir="rtl"] .command-shortcut.pull-right {
  float: left !important;
  float: left;
}
.command-shortcut:before {
  content: "(command mode)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
[dir="rtl"] .edit-shortcut.pull-right {
  float: left !important;
  float: left;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
[dir="ltr"] #find-and-replace .input-group-btn + .form-control {
  border-left: none;
}
[dir="rtl"] #find-and-replace .input-group-btn + .form-control {
  border-right: none;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    /*This file contains any manual css for this page that needs to override the global styles.
This is only required when different pages style the same element differently. This is just
a hack to deal with our current css styles and no new styling should be added in this file.*/

#ipython-main-app {
    position: relative;
}
#jupyter-main-app {
    position: relative;
}

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">
DATE: April 11, 2020
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Assignment-1:">Assignment 1:<a class="anchor-link" href="#Assignment-1:">&#182;</a></h2><p>In this assignment, we construct <strong>dense neural networks</strong> for classifying images from the <code>MNIST</code> dataset: <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>. The MNIST database consists of a set of 70,000 small (28x28 pixel) grayscale images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents.</p>
<blockquote><p>In <strong>EXPERIMENTS 1 to 3</strong>, we consider <code>dense neural network</code> models with one hidden layer. We start with one node in this hidden layer and progressively increase the number of (hidden) nodes in the layer until we find the "best" model in Experiment 3.</p>
</blockquote>
<p>But our goal in these <code>three</code> experiments is <em>not only</em> to find the "best" <code>dense neural network</code> model with one hidden but <em>also</em> to explore what the node(s) in the hidden layer are "detecting" and what their outputs (i.e. <code>activation values</code>) contribute to the final classification of an image. These are examples of intrinsic <code>global explanations</code> and <code>local explanations</code> , respectively, as described in this recent survey article:</p>
<p><a href="https://cacm.acm.org/magazines/2020/1/241703-techniques-for-interpretable-machine-learning/fulltext">https://cacm.acm.org/magazines/2020/1/241703-techniques-for-interpretable-machine-learning/fulltext</a></p>
<p>To determine what the hidden node is "detecting" we use gradient descent to find a visual pattern that maximizes the activation value of the hidden node, i.e. the visual pattern that the hidden node is maximally responsive to. In this case, the exercise is probably not very insightful. But in <code>Assignment 2</code>  we will repeat this analysis with "clusters of nodes" (<code>convnet filters</code>) in <code>convolutional neural network</code> models. By the examining the patterns that each convnet filter is maximally responsive (but also by visualizing the convnet activations for particular test images) we should be able to determine what (some of) the filters in the <code>CCN</code> model are up to. The (activation values of the) hidden nodes (actually, groups of hidden nodes corresponding or <em>convnet filters</em>) can be visualized in various ways to show what "features" (edges, texture, etc.) of the input images they capture.</p>
<blockquote><p>In <strong>EXPERIMENT 4</strong> we use PCA decomposition to reduce the number of dimensions of our training set of 28x28 dimensional MNIST images from 784 to 154 (with 95% of training images variance lying along these components). We also reduce the number of dimensions of 'best' model from <code>Experiment 3</code> to 154 inputs nodes and train it on the new lower dimensional data.</p>
<p>In <strong>EXPERIMENT 5</strong> we use a Random Forest classifier to get the relative importance of the 784 features (pixels) of the 28x28 dimensional images in training set of MNIST images and select the top 70 features (pixels). We train our 'best' <code>dense neural network</code> using these 70 features and compare its performance to the the <code>dense neural network</code> models from EXPERIMENTS 3 and 4.</p>
</blockquote>
<p>Here are more details for the first <code>three</code> experiments:</p>
<ul>
<li><p><strong>EXPERIMENT 1</strong>: Our <code>dense neural network</code> will consist of 784 input nodes, a hidden layer with <code>1 node</code> and 10 output nodes (corresponding to the 10 digits). We use <code>mnist.load_data()</code> to get the 70,000 images divided into a set of 60,000 training images and 10,000 test images. We hold back 5,000 of the 60,000 training images for validation. After training the model, we group the 60,000 <code>activation values</code> of the hidden node for the (original) set of training images by the 10 predicted classes and visualize these sets of values using a <code>boxplot</code>. We expect the overlap between the range of values in the "boxes" to be minimal. In addition, we find the pattern that maximally activates the hidden node as a "warm up" exercise for similar analysis we will perform on <code>CNN</code> models in <code>Assignment 2</code>.</p>
</li>
<li><p><strong>EXPERIMENT 2</strong>: This time our <code>dense neural network</code> will have 784 input nodes, a hidden layer with <code>2 nodes</code> and 10 output nodes (corresponding to the 10 digits). For each of the 60,000 images, the output of the two hidden nodes are plotted using a <code>scatterplot</code>. We color code the points according to which of the 10 classes the the output of the two nodes predicts. Ideally, just like in <code>EXPERIMENT 1</code>, the color clusters should have very little overlap.</p>
</li>
</ul>
<p><strong>NOTE</strong>: For EXPERIMENTS 1 &amp; 2 we also perform the following additional tasks:</p>
<blockquote><ol>
<li>We use Matplotlib to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side.</li>
<li>For each model we obtain the confusion matrix and use it to display sample images of true vs false positives and negatives.</li>
</ol>
</blockquote>
<ul>
<li><strong>EXPERIMENT 3</strong>: Students can experiment with more hidden nodes (in the hidden layer) to obtain the <code>best</code> model. This <code>final</code> model will be used in EXPERIMENTS 4 &amp; 5.</li>
</ul>
<p><strong>References</strong>:
<a href="https://github.com/fchollet/deep-learning-with-python-notebooks">https://github.com/fchollet/deep-learning-with-python-notebooks</a> (2.1 &amp; 5.4)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Import-packages-needed-(and-set-seed)">Import packages needed (and set seed)<a class="anchor-link" href="#Import-packages-needed-(and-set-seed)">&#182;</a></h2><p>Since Keras in part of TensorFlow 2.x, we import keras from tensorflow and use tenserflow.keras.xxx to import all other Keras packages. The seed argument produces a deterministic sequence of tensors across multiple calls.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="n">keras</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[1]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;2.2.4-tf&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[2]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;2.1.0&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-MNIST-dataset">The MNIST dataset<a class="anchor-link" href="#The-MNIST-dataset">&#182;</a></h2><p>The MNIST dataset of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. It comes prepackaged as part of tf.Keras. Use the <code>mnist.load_data()</code> to the get these datasets (and the corresponding labels) as Numpy arrays.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>

<span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="EXPERIMENT--3:">EXPERIMENT  3:<a class="anchor-link" href="#EXPERIMENT--3:">&#182;</a></h1><p>We want to select the <em>best</em> <code>DNN</code> model subject to some the following restriction on <code>hyperparameters</code>:</p>
<ul>
<li>The number of hidden layers will be one. </li>
</ul>
<p>We will use <code>sklearn.grid_search.GridSearchCV</code> to find the <code>best</code> number of neurons for the hidden layer.</p>
<p>As before we will need 784 input nodes and 10 output nodes (corresponding to the 10 digits). We use <code>mnist.load_data()</code> to get the 70,000 images divided into a set of 60,000 training images and 10,000 test images. We hold back 5,000 of the 60,000 training images for validation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preparing-the-data">Preparing the data<a class="anchor-link" href="#Preparing-the-data">&#182;</a></h2><p>Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in the [0, 1] interval.</p>
<p>Our training images are stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval.</p>
<p>We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>

<span class="n">test_images</span> <span class="o">=</span> <span class="n">test_images</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">test_images</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Validating-our-approach">Validating our approach<a class="anchor-link" href="#Validating-our-approach">&#182;</a></h2><p>Since we will be using categorical_crossentropy as the loss function we need to use to_categorical to convert the class vector of lables to binary matrix representation.</p>
<p>We set apart 5,000 samples of our training data to use as a validation set. Since we will be using <code>sparse_categorical_crossentropy</code> as the loss function we <strong>do not</strong> need to use <code>to_categorical</code> to convert the class vector of labels to binary matrix representation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">val_images</span><span class="p">,</span> <span class="n">train_images</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span> <span class="n">train_images</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span> 
<span class="n">val_labels</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[:</span><span class="mi">5000</span><span class="p">],</span> <span class="n">train_labels</span><span class="p">[</span><span class="mi">5000</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Fine-Tuning-the-Hyperparameters">Fine-Tuning the Hyperparameters<a class="anchor-link" href="#Fine-Tuning-the-Hyperparameters">&#182;</a></h2><p>We adapt the code from Chapter 10 of Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow by A. Geron:</p>
<p><a href="https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb">https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>See also <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html">https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch10.html</a>.</p>
<p>We define a <code>build_model</code> function to create a <code>DNN</code> model with a given number of hidden layers and a fixed given number of nodes per hidden layer. (What if we wanted the number of nodes to vary by layer?)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">n_hidden</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_neurons</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,)):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>No that the <code>build_model</code> function creates a neural network with the same (<em>n_neurons</em>) number of nodes in each hidden layer. Here is the revised <code>build_model2</code> that takes a tuple <code>(n1,n2...)</code>, where the number of coordinates is the number of <code>hidden layers</code> with <code>n1</code> nodes in the first hidden layer, <code>n2</code> nodes in the second, etc.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">build_model2</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,)):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">InputLayer</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">)):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">[</span><span class="n">layer</span><span class="p">],</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>New we a create a <code>KerasClassifier</code> object, the class is an implementation of the scikit-learn classifier API for Keras. It is actually a thin wrapper around the model that is built using our <code>build_model</code> function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.wrappers.scikit_learn</span> <span class="kn">import</span> <span class="n">KerasClassifier</span>
<span class="n">keras_clf</span> <span class="o">=</span> <span class="n">KerasClassifier</span><span class="p">(</span><span class="n">build_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We first consider the performance from using 1 to 100 hidden nodes, i.e. we use the following grid parameters.</p>
<div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neurons&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)}</span>
<span class="n">param_grid</span>
</pre></div>
<p>and found that 20 hidden nodes provided the best model with a test accuracy of 95.99%.</p>
<p>We also tried multiples of 100 from 100 up to 900. Running the test and found the 500 nodes gave the best result with an accuracy of 99.68%. We demonstrate this latter.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neurons&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">101</span><span class="p">)}</span>
<span class="n">param_grid</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[10]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;n_neurons&#39;: range(1, 101)}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">grid_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">keras_clf</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">verbose</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">grid_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_images</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                  <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">val_images</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">),</span>
                  <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 3 folds for each of 100 candidates, totalling 300 fits
[CV] n_neurons=1 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>36666/36666 [==============================] - 2s 43us/sample - loss: 2.0357 - accuracy: 0.2119 - val_loss: 1.9155 - val_accuracy: 0.2374
Epoch 2/30
36666/36666 [==============================] - 1s 35us/sample - loss: 1.8709 - accuracy: 0.2552 - val_loss: 1.8285 - val_accuracy: 0.2614
Epoch 3/30
36666/36666 [==============================] - 1s 36us/sample - loss: 1.8038 - accuracy: 0.2846 - val_loss: 1.7706 - val_accuracy: 0.297661 - accura
Epoch 4/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.7496 - accuracy: 0.3096 - val_loss: 1.7166 - val_accuracy: 0.3290
Epoch 5/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.7004 - accuracy: 0.3205 - val_loss: 1.6710 - val_accuracy: 0.3176
Epoch 6/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.6644 - accuracy: 0.3276 - val_loss: 1.6403 - val_accuracy: 0.3314
Epoch 7/30
36666/36666 [==============================] - 1s 36us/sample - loss: 1.6390 - accuracy: 0.3354 - val_loss: 1.6213 - val_accuracy: 0.3376
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 1.6207 - accuracy: 0.3410 - val_loss: 1.6086 - val_accuracy: 0.3442
Epoch 9/30
36666/36666 [==============================] - 1s 33us/sample - loss: 1.6072 - accuracy: 0.3420 - val_loss: 1.5901 - val_accuracy: 0.3492
Epoch 10/30
36666/36666 [==============================] - 1s 37us/sample - loss: 1.5964 - accuracy: 0.3461 - val_loss: 1.5833 - val_accuracy: 0.3596
Epoch 11/30
36666/36666 [==============================] - 1s 33us/sample - loss: 1.5874 - accuracy: 0.3517 - val_loss: 1.5722 - val_accuracy: 0.3536
Epoch 12/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.5805 - accuracy: 0.3599 - val_loss: 1.5655 - val_accuracy: 0.3604
Epoch 13/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.5745 - accuracy: 0.3668 - val_loss: 1.5599 - val_accuracy: 0.3576
Epoch 14/30
36666/36666 [==============================] - 1s 30us/sample - loss: 1.5698 - accuracy: 0.3673 - val_loss: 1.5570 - val_accuracy: 0.3682
Epoch 15/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.5663 - accuracy: 0.3661 - val_loss: 1.5556 - val_accuracy: 0.3742
Epoch 16/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.5628 - accuracy: 0.3677 - val_loss: 1.5477 - val_accuracy: 0.3696
Epoch 17/30
36666/36666 [==============================] - 1s 33us/sample - loss: 1.5604 - accuracy: 0.3729 - val_loss: 1.5476 - val_accuracy: 0.3610
Epoch 18/30
36666/36666 [==============================] - 1s 30us/sample - loss: 1.5582 - accuracy: 0.3684 - val_loss: 1.5433 - val_accuracy: 0.3796
Epoch 19/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.5563 - accuracy: 0.3738 - val_loss: 1.5413 - val_accuracy: 0.3662
Epoch 20/30
36666/36666 [==============================] - 1s 30us/sample - loss: 1.5552 - accuracy: 0.3700 - val_loss: 1.5367 - val_accuracy: 0.3750
Epoch 21/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.5536 - accuracy: 0.3700 - val_loss: 1.5364 - val_accuracy: 0.3854
Epoch 22/30
36666/36666 [==============================] - 1s 30us/sample - loss: 1.5521 - accuracy: 0.3727 - val_loss: 1.5352 - val_accuracy: 0.3874
Epoch 23/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.5515 - accuracy: 0.3701 - val_loss: 1.5393 - val_accuracy: 0.3586
Epoch 24/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.5503 - accuracy: 0.3691 - val_loss: 1.5376 - val_accuracy: 0.3776
18334/18334 [==============================] - 0s 22us/sample - loss: 1.5726 - accuracy: 0.3775s - loss: 1.5715 - accuracy: 
[CV] ...................................... n_neurons=1, total=  29.4s
[CV] n_neurons=1 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   29.4s remaining:    0.0s
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>36667/36667 [==============================] - 1s 39us/sample - loss: 2.0274 - accuracy: 0.2141 - val_loss: 1.9179 - val_accuracy: 0.2356
Epoch 2/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.8757 - accuracy: 0.2549 - val_loss: 1.8207 - val_accuracy: 0.2808
Epoch 3/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.8066 - accuracy: 0.2759 - val_loss: 1.7689 - val_accuracy: 0.2936
Epoch 4/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.7666 - accuracy: 0.2886 - val_loss: 1.7396 - val_accuracy: 0.3000
Epoch 5/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.7387 - accuracy: 0.2971 - val_loss: 1.7179 - val_accuracy: 0.3174
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.7161 - accuracy: 0.3108 - val_loss: 1.6956 - val_accuracy: 0.3264
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 1.6953 - accuracy: 0.3256 - val_loss: 1.6731 - val_accuracy: 0.3438
Epoch 8/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6741 - accuracy: 0.3464 - val_loss: 1.6540 - val_accuracy: 0.3586
Epoch 9/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.6555 - accuracy: 0.3554 - val_loss: 1.6359 - val_accuracy: 0.3636
Epoch 10/30
36667/36667 [==============================] - 1s 36us/sample - loss: 1.6401 - accuracy: 0.3659 - val_loss: 1.6230 - val_accuracy: 0.3782
Epoch 11/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.6275 - accuracy: 0.3715 - val_loss: 1.6129 - val_accuracy: 0.3826
Epoch 12/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.6172 - accuracy: 0.3794 - val_loss: 1.6053 - val_accuracy: 0.3898
Epoch 13/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.6098 - accuracy: 0.3841 - val_loss: 1.6036 - val_accuracy: 0.3888
Epoch 14/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6028 - accuracy: 0.3896 - val_loss: 1.5937 - val_accuracy: 0.4014
Epoch 15/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5981 - accuracy: 0.3902 - val_loss: 1.5893 - val_accuracy: 0.3910
Epoch 16/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5944 - accuracy: 0.3926 - val_loss: 1.5840 - val_accuracy: 0.3986
Epoch 17/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5904 - accuracy: 0.3943 - val_loss: 1.5812 - val_accuracy: 0.4000
Epoch 18/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5880 - accuracy: 0.3959 - val_loss: 1.5796 - val_accuracy: 0.4096
Epoch 19/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5849 - accuracy: 0.3955 - val_loss: 1.5770 - val_accuracy: 0.4066
Epoch 20/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.5834 - accuracy: 0.3992 - val_loss: 1.5761 - val_accuracy: 0.4018
Epoch 21/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.5819 - accuracy: 0.3979 - val_loss: 1.5720 - val_accuracy: 0.4144
Epoch 22/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5799 - accuracy: 0.3983 - val_loss: 1.5690 - val_accuracy: 0.4160
Epoch 23/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.5781 - accuracy: 0.4003 - val_loss: 1.5696 - val_accuracy: 0.4088
Epoch 24/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5767 - accuracy: 0.4012 - val_loss: 1.5684 - val_accuracy: 0.4134
Epoch 25/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5748 - accuracy: 0.4014 - val_loss: 1.5712 - val_accuracy: 0.4146
Epoch 26/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5750 - accuracy: 0.4027 - val_loss: 1.5679 - val_accuracy: 0.4192
Epoch 27/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5735 - accuracy: 0.4028 - val_loss: 1.5780 - val_accuracy: 0.4022
Epoch 28/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.5722 - accuracy: 0.4037 - val_loss: 1.5644 - val_accuracy: 0.4138
Epoch 29/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.5714 - accuracy: 0.4012 - val_loss: 1.5648 - val_accuracy: 0.4120
Epoch 30/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.5705 - accuracy: 0.4032 - val_loss: 1.5698 - val_accuracy: 0.4126
18333/18333 [==============================] - 0s 21us/sample - loss: 1.5757 - accuracy: 0.3996
[CV] ...................................... n_neurons=1, total=  35.7s
[CV] n_neurons=1 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 2.0350 - accuracy: 0.2317 - val_loss: 1.9329 - val_accuracy: 0.2484
Epoch 2/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.8832 - accuracy: 0.2692 - val_loss: 1.8255 - val_accuracy: 0.2804
Epoch 3/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.8106 - accuracy: 0.2811 - val_loss: 1.7813 - val_accuracy: 0.2988
Epoch 4/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.7690 - accuracy: 0.3000 - val_loss: 1.7417 - val_accuracy: 0.3010
Epoch 5/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.7398 - accuracy: 0.3055 - val_loss: 1.7179 - val_accuracy: 0.3096
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.7181 - accuracy: 0.3128 - val_loss: 1.7020 - val_accuracy: 0.3092
Epoch 7/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6994 - accuracy: 0.3189 - val_loss: 1.6881 - val_accuracy: 0.3158
Epoch 8/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.6843 - accuracy: 0.3251 - val_loss: 1.6700 - val_accuracy: 0.3194
Epoch 9/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6711 - accuracy: 0.3283 - val_loss: 1.6586 - val_accuracy: 0.3152
Epoch 10/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6618 - accuracy: 0.3315 - val_loss: 1.6481 - val_accuracy: 0.3316
Epoch 11/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6532 - accuracy: 0.3335 - val_loss: 1.6390 - val_accuracy: 0.3332
Epoch 12/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.6463 - accuracy: 0.3356 - val_loss: 1.6331 - val_accuracy: 0.3406
Epoch 13/30
36667/36667 [==============================] - 1s 34us/sample - loss: 1.6407 - accuracy: 0.3429 - val_loss: 1.6265 - val_accuracy: 0.3282
Epoch 14/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.6357 - accuracy: 0.3419 - val_loss: 1.6219 - val_accuracy: 0.3392
Epoch 15/30
36667/36667 [==============================] - 2s 47us/sample - loss: 1.6313 - accuracy: 0.3437 - val_loss: 1.6169 - val_accuracy: 0.3446
Epoch 16/30
36667/36667 [==============================] - 2s 42us/sample - loss: 1.6278 - accuracy: 0.3464 - val_loss: 1.6176 - val_accuracy: 0.3414
Epoch 17/30
36667/36667 [==============================] - 1s 35us/sample - loss: 1.6249 - accuracy: 0.3462 - val_loss: 1.6126 - val_accuracy: 0.3516
Epoch 18/30
36667/36667 [==============================] - 1s 34us/sample - loss: 1.6224 - accuracy: 0.3502 - val_loss: 1.6083 - val_accuracy: 0.3498
Epoch 19/30
36667/36667 [==============================] - 1s 38us/sample - loss: 1.6188 - accuracy: 0.3520 - val_loss: 1.6067 - val_accuracy: 0.3528
Epoch 20/30
36667/36667 [==============================] - 1s 36us/sample - loss: 1.6173 - accuracy: 0.3523 - val_loss: 1.6000 - val_accuracy: 0.3430
Epoch 21/30
36667/36667 [==============================] - 1s 36us/sample - loss: 1.6153 - accuracy: 0.3517 - val_loss: 1.6052 - val_accuracy: 0.3444
Epoch 22/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.6132 - accuracy: 0.3536 - val_loss: 1.5990 - val_accuracy: 0.3570
Epoch 23/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6117 - accuracy: 0.3541 - val_loss: 1.6025 - val_accuracy: 0.3560
Epoch 24/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.6099 - accuracy: 0.3559 - val_loss: 1.5978 - val_accuracy: 0.3528
Epoch 25/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.6083 - accuracy: 0.3560 - val_loss: 1.5939 - val_accuracy: 0.3432
Epoch 26/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6070 - accuracy: 0.3575 - val_loss: 1.5959 - val_accuracy: 0.3456
Epoch 27/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6053 - accuracy: 0.3561 - val_loss: 1.5931 - val_accuracy: 0.3538
Epoch 28/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.6048 - accuracy: 0.3566 - val_loss: 1.5948 - val_accuracy: 0.3660
Epoch 29/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.6033 - accuracy: 0.3593 - val_loss: 1.5868 - val_accuracy: 0.3630
Epoch 30/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.6028 - accuracy: 0.3574 - val_loss: 1.5899 - val_accuracy: 0.3666
18333/18333 [==============================] - 0s 21us/sample - loss: 1.6180 - accuracy: 0.3559
[CV] ...................................... n_neurons=1, total=  37.7s
[CV] n_neurons=2 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 1s 40us/sample - loss: 1.7595 - accuracy: 0.3182 - val_loss: 1.5168 - val_accuracy: 0.3850
Epoch 2/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.4440 - accuracy: 0.4306 - val_loss: 1.3792 - val_accuracy: 0.4606
Epoch 3/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.3217 - accuracy: 0.5003 - val_loss: 1.2786 - val_accuracy: 0.5272
Epoch 4/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.2227 - accuracy: 0.5549 - val_loss: 1.1970 - val_accuracy: 0.5758
Epoch 5/30
36666/36666 [==============================] - 1s 31us/sample - loss: 1.1523 - accuracy: 0.5985 - val_loss: 1.1467 - val_accuracy: 0.6120
Epoch 6/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.1081 - accuracy: 0.6253 - val_loss: 1.1126 - val_accuracy: 0.6350
Epoch 7/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.0831 - accuracy: 0.6394 - val_loss: 1.1005 - val_accuracy: 0.6346
Epoch 8/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.0669 - accuracy: 0.6476 - val_loss: 1.0878 - val_accuracy: 0.6378
Epoch 9/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.0567 - accuracy: 0.6502 - val_loss: 1.0841 - val_accuracy: 0.6636
Epoch 10/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.0500 - accuracy: 0.6566 - val_loss: 1.0742 - val_accuracy: 0.6606
Epoch 11/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.0447 - accuracy: 0.6579 - val_loss: 1.0698 - val_accuracy: 0.6674
Epoch 12/30
36666/36666 [==============================] - 1s 32us/sample - loss: 1.0403 - accuracy: 0.6627 - val_loss: 1.0666 - val_accuracy: 0.6662
Epoch 13/30
36666/36666 [==============================] - 1s 34us/sample - loss: 1.0364 - accuracy: 0.6646 - val_loss: 1.0634 - val_accuracy: 0.6692
Epoch 14/30
36666/36666 [==============================] - 1s 35us/sample - loss: 1.0343 - accuracy: 0.6663 - val_loss: 1.0626 - val_accuracy: 0.6696
Epoch 15/30
36666/36666 [==============================] - 1s 33us/sample - loss: 1.0331 - accuracy: 0.6675 - val_loss: 1.0602 - val_accuracy: 0.6708
Epoch 16/30
36666/36666 [==============================] - 1s 38us/sample - loss: 1.0309 - accuracy: 0.6698 - val_loss: 1.0600 - val_accuracy: 0.6756
Epoch 17/30
36666/36666 [==============================] - 1s 34us/sample - loss: 1.0289 - accuracy: 0.6706 - val_loss: 1.0699 - val_accuracy: 0.6748
Epoch 18/30
36666/36666 [==============================] - 1s 35us/sample - loss: 1.0284 - accuracy: 0.6719 - val_loss: 1.0662 - val_accuracy: 0.6690
18334/18334 [==============================] - 0s 22us/sample - loss: 1.0816 - accuracy: 0.6492
[CV] ...................................... n_neurons=2, total=  22.6s
[CV] n_neurons=2 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 1s 40us/sample - loss: 1.6743 - accuracy: 0.3810 - val_loss: 1.4497 - val_accuracy: 0.4358
Epoch 2/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.3500 - accuracy: 0.4717 - val_loss: 1.2761 - val_accuracy: 0.5104
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 1.2288 - accuracy: 0.5332 - val_loss: 1.1903 - val_accuracy: 0.5582
Epoch 4/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1698 - accuracy: 0.5637 - val_loss: 1.1509 - val_accuracy: 0.5822
Epoch 5/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1353 - accuracy: 0.5866 - val_loss: 1.1280 - val_accuracy: 0.5932
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1087 - accuracy: 0.6096 - val_loss: 1.0981 - val_accuracy: 0.6288
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.0823 - accuracy: 0.6324 - val_loss: 1.0790 - val_accuracy: 0.6512
Epoch 8/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.0565 - accuracy: 0.6514 - val_loss: 1.0553 - val_accuracy: 0.6668
Epoch 9/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.0363 - accuracy: 0.6623 - val_loss: 1.0416 - val_accuracy: 0.6708
Epoch 10/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.0201 - accuracy: 0.6691 - val_loss: 1.0253 - val_accuracy: 0.6856
Epoch 11/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.0087 - accuracy: 0.6752 - val_loss: 1.0180 - val_accuracy: 0.6868
Epoch 12/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.9996 - accuracy: 0.6796 - val_loss: 1.0115 - val_accuracy: 0.6888
Epoch 13/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.9919 - accuracy: 0.6833 - val_loss: 1.0046 - val_accuracy: 0.6912
Epoch 14/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.9863 - accuracy: 0.6841 - val_loss: 1.0033 - val_accuracy: 0.6952
Epoch 15/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.9807 - accuracy: 0.6840 - val_loss: 1.0034 - val_accuracy: 0.6958
Epoch 16/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.9766 - accuracy: 0.6874 - val_loss: 0.9975 - val_accuracy: 0.6980
Epoch 17/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.9732 - accuracy: 0.6873 - val_loss: 1.0004 - val_accuracy: 0.6944
Epoch 18/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.9711 - accuracy: 0.6879 - val_loss: 0.9968 - val_accuracy: 0.6924
Epoch 19/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.9686 - accuracy: 0.6920 - val_loss: 0.9944 - val_accuracy: 0.6936
Epoch 20/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.9664 - accuracy: 0.6918 - val_loss: 0.9906 - val_accuracy: 0.7004
Epoch 21/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.9644 - accuracy: 0.6909 - val_loss: 1.0007 - val_accuracy: 0.6832
Epoch 22/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.9624 - accuracy: 0.6918 - val_loss: 1.0058 - val_accuracy: 0.6832
18333/18333 [==============================] - 0s 22us/sample - loss: 1.0011 - accuracy: 0.6820
[CV] ...................................... n_neurons=2, total=  27.3s
[CV] n_neurons=2 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 1s 38us/sample - loss: 1.7228 - accuracy: 0.3744 - val_loss: 1.5191 - val_accuracy: 0.4710
Epoch 2/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.4448 - accuracy: 0.4900 - val_loss: 1.3546 - val_accuracy: 0.5262
Epoch 3/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.3292 - accuracy: 0.5229 - val_loss: 1.2680 - val_accuracy: 0.5388
Epoch 4/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.2616 - accuracy: 0.5435 - val_loss: 1.2204 - val_accuracy: 0.5472
Epoch 5/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.2246 - accuracy: 0.5511 - val_loss: 1.2015 - val_accuracy: 0.5512
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.2027 - accuracy: 0.5592 - val_loss: 1.1791 - val_accuracy: 0.5644
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.1864 - accuracy: 0.5709 - val_loss: 1.1739 - val_accuracy: 0.5706
Epoch 8/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1728 - accuracy: 0.5802 - val_loss: 1.1581 - val_accuracy: 0.5872
Epoch 9/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1588 - accuracy: 0.5919 - val_loss: 1.1449 - val_accuracy: 0.6044
Epoch 10/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1470 - accuracy: 0.6029 - val_loss: 1.1351 - val_accuracy: 0.6086
Epoch 11/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1365 - accuracy: 0.6079 - val_loss: 1.1352 - val_accuracy: 0.6074
Epoch 12/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1273 - accuracy: 0.6151 - val_loss: 1.1203 - val_accuracy: 0.6244
Epoch 13/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1183 - accuracy: 0.6230 - val_loss: 1.1110 - val_accuracy: 0.6292
Epoch 14/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.1099 - accuracy: 0.6270 - val_loss: 1.1063 - val_accuracy: 0.6376
Epoch 15/30
36667/36667 [==============================] - 1s 33us/sample - loss: 1.1027 - accuracy: 0.6307 - val_loss: 1.1015 - val_accuracy: 0.6296
Epoch 16/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.0962 - accuracy: 0.6339 - val_loss: 1.0933 - val_accuracy: 0.6384
Epoch 17/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.0906 - accuracy: 0.6383 - val_loss: 1.0919 - val_accuracy: 0.6416
Epoch 18/30
36667/36667 [==============================] - 1s 31us/sample - loss: 1.0867 - accuracy: 0.6387 - val_loss: 1.0863 - val_accuracy: 0.6498
Epoch 19/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.0822 - accuracy: 0.6405 - val_loss: 1.0959 - val_accuracy: 0.6390
Epoch 20/30
36667/36667 [==============================] - 1s 32us/sample - loss: 1.0790 - accuracy: 0.6416 - val_loss: 1.0905 - val_accuracy: 0.6496
18333/18333 [==============================] - 0s 21us/sample - loss: 1.0734 - accuracy: 0.6334
[CV] ...................................... n_neurons=2, total=  24.3s
[CV] n_neurons=3 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 41us/sample - loss: 1.4275 - accuracy: 0.5321 - val_loss: 1.1220 - val_accuracy: 0.6754
Epoch 2/30
36666/36666 [==============================] - 1s 33us/sample - loss: 1.0028 - accuracy: 0.7024 - val_loss: 0.9266 - val_accuracy: 0.7282
Epoch 3/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.8778 - accuracy: 0.7308 - val_loss: 0.8411 - val_accuracy: 0.7482
Epoch 4/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.8165 - accuracy: 0.7466 - val_loss: 0.7960 - val_accuracy: 0.7640
Epoch 5/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.7777 - accuracy: 0.7585 - val_loss: 0.7675 - val_accuracy: 0.7686
Epoch 6/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.7493 - accuracy: 0.7704 - val_loss: 0.7388 - val_accuracy: 0.7894
Epoch 7/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.7266 - accuracy: 0.7835 - val_loss: 0.7192 - val_accuracy: 0.7944
Epoch 8/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.7078 - accuracy: 0.7916 - val_loss: 0.7034 - val_accuracy: 0.8046
Epoch 9/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.6947 - accuracy: 0.7975 - val_loss: 0.6934 - val_accuracy: 0.8014
Epoch 10/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.6861 - accuracy: 0.8010 - val_loss: 0.6875 - val_accuracy: 0.8076
Epoch 11/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.6798 - accuracy: 0.8037 - val_loss: 0.6834 - val_accuracy: 0.8098
Epoch 12/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.6742 - accuracy: 0.8056 - val_loss: 0.6790 - val_accuracy: 0.8104
Epoch 13/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.6716 - accuracy: 0.8068 - val_loss: 0.6804 - val_accuracy: 0.8112
Epoch 14/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.6680 - accuracy: 0.8062 - val_loss: 0.6776 - val_accuracy: 0.8100
Epoch 15/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.6667 - accuracy: 0.8082 - val_loss: 0.6722 - val_accuracy: 0.8112
Epoch 16/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.6648 - accuracy: 0.8095 - val_loss: 0.6803 - val_accuracy: 0.8122
Epoch 17/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.6640 - accuracy: 0.8098 - val_loss: 0.6780 - val_accuracy: 0.8114
18334/18334 [==============================] - 0s 20us/sample - loss: 0.6994 - accuracy: 0.8038
[CV] ...................................... n_neurons=3, total=  21.7s
[CV] n_neurons=3 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 1s 40us/sample - loss: 1.4352 - accuracy: 0.5355 - val_loss: 1.0921 - val_accuracy: 0.6692
Epoch 2/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.9904 - accuracy: 0.6766 - val_loss: 0.9114 - val_accuracy: 0.6958
Epoch 3/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.8911 - accuracy: 0.7017 - val_loss: 0.8522 - val_accuracy: 0.7316
Epoch 4/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.8455 - accuracy: 0.7182 - val_loss: 0.8237 - val_accuracy: 0.7406
Epoch 5/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.8141 - accuracy: 0.7288 - val_loss: 0.7944 - val_accuracy: 0.7444
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7915 - accuracy: 0.7372 - val_loss: 0.7732 - val_accuracy: 0.7552
Epoch 7/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7739 - accuracy: 0.7453 - val_loss: 0.7574 - val_accuracy: 0.7626
Epoch 8/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7608 - accuracy: 0.7493 - val_loss: 0.7498 - val_accuracy: 0.7678
Epoch 9/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.7489 - accuracy: 0.7560 - val_loss: 0.7428 - val_accuracy: 0.7664
Epoch 10/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7399 - accuracy: 0.7585 - val_loss: 0.7301 - val_accuracy: 0.7748
Epoch 11/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7329 - accuracy: 0.7597 - val_loss: 0.7263 - val_accuracy: 0.7790
Epoch 12/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7265 - accuracy: 0.7650 - val_loss: 0.7252 - val_accuracy: 0.7836
Epoch 13/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.7207 - accuracy: 0.7678 - val_loss: 0.7177 - val_accuracy: 0.7832
Epoch 14/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.7163 - accuracy: 0.7677 - val_loss: 0.7127 - val_accuracy: 0.7852
Epoch 15/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.7119 - accuracy: 0.7692 - val_loss: 0.7123 - val_accuracy: 0.7814
Epoch 16/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.7085 - accuracy: 0.7720 - val_loss: 0.7090 - val_accuracy: 0.7680
Epoch 17/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7055 - accuracy: 0.7733 - val_loss: 0.7055 - val_accuracy: 0.7910
Epoch 18/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7024 - accuracy: 0.7748 - val_loss: 0.7086 - val_accuracy: 0.7876
Epoch 19/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7006 - accuracy: 0.7748 - val_loss: 0.7062 - val_accuracy: 0.7858
18333/18333 [==============================] - 0s 20us/sample - loss: 0.7349 - accuracy: 0.7688
[CV] ...................................... n_neurons=3, total=  23.1s
[CV] n_neurons=3 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 1s 40us/sample - loss: 1.4306 - accuracy: 0.4996 - val_loss: 1.0043 - val_accuracy: 0.6796
Epoch 2/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.9444 - accuracy: 0.6962 - val_loss: 0.8291 - val_accuracy: 0.7378
Epoch 3/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.8433 - accuracy: 0.7296 - val_loss: 0.7738 - val_accuracy: 0.7570
Epoch 4/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7998 - accuracy: 0.7476 - val_loss: 0.7377 - val_accuracy: 0.7764
Epoch 5/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7757 - accuracy: 0.7608 - val_loss: 0.7256 - val_accuracy: 0.7750
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7601 - accuracy: 0.7667 - val_loss: 0.7169 - val_accuracy: 0.7814
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.7483 - accuracy: 0.7696 - val_loss: 0.7031 - val_accuracy: 0.7856
Epoch 8/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7403 - accuracy: 0.7725 - val_loss: 0.7005 - val_accuracy: 0.7914
Epoch 9/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7342 - accuracy: 0.7766 - val_loss: 0.6963 - val_accuracy: 0.7920
Epoch 10/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.7286 - accuracy: 0.7803 - val_loss: 0.6897 - val_accuracy: 0.7954
Epoch 11/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7242 - accuracy: 0.7808 - val_loss: 0.6938 - val_accuracy: 0.7914
Epoch 12/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.7211 - accuracy: 0.7818 - val_loss: 0.6879 - val_accuracy: 0.7942
Epoch 13/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.7176 - accuracy: 0.7831 - val_loss: 0.6856 - val_accuracy: 0.7942
Epoch 14/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.7142 - accuracy: 0.7848 - val_loss: 0.6816 - val_accuracy: 0.7986
Epoch 15/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.7122 - accuracy: 0.7855 - val_loss: 0.6862 - val_accuracy: 0.7918
Epoch 16/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.7097 - accuracy: 0.7869 - val_loss: 0.6833 - val_accuracy: 0.8016
18333/18333 [==============================] - 0s 22us/sample - loss: 0.7131 - accuracy: 0.7860
[CV] ...................................... n_neurons=3, total=  19.5s
[CV] n_neurons=4 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 50us/sample - loss: 1.1599 - accuracy: 0.6042 - val_loss: 0.7494 - val_accuracy: 0.7790
Epoch 2/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.7135 - accuracy: 0.7854 - val_loss: 0.6300 - val_accuracy: 0.8190
Epoch 3/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.6432 - accuracy: 0.8093 - val_loss: 0.5964 - val_accuracy: 0.8292
Epoch 4/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.6129 - accuracy: 0.8205 - val_loss: 0.5811 - val_accuracy: 0.8366
Epoch 5/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.5948 - accuracy: 0.8264 - val_loss: 0.5690 - val_accuracy: 0.8398
Epoch 6/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5816 - accuracy: 0.8314 - val_loss: 0.5602 - val_accuracy: 0.8420
Epoch 7/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5699 - accuracy: 0.8372 - val_loss: 0.5564 - val_accuracy: 0.8482
Epoch 8/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.5605 - accuracy: 0.8399 - val_loss: 0.5461 - val_accuracy: 0.8478
Epoch 9/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5510 - accuracy: 0.8421 - val_loss: 0.5349 - val_accuracy: 0.8508
Epoch 10/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5418 - accuracy: 0.8473 - val_loss: 0.5316 - val_accuracy: 0.8526
Epoch 11/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5337 - accuracy: 0.8505 - val_loss: 0.5292 - val_accuracy: 0.8570
Epoch 12/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5256 - accuracy: 0.8516 - val_loss: 0.5191 - val_accuracy: 0.8554
Epoch 13/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5172 - accuracy: 0.8556 - val_loss: 0.5157 - val_accuracy: 0.8610
Epoch 14/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.5097 - accuracy: 0.8587 - val_loss: 0.5044 - val_accuracy: 0.8642
Epoch 15/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5020 - accuracy: 0.8611 - val_loss: 0.4991 - val_accuracy: 0.8672
Epoch 16/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4940 - accuracy: 0.8625 - val_loss: 0.4957 - val_accuracy: 0.8658
Epoch 17/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4873 - accuracy: 0.8648 - val_loss: 0.4985 - val_accuracy: 0.8650
Epoch 18/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.4825 - accuracy: 0.8657 - val_loss: 0.4935 - val_accuracy: 0.8696
Epoch 19/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.4777 - accuracy: 0.8679 - val_loss: 0.4829 - val_accuracy: 0.8706
Epoch 20/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.4731 - accuracy: 0.8693 - val_loss: 0.4832 - val_accuracy: 0.8670
Epoch 21/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4691 - accuracy: 0.8710 - val_loss: 0.4815 - val_accuracy: 0.8716
Epoch 22/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4654 - accuracy: 0.8706 - val_loss: 0.4809 - val_accuracy: 0.8708
Epoch 23/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4629 - accuracy: 0.8728 - val_loss: 0.4768 - val_accuracy: 0.8724
Epoch 24/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.4609 - accuracy: 0.8736 - val_loss: 0.4800 - val_accuracy: 0.8702
Epoch 25/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4585 - accuracy: 0.8740 - val_loss: 0.4743 - val_accuracy: 0.8742
Epoch 26/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4565 - accuracy: 0.8746 - val_loss: 0.4825 - val_accuracy: 0.8734
Epoch 27/30
36666/36666 [==============================] - 1s 31us/sample - loss: 0.4552 - accuracy: 0.8755 - val_loss: 0.4709 - val_accuracy: 0.8754
Epoch 28/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4535 - accuracy: 0.8760 - val_loss: 0.4754 - val_accuracy: 0.8776
Epoch 29/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4520 - accuracy: 0.8768 - val_loss: 0.4756 - val_accuracy: 0.8742
18334/18334 [==============================] - 0s 22us/sample - loss: 0.5079 - accuracy: 0.8666
[CV] ...................................... n_neurons=4, total=  35.2s
[CV] n_neurons=4 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 41us/sample - loss: 1.4128 - accuracy: 0.5363 - val_loss: 0.8691 - val_accuracy: 0.7590
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.7739 - accuracy: 0.7707 - val_loss: 0.6898 - val_accuracy: 0.8076
Epoch 3/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.6773 - accuracy: 0.8005 - val_loss: 0.6468 - val_accuracy: 0.8228
Epoch 4/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.6401 - accuracy: 0.8144 - val_loss: 0.6183 - val_accuracy: 0.8256
Epoch 5/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.6164 - accuracy: 0.8225 - val_loss: 0.6006 - val_accuracy: 0.8336
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5993 - accuracy: 0.8270 - val_loss: 0.5864 - val_accuracy: 0.8392
Epoch 7/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5851 - accuracy: 0.8310 - val_loss: 0.5772 - val_accuracy: 0.8390
Epoch 8/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5734 - accuracy: 0.8351 - val_loss: 0.5697 - val_accuracy: 0.8432
Epoch 9/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5647 - accuracy: 0.8377 - val_loss: 0.5726 - val_accuracy: 0.8380
Epoch 10/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.5569 - accuracy: 0.8391 - val_loss: 0.5584 - val_accuracy: 0.8454
Epoch 11/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5511 - accuracy: 0.8415 - val_loss: 0.5511 - val_accuracy: 0.8464
Epoch 12/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.5458 - accuracy: 0.8435 - val_loss: 0.5557 - val_accuracy: 0.8484
Epoch 13/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.5419 - accuracy: 0.8448 - val_loss: 0.5402 - val_accuracy: 0.8484
Epoch 14/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5366 - accuracy: 0.8474 - val_loss: 0.5407 - val_accuracy: 0.8496
Epoch 15/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5327 - accuracy: 0.8486 - val_loss: 0.5413 - val_accuracy: 0.8466
18333/18333 [==============================] - 0s 20us/sample - loss: 0.5799 - accuracy: 0.8365
[CV] ...................................... n_neurons=4, total=  18.6s
[CV] n_neurons=4 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 41us/sample - loss: 1.3230 - accuracy: 0.5670 - val_loss: 0.9416 - val_accuracy: 0.6970
Epoch 2/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.8202 - accuracy: 0.7526 - val_loss: 0.7058 - val_accuracy: 0.8012
Epoch 3/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.6512 - accuracy: 0.8142 - val_loss: 0.5938 - val_accuracy: 0.8316
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.5766 - accuracy: 0.8342 - val_loss: 0.5440 - val_accuracy: 0.8444
Epoch 5/30
36667/36667 [==============================] - 1s 31us/sample - loss: 0.5387 - accuracy: 0.8434 - val_loss: 0.5169 - val_accuracy: 0.8546
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5160 - accuracy: 0.8492 - val_loss: 0.5014 - val_accuracy: 0.8568
Epoch 7/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.5022 - accuracy: 0.8542 - val_loss: 0.4823 - val_accuracy: 0.8582
Epoch 8/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.4917 - accuracy: 0.8573 - val_loss: 0.4789 - val_accuracy: 0.8656
Epoch 9/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.4833 - accuracy: 0.8609 - val_loss: 0.4761 - val_accuracy: 0.8680
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.4786 - accuracy: 0.8619 - val_loss: 0.4695 - val_accuracy: 0.8664
Epoch 11/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.4738 - accuracy: 0.8653 - val_loss: 0.4601 - val_accuracy: 0.8728
Epoch 12/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.4695 - accuracy: 0.8680 - val_loss: 0.4617 - val_accuracy: 0.8696
Epoch 13/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.4659 - accuracy: 0.8673 - val_loss: 0.4619 - val_accuracy: 0.8688
18333/18333 [==============================] - 0s 22us/sample - loss: 0.4615 - accuracy: 0.8646
[CV] ...................................... n_neurons=4, total=  16.4s
[CV] n_neurons=5 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 41us/sample - loss: 1.1521 - accuracy: 0.6519 - val_loss: 0.6949 - val_accuracy: 0.8148
Epoch 2/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.6088 - accuracy: 0.8294 - val_loss: 0.5123 - val_accuracy: 0.8584
Epoch 3/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.5089 - accuracy: 0.8546 - val_loss: 0.4580 - val_accuracy: 0.8756
Epoch 4/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.4656 - accuracy: 0.8660 - val_loss: 0.4361 - val_accuracy: 0.8766
Epoch 5/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.4382 - accuracy: 0.8727 - val_loss: 0.4137 - val_accuracy: 0.8846
Epoch 6/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.4196 - accuracy: 0.8789 - val_loss: 0.4016 - val_accuracy: 0.8908
Epoch 7/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.4067 - accuracy: 0.8834 - val_loss: 0.3963 - val_accuracy: 0.8900
Epoch 8/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.3965 - accuracy: 0.8870 - val_loss: 0.3843 - val_accuracy: 0.8940
Epoch 9/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3890 - accuracy: 0.8886 - val_loss: 0.3875 - val_accuracy: 0.8940
Epoch 10/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3829 - accuracy: 0.8906 - val_loss: 0.3799 - val_accuracy: 0.8980
Epoch 11/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3767 - accuracy: 0.8930 - val_loss: 0.3802 - val_accuracy: 0.8946
Epoch 12/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.3726 - accuracy: 0.8949 - val_loss: 0.3816 - val_accuracy: 0.8938
18334/18334 [==============================] - 0s 20us/sample - loss: 0.4104 - accuracy: 0.8870
[CV] ...................................... n_neurons=5, total=  15.4s
[CV] n_neurons=5 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 1.0076 - accuracy: 0.6814 - val_loss: 0.5988 - val_accuracy: 0.8284
Epoch 2/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.5535 - accuracy: 0.8339 - val_loss: 0.4781 - val_accuracy: 0.8608
Epoch 3/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.4703 - accuracy: 0.8612 - val_loss: 0.4302 - val_accuracy: 0.8776
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.4306 - accuracy: 0.8754 - val_loss: 0.4065 - val_accuracy: 0.8856
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.4076 - accuracy: 0.8812 - val_loss: 0.3920 - val_accuracy: 0.8864
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3935 - accuracy: 0.8870 - val_loss: 0.3828 - val_accuracy: 0.8932
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3827 - accuracy: 0.8909 - val_loss: 0.3761 - val_accuracy: 0.8954
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3755 - accuracy: 0.8934 - val_loss: 0.3812 - val_accuracy: 0.8948
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3697 - accuracy: 0.8951 - val_loss: 0.3779 - val_accuracy: 0.8930
18333/18333 [==============================] - 0s 20us/sample - loss: 0.4133 - accuracy: 0.8834
[CV] ...................................... n_neurons=5, total=  12.0s
[CV] n_neurons=5 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 41us/sample - loss: 1.0668 - accuracy: 0.6439 - val_loss: 0.6448 - val_accuracy: 0.8122
Epoch 2/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.5921 - accuracy: 0.8212 - val_loss: 0.5067 - val_accuracy: 0.8568
Epoch 3/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.5027 - accuracy: 0.8529 - val_loss: 0.4465 - val_accuracy: 0.8782
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.4572 - accuracy: 0.8676 - val_loss: 0.4171 - val_accuracy: 0.8856
Epoch 5/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.4323 - accuracy: 0.8769 - val_loss: 0.4034 - val_accuracy: 0.8894
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.4172 - accuracy: 0.8819 - val_loss: 0.3962 - val_accuracy: 0.8884
Epoch 7/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.4071 - accuracy: 0.8840 - val_loss: 0.3830 - val_accuracy: 0.8902
Epoch 8/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.3993 - accuracy: 0.8858 - val_loss: 0.3813 - val_accuracy: 0.8934
Epoch 9/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3932 - accuracy: 0.8878 - val_loss: 0.3816 - val_accuracy: 0.8920
Epoch 10/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.3890 - accuracy: 0.8895 - val_loss: 0.3753 - val_accuracy: 0.8962
Epoch 11/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3845 - accuracy: 0.8920 - val_loss: 0.3731 - val_accuracy: 0.8934
Epoch 12/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3810 - accuracy: 0.8923 - val_loss: 0.3722 - val_accuracy: 0.8946
Epoch 13/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.3773 - accuracy: 0.8931 - val_loss: 0.3767 - val_accuracy: 0.8916
Epoch 14/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.3737 - accuracy: 0.8936 - val_loss: 0.3684 - val_accuracy: 0.8960
Epoch 15/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3708 - accuracy: 0.8949 - val_loss: 0.3694 - val_accuracy: 0.8946
Epoch 16/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3690 - accuracy: 0.8953 - val_loss: 0.3660 - val_accuracy: 0.8946
Epoch 17/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3663 - accuracy: 0.8958 - val_loss: 0.3702 - val_accuracy: 0.8940
Epoch 18/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3643 - accuracy: 0.8968 - val_loss: 0.3662 - val_accuracy: 0.8946
18333/18333 [==============================] - 0s 21us/sample - loss: 0.3731 - accuracy: 0.8941
[CV] ...................................... n_neurons=5, total=  23.0s
[CV] n_neurons=6 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 42us/sample - loss: 1.0235 - accuracy: 0.6729 - val_loss: 0.5305 - val_accuracy: 0.8490
Epoch 2/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.5001 - accuracy: 0.8542 - val_loss: 0.4449 - val_accuracy: 0.8726
Epoch 3/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.4501 - accuracy: 0.8691 - val_loss: 0.4159 - val_accuracy: 0.8820
Epoch 4/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.4289 - accuracy: 0.8742 - val_loss: 0.4051 - val_accuracy: 0.8852
Epoch 5/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.4154 - accuracy: 0.8785 - val_loss: 0.3978 - val_accuracy: 0.8894
Epoch 6/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.4058 - accuracy: 0.8815 - val_loss: 0.3910 - val_accuracy: 0.8902
Epoch 7/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.3978 - accuracy: 0.8848 - val_loss: 0.3918 - val_accuracy: 0.8896
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.3731 - accuracy: 0.8941 - val_loss: 0.3463 - val_accuracy: 0.9036
Epoch 9/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.3396 - accuracy: 0.9027 - val_loss: 0.3375 - val_accuracy: 0.9036
Epoch 10/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3281 - accuracy: 0.9077 - val_loss: 0.3244 - val_accuracy: 0.9122
Epoch 11/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.3208 - accuracy: 0.9092 - val_loss: 0.3195 - val_accuracy: 0.9128
Epoch 12/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.3156 - accuracy: 0.9113 - val_loss: 0.3276 - val_accuracy: 0.9096
Epoch 13/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3121 - accuracy: 0.9117 - val_loss: 0.3192 - val_accuracy: 0.9096
Epoch 14/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3095 - accuracy: 0.9139 - val_loss: 0.3178 - val_accuracy: 0.9130
Epoch 15/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.3072 - accuracy: 0.9138 - val_loss: 0.3160 - val_accuracy: 0.9110
Epoch 16/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3048 - accuracy: 0.9146 - val_loss: 0.3109 - val_accuracy: 0.9110
Epoch 17/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.3038 - accuracy: 0.9154 - val_loss: 0.3160 - val_accuracy: 0.9132
Epoch 18/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3024 - accuracy: 0.9157 - val_loss: 0.3158 - val_accuracy: 0.9156
18334/18334 [==============================] - 1s 28us/sample - loss: 0.3466 - accuracy: 0.9087
[CV] ...................................... n_neurons=6, total=  23.7s
[CV] n_neurons=6 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.8545 - accuracy: 0.7522 - val_loss: 0.4649 - val_accuracy: 0.8650
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.4291 - accuracy: 0.8766 - val_loss: 0.3740 - val_accuracy: 0.8934
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.3760 - accuracy: 0.8943 - val_loss: 0.3510 - val_accuracy: 0.9040.
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3547 - accuracy: 0.9007 - val_loss: 0.3352 - val_accuracy: 0.9070
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.3425 - accuracy: 0.9038 - val_loss: 0.3284 - val_accuracy: 0.9128
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.3342 - accuracy: 0.9071 - val_loss: 0.3230 - val_accuracy: 0.9130
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3283 - accuracy: 0.9090 - val_loss: 0.3202 - val_accuracy: 0.9132
Epoch 8/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3240 - accuracy: 0.9092 - val_loss: 0.3253 - val_accuracy: 0.9128
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.3205 - accuracy: 0.9116 - val_loss: 0.3343 - val_accuracy: 0.9074
18333/18333 [==============================] - 0s 21us/sample - loss: 0.3614 - accuracy: 0.8992
[CV] ...................................... n_neurons=6, total=  13.1s
[CV] n_neurons=6 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 1.1482 - accuracy: 0.6331 - val_loss: 0.6535 - val_accuracy: 0.7960
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.5732 - accuracy: 0.8291 - val_loss: 0.4435 - val_accuracy: 0.8736
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.4413 - accuracy: 0.8714 - val_loss: 0.3896 - val_accuracy: 0.8880
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.4006 - accuracy: 0.8840 - val_loss: 0.3670 - val_accuracy: 0.8940
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3756 - accuracy: 0.8933 - val_loss: 0.3536 - val_accuracy: 0.9022
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3592 - accuracy: 0.8983 - val_loss: 0.3446 - val_accuracy: 0.9000
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3475 - accuracy: 0.9022 - val_loss: 0.3280 - val_accuracy: 0.9058
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3385 - accuracy: 0.9051 - val_loss: 0.3282 - val_accuracy: 0.9052
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3317 - accuracy: 0.9066 - val_loss: 0.3202 - val_accuracy: 0.9090
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3256 - accuracy: 0.9093 - val_loss: 0.3188 - val_accuracy: 0.9098
Epoch 11/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.3215 - accuracy: 0.9097 - val_loss: 0.3228 - val_accuracy: 0.9076
Epoch 12/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.3182 - accuracy: 0.9123 - val_loss: 0.3107 - val_accuracy: 0.9138
Epoch 13/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3152 - accuracy: 0.9133 - val_loss: 0.3113 - val_accuracy: 0.9116
Epoch 14/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3117 - accuracy: 0.9141 - val_loss: 0.3084 - val_accuracy: 0.9140
Epoch 15/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.3093 - accuracy: 0.9149 - val_loss: 0.3089 - val_accuracy: 0.9158
Epoch 16/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3083 - accuracy: 0.9150 - val_loss: 0.3105 - val_accuracy: 0.9158
18333/18333 [==============================] - 0s 21us/sample - loss: 0.3269 - accuracy: 0.9071
[CV] ...................................... n_neurons=6, total=  21.4s
[CV] n_neurons=7 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.8063 - accuracy: 0.7647 - val_loss: 0.4345 - val_accuracy: 0.8792
Epoch 2/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.4046 - accuracy: 0.8834 - val_loss: 0.3644 - val_accuracy: 0.8956
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.3583 - accuracy: 0.8954 - val_loss: 0.3375 - val_accuracy: 0.9038
Epoch 4/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.3384 - accuracy: 0.9032 - val_loss: 0.3250 - val_accuracy: 0.9082
Epoch 5/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.3258 - accuracy: 0.9068 - val_loss: 0.3185 - val_accuracy: 0.9112
Epoch 6/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.3171 - accuracy: 0.9084 - val_loss: 0.3193 - val_accuracy: 0.9034
Epoch 7/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.3112 - accuracy: 0.9110 - val_loss: 0.3121 - val_accuracy: 0.9112
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.3062 - accuracy: 0.9126 - val_loss: 0.3071 - val_accuracy: 0.9150
Epoch 9/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.3025 - accuracy: 0.9145 - val_loss: 0.3113 - val_accuracy: 0.9128
Epoch 10/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2993 - accuracy: 0.9150 - val_loss: 0.3088 - val_accuracy: 0.9170
18334/18334 [==============================] - 0s 21us/sample - loss: 0.3417 - accuracy: 0.9068
[CV] ...................................... n_neurons=7, total=  13.6s
[CV] n_neurons=7 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.7909 - accuracy: 0.7796 - val_loss: 0.4539 - val_accuracy: 0.8756
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.4096 - accuracy: 0.8861 - val_loss: 0.3696 - val_accuracy: 0.8960
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.3588 - accuracy: 0.8994 - val_loss: 0.3474 - val_accuracy: 0.9014
Epoch 4/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.3356 - accuracy: 0.9064 - val_loss: 0.3244 - val_accuracy: 0.9092
Epoch 5/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.3204 - accuracy: 0.9111 - val_loss: 0.3172 - val_accuracy: 0.9138
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.3091 - accuracy: 0.9148 - val_loss: 0.3117 - val_accuracy: 0.9148
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.3012 - accuracy: 0.9173 - val_loss: 0.3014 - val_accuracy: 0.9140
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2945 - accuracy: 0.9199 - val_loss: 0.3041 - val_accuracy: 0.9152
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2895 - accuracy: 0.9218 - val_loss: 0.3093 - val_accuracy: 0.9162
18333/18333 [==============================] - 0s 21us/sample - loss: 0.3313 - accuracy: 0.9081
[CV] ...................................... n_neurons=7, total=  13.3s
[CV] n_neurons=7 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.8007 - accuracy: 0.7544 - val_loss: 0.4413 - val_accuracy: 0.8752
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.4204 - accuracy: 0.8804 - val_loss: 0.3741 - val_accuracy: 0.8930
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3772 - accuracy: 0.8934 - val_loss: 0.3469 - val_accuracy: 0.9012
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.3569 - accuracy: 0.8997 - val_loss: 0.3356 - val_accuracy: 0.9032
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3444 - accuracy: 0.9042 - val_loss: 0.3302 - val_accuracy: 0.9114
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3352 - accuracy: 0.9077 - val_loss: 0.3348 - val_accuracy: 0.9084
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3270 - accuracy: 0.9091 - val_loss: 0.3183 - val_accuracy: 0.9128
Epoch 8/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.3195 - accuracy: 0.9108 - val_loss: 0.3239 - val_accuracy: 0.9136
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3134 - accuracy: 0.9134 - val_loss: 0.3196 - val_accuracy: 0.9126
18333/18333 [==============================] - 0s 21us/sample - loss: 0.3333 - accuracy: 0.9049
[CV] ...................................... n_neurons=7, total=  12.2s
[CV] n_neurons=8 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.7255 - accuracy: 0.7862 - val_loss: 0.3743 - val_accuracy: 0.8992
Epoch 2/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3543 - accuracy: 0.8976 - val_loss: 0.3164 - val_accuracy: 0.9128
Epoch 3/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3139 - accuracy: 0.9109 - val_loss: 0.2941 - val_accuracy: 0.9206
Epoch 4/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2959 - accuracy: 0.9158 - val_loss: 0.2864 - val_accuracy: 0.9196
Epoch 5/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2837 - accuracy: 0.9194 - val_loss: 0.2793 - val_accuracy: 0.9248
Epoch 6/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2753 - accuracy: 0.9225 - val_loss: 0.2760 - val_accuracy: 0.9262
Epoch 7/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2707 - accuracy: 0.9235 - val_loss: 0.2764 - val_accuracy: 0.9262
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2658 - accuracy: 0.9254 - val_loss: 0.2754 - val_accuracy: 0.9276
Epoch 9/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2624 - accuracy: 0.9264 - val_loss: 0.2756 - val_accuracy: 0.9208
Epoch 10/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2595 - accuracy: 0.9282 - val_loss: 0.2741 - val_accuracy: 0.9244
Epoch 11/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2570 - accuracy: 0.9278 - val_loss: 0.2757 - val_accuracy: 0.9248
Epoch 12/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2544 - accuracy: 0.9299 - val_loss: 0.2843 - val_accuracy: 0.9242
18334/18334 [==============================] - 0s 21us/sample - loss: 0.3098 - accuracy: 0.9159
[CV] ...................................... n_neurons=8, total=  15.7s
[CV] n_neurons=8 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.7030 - accuracy: 0.7953 - val_loss: 0.3730 - val_accuracy: 0.8926
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3465 - accuracy: 0.9029 - val_loss: 0.3122 - val_accuracy: 0.9098
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3077 - accuracy: 0.9144 - val_loss: 0.2983 - val_accuracy: 0.9158
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2916 - accuracy: 0.9199 - val_loss: 0.2850 - val_accuracy: 0.9184
Epoch 5/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2813 - accuracy: 0.9230 - val_loss: 0.2834 - val_accuracy: 0.9200
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2744 - accuracy: 0.9247 - val_loss: 0.2811 - val_accuracy: 0.9202
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2682 - accuracy: 0.9268 - val_loss: 0.2756 - val_accuracy: 0.9214
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2627 - accuracy: 0.9289 - val_loss: 0.2809 - val_accuracy: 0.9208
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2590 - accuracy: 0.9293 - val_loss: 0.2788 - val_accuracy: 0.9206
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2995 - accuracy: 0.9200
[CV] ...................................... n_neurons=8, total=  12.1s
[CV] n_neurons=8 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.6583 - accuracy: 0.8097 - val_loss: 0.3702 - val_accuracy: 0.8966
Epoch 2/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3582 - accuracy: 0.8983 - val_loss: 0.3229 - val_accuracy: 0.9096
Epoch 3/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3259 - accuracy: 0.9065 - val_loss: 0.2990 - val_accuracy: 0.9172
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3086 - accuracy: 0.9113 - val_loss: 0.2893 - val_accuracy: 0.9188
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2965 - accuracy: 0.9153 - val_loss: 0.2835 - val_accuracy: 0.9234
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2871 - accuracy: 0.9175 - val_loss: 0.2904 - val_accuracy: 0.9198
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2796 - accuracy: 0.9216 - val_loss: 0.2730 - val_accuracy: 0.9248
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2732 - accuracy: 0.9226 - val_loss: 0.2767 - val_accuracy: 0.9228
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2676 - accuracy: 0.9252 - val_loss: 0.2725 - val_accuracy: 0.9226
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2637 - accuracy: 0.9262 - val_loss: 0.2729 - val_accuracy: 0.9268
Epoch 11/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2594 - accuracy: 0.9276 - val_loss: 0.2712 - val_accuracy: 0.9254
Epoch 12/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2561 - accuracy: 0.9277 - val_loss: 0.2707 - val_accuracy: 0.9260
Epoch 13/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2530 - accuracy: 0.9296 - val_loss: 0.2724 - val_accuracy: 0.9226
Epoch 14/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2506 - accuracy: 0.9303 - val_loss: 0.2689 - val_accuracy: 0.9252
Epoch 15/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2487 - accuracy: 0.9304 - val_loss: 0.2657 - val_accuracy: 0.9236
Epoch 16/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2462 - accuracy: 0.9317 - val_loss: 0.2718 - val_accuracy: 0.9234
Epoch 17/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2441 - accuracy: 0.9314 - val_loss: 0.2655 - val_accuracy: 0.9252
Epoch 18/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2425 - accuracy: 0.9323 - val_loss: 0.2708 - val_accuracy: 0.9238
Epoch 19/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.2407 - accuracy: 0.9325 - val_loss: 0.2766 - val_accuracy: 0.9234
18333/18333 [==============================] - 0s 22us/sample - loss: 0.2767 - accuracy: 0.9254
[CV] ...................................... n_neurons=8, total=  24.4s
[CV] n_neurons=9 .....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.6831 - accuracy: 0.8069 - val_loss: 0.3692 - val_accuracy: 0.9028
Epoch 2/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3488 - accuracy: 0.9028 - val_loss: 0.3148 - val_accuracy: 0.9158
Epoch 3/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.3124 - accuracy: 0.9127 - val_loss: 0.2949 - val_accuracy: 0.9208
Epoch 4/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2971 - accuracy: 0.9163 - val_loss: 0.2887 - val_accuracy: 0.9214
Epoch 5/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2874 - accuracy: 0.9196 - val_loss: 0.2883 - val_accuracy: 0.9250
Epoch 6/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2812 - accuracy: 0.9215 - val_loss: 0.2842 - val_accuracy: 0.9236
Epoch 7/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2771 - accuracy: 0.9228 - val_loss: 0.2827 - val_accuracy: 0.9256
Epoch 8/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2726 - accuracy: 0.9247 - val_loss: 0.2834 - val_accuracy: 0.9242
Epoch 9/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2692 - accuracy: 0.9263 - val_loss: 0.2803 - val_accuracy: 0.9250
Epoch 10/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2665 - accuracy: 0.9272 - val_loss: 0.2804 - val_accuracy: 0.9242
Epoch 11/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2633 - accuracy: 0.9273 - val_loss: 0.2868 - val_accuracy: 0.9260
18334/18334 [==============================] - 0s 21us/sample - loss: 0.3060 - accuracy: 0.9156
[CV] ...................................... n_neurons=9, total=  14.7s
[CV] n_neurons=9 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.6569 - accuracy: 0.8167 - val_loss: 0.3610 - val_accuracy: 0.8964
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3411 - accuracy: 0.9051 - val_loss: 0.3132 - val_accuracy: 0.9102
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3105 - accuracy: 0.9143 - val_loss: 0.2981 - val_accuracy: 0.9174
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2952 - accuracy: 0.9182 - val_loss: 0.2883 - val_accuracy: 0.9212
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2843 - accuracy: 0.9215 - val_loss: 0.2872 - val_accuracy: 0.9220
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2769 - accuracy: 0.9246 - val_loss: 0.2843 - val_accuracy: 0.9216
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2709 - accuracy: 0.9260 - val_loss: 0.2821 - val_accuracy: 0.9216
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2656 - accuracy: 0.9280 - val_loss: 0.2884 - val_accuracy: 0.9210
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2623 - accuracy: 0.9285 - val_loss: 0.2904 - val_accuracy: 0.9210
18333/18333 [==============================] - 0s 21us/sample - loss: 0.3106 - accuracy: 0.9168
[CV] ...................................... n_neurons=9, total=  12.3s
[CV] n_neurons=9 .....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.7171 - accuracy: 0.7907 - val_loss: 0.3762 - val_accuracy: 0.8964
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.3658 - accuracy: 0.8973 - val_loss: 0.3279 - val_accuracy: 0.9070
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.3315 - accuracy: 0.9062 - val_loss: 0.3042 - val_accuracy: 0.9166
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3089 - accuracy: 0.9125 - val_loss: 0.2956 - val_accuracy: 0.9150
Epoch 5/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.2924 - accuracy: 0.9172 - val_loss: 0.2799 - val_accuracy: 0.9218
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2811 - accuracy: 0.9205 - val_loss: 0.2867 - val_accuracy: 0.9194
Epoch 7/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.2728 - accuracy: 0.9242 - val_loss: 0.2695 - val_accuracy: 0.9266
Epoch 8/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2667 - accuracy: 0.9253 - val_loss: 0.2738 - val_accuracy: 0.9244
Epoch 9/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.2623 - accuracy: 0.9263 - val_loss: 0.2707 - val_accuracy: 0.9282
18333/18333 [==============================] - 0s 22us/sample - loss: 0.2837 - accuracy: 0.9219
[CV] ...................................... n_neurons=9, total=  12.2s
[CV] n_neurons=10 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.6613 - accuracy: 0.8142 - val_loss: 0.3567 - val_accuracy: 0.9056
Epoch 2/30
36666/36666 [==============================] - 1s 32us/sample - loss: 0.3329 - accuracy: 0.9056 - val_loss: 0.2941 - val_accuracy: 0.9190
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2864 - accuracy: 0.9183 - val_loss: 0.2649 - val_accuracy: 0.9262
Epoch 4/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2648 - accuracy: 0.9243 - val_loss: 0.2549 - val_accuracy: 0.9292
Epoch 5/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2519 - accuracy: 0.9273 - val_loss: 0.2528 - val_accuracy: 0.9288
Epoch 6/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2436 - accuracy: 0.9296 - val_loss: 0.2498 - val_accuracy: 0.9318
Epoch 7/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2372 - accuracy: 0.9323 - val_loss: 0.2494 - val_accuracy: 0.9330
Epoch 8/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2318 - accuracy: 0.9344 - val_loss: 0.2518 - val_accuracy: 0.9308
Epoch 9/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2277 - accuracy: 0.9351 - val_loss: 0.2476 - val_accuracy: 0.9320
Epoch 10/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2242 - accuracy: 0.9367 - val_loss: 0.2525 - val_accuracy: 0.9320
Epoch 11/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2205 - accuracy: 0.9368 - val_loss: 0.2492 - val_accuracy: 0.9324
18334/18334 [==============================] - 0s 20us/sample - loss: 0.2721 - accuracy: 0.9243
[CV] ..................................... n_neurons=10, total=  14.7s
[CV] n_neurons=10 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.6596 - accuracy: 0.8145 - val_loss: 0.3482 - val_accuracy: 0.9028
Epoch 2/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3295 - accuracy: 0.9076 - val_loss: 0.2918 - val_accuracy: 0.9160
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2908 - accuracy: 0.9193 - val_loss: 0.2736 - val_accuracy: 0.9218
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2713 - accuracy: 0.9239 - val_loss: 0.2649 - val_accuracy: 0.9236
Epoch 5/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2568 - accuracy: 0.9288 - val_loss: 0.2597 - val_accuracy: 0.9250
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2453 - accuracy: 0.9315 - val_loss: 0.2481 - val_accuracy: 0.9292
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2345 - accuracy: 0.9341 - val_loss: 0.2410 - val_accuracy: 0.9304
Epoch 8/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2261 - accuracy: 0.9365 - val_loss: 0.2500 - val_accuracy: 0.9328
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2195 - accuracy: 0.9383 - val_loss: 0.2451 - val_accuracy: 0.9322
18333/18333 [==============================] - 0s 23us/sample - loss: 0.2681 - accuracy: 0.9239
[CV] ..................................... n_neurons=10, total=  12.0s
[CV] n_neurons=10 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.6630 - accuracy: 0.8049 - val_loss: 0.3358 - val_accuracy: 0.9024
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.3295 - accuracy: 0.9053 - val_loss: 0.2862 - val_accuracy: 0.9168
Epoch 3/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.2930 - accuracy: 0.9148 - val_loss: 0.2662 - val_accuracy: 0.9240
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2726 - accuracy: 0.9216 - val_loss: 0.2556 - val_accuracy: 0.9258
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2585 - accuracy: 0.9263 - val_loss: 0.2478 - val_accuracy: 0.9286
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2474 - accuracy: 0.9298 - val_loss: 0.2482 - val_accuracy: 0.9282
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2393 - accuracy: 0.9314 - val_loss: 0.2369 - val_accuracy: 0.9324
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2323 - accuracy: 0.9340 - val_loss: 0.2409 - val_accuracy: 0.9308
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2278 - accuracy: 0.9345 - val_loss: 0.2357 - val_accuracy: 0.9338
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2235 - accuracy: 0.9369 - val_loss: 0.2363 - val_accuracy: 0.9352
Epoch 11/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2198 - accuracy: 0.9374 - val_loss: 0.2428 - val_accuracy: 0.9316
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2521 - accuracy: 0.9289
[CV] ..................................... n_neurons=10, total=  15.5s
[CV] n_neurons=11 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.5785 - accuracy: 0.8375 - val_loss: 0.3131 - val_accuracy: 0.9134
Epoch 2/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.3123 - accuracy: 0.9115 - val_loss: 0.2797 - val_accuracy: 0.9216
Epoch 3/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2841 - accuracy: 0.9180 - val_loss: 0.2607 - val_accuracy: 0.9244
Epoch 4/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2695 - accuracy: 0.9229 - val_loss: 0.2560 - val_accuracy: 0.9286
Epoch 5/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2589 - accuracy: 0.9257 - val_loss: 0.2517 - val_accuracy: 0.9292
Epoch 6/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2516 - accuracy: 0.9286 - val_loss: 0.2497 - val_accuracy: 0.9276
Epoch 7/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2463 - accuracy: 0.9296 - val_loss: 0.2477 - val_accuracy: 0.9294
Epoch 8/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2413 - accuracy: 0.9322 - val_loss: 0.2466 - val_accuracy: 0.9314
Epoch 9/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2368 - accuracy: 0.9335 - val_loss: 0.2419 - val_accuracy: 0.9304
Epoch 10/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2333 - accuracy: 0.9352 - val_loss: 0.2442 - val_accuracy: 0.9316
Epoch 11/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2301 - accuracy: 0.9354 - val_loss: 0.2524 - val_accuracy: 0.9280
18334/18334 [==============================] - 0s 22us/sample - loss: 0.2735 - accuracy: 0.9231
[CV] ..................................... n_neurons=11, total=  14.5s
[CV] n_neurons=11 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.5395 - accuracy: 0.8530 - val_loss: 0.3187 - val_accuracy: 0.9064
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.3082 - accuracy: 0.9134 - val_loss: 0.2816 - val_accuracy: 0.9166
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2809 - accuracy: 0.9207 - val_loss: 0.2653 - val_accuracy: 0.9264
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2661 - accuracy: 0.9262 - val_loss: 0.2570 - val_accuracy: 0.9260
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2546 - accuracy: 0.9292 - val_loss: 0.2576 - val_accuracy: 0.9276
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2458 - accuracy: 0.9317 - val_loss: 0.2451 - val_accuracy: 0.9322
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2378 - accuracy: 0.9341 - val_loss: 0.2390 - val_accuracy: 0.9330
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2311 - accuracy: 0.9367 - val_loss: 0.2415 - val_accuracy: 0.9332
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2262 - accuracy: 0.9381 - val_loss: 0.2378 - val_accuracy: 0.9354
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2203 - accuracy: 0.9400 - val_loss: 0.2431 - val_accuracy: 0.9350
Epoch 11/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2165 - accuracy: 0.9411 - val_loss: 0.2378 - val_accuracy: 0.9348
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2701 - accuracy: 0.9275
[CV] ..................................... n_neurons=11, total=  15.5s
[CV] n_neurons=11 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.6178 - accuracy: 0.8258 - val_loss: 0.3170 - val_accuracy: 0.9118
Epoch 2/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.3101 - accuracy: 0.9121 - val_loss: 0.2726 - val_accuracy: 0.9206
Epoch 3/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2805 - accuracy: 0.9211 - val_loss: 0.2531 - val_accuracy: 0.9324
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2638 - accuracy: 0.9261 - val_loss: 0.2524 - val_accuracy: 0.9308
Epoch 5/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2517 - accuracy: 0.9296 - val_loss: 0.2454 - val_accuracy: 0.9348
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2418 - accuracy: 0.9324 - val_loss: 0.2572 - val_accuracy: 0.9306
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2343 - accuracy: 0.9350 - val_loss: 0.2308 - val_accuracy: 0.9374
Epoch 8/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2270 - accuracy: 0.9374 - val_loss: 0.2360 - val_accuracy: 0.9358
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2226 - accuracy: 0.9391 - val_loss: 0.2342 - val_accuracy: 0.9378
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2473 - accuracy: 0.9324
[CV] ..................................... n_neurons=11, total=  11.8s
[CV] n_neurons=12 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.5908 - accuracy: 0.8326 - val_loss: 0.3126 - val_accuracy: 0.9142
Epoch 2/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.3081 - accuracy: 0.9118 - val_loss: 0.2757 - val_accuracy: 0.9214
Epoch 3/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2799 - accuracy: 0.9206 - val_loss: 0.2584 - val_accuracy: 0.9276
Epoch 4/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2659 - accuracy: 0.9237 - val_loss: 0.2521 - val_accuracy: 0.9276
Epoch 5/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2550 - accuracy: 0.9277 - val_loss: 0.2480 - val_accuracy: 0.9306
Epoch 6/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2473 - accuracy: 0.9288 - val_loss: 0.2456 - val_accuracy: 0.9296
Epoch 7/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2399 - accuracy: 0.9321 - val_loss: 0.2461 - val_accuracy: 0.9292
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2334 - accuracy: 0.9342 - val_loss: 0.2397 - val_accuracy: 0.9340
Epoch 9/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.2278 - accuracy: 0.9349 - val_loss: 0.2385 - val_accuracy: 0.9338
Epoch 10/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.2238 - accuracy: 0.9372 - val_loss: 0.2415 - val_accuracy: 0.9326
Epoch 11/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.2181 - accuracy: 0.9371 - val_loss: 0.2455 - val_accuracy: 0.9326
18334/18334 [==============================] - 0s 22us/sample - loss: 0.2692 - accuracy: 0.9242
[CV] ..................................... n_neurons=12, total=  16.0s
[CV] n_neurons=12 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.5527 - accuracy: 0.8480 - val_loss: 0.3162 - val_accuracy: 0.9088
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.3125 - accuracy: 0.9125 - val_loss: 0.2832 - val_accuracy: 0.9190
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2797 - accuracy: 0.9222 - val_loss: 0.2667 - val_accuracy: 0.9256
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2582 - accuracy: 0.9287 - val_loss: 0.2512 - val_accuracy: 0.9268
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2427 - accuracy: 0.9326 - val_loss: 0.2533 - val_accuracy: 0.9296
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2328 - accuracy: 0.9345 - val_loss: 0.2424 - val_accuracy: 0.9334
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2240 - accuracy: 0.9373 - val_loss: 0.2338 - val_accuracy: 0.9338
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2178 - accuracy: 0.9387 - val_loss: 0.2437 - val_accuracy: 0.9342
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2127 - accuracy: 0.9404 - val_loss: 0.2466 - val_accuracy: 0.9324
18333/18333 [==============================] - 0s 22us/sample - loss: 0.2633 - accuracy: 0.9263
[CV] ..................................... n_neurons=12, total=  12.3s
[CV] n_neurons=12 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.5413 - accuracy: 0.8499 - val_loss: 0.2997 - val_accuracy: 0.9146
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2963 - accuracy: 0.9151 - val_loss: 0.2602 - val_accuracy: 0.9246
Epoch 3/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2659 - accuracy: 0.9250 - val_loss: 0.2379 - val_accuracy: 0.9334
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2493 - accuracy: 0.9292 - val_loss: 0.2326 - val_accuracy: 0.9342
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2392 - accuracy: 0.9328 - val_loss: 0.2264 - val_accuracy: 0.9372
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2313 - accuracy: 0.9353 - val_loss: 0.2298 - val_accuracy: 0.9326
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2251 - accuracy: 0.9367 - val_loss: 0.2188 - val_accuracy: 0.9354
Epoch 8/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2197 - accuracy: 0.9383 - val_loss: 0.2232 - val_accuracy: 0.9364
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2168 - accuracy: 0.9396 - val_loss: 0.2251 - val_accuracy: 0.9370
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2479 - accuracy: 0.9321
[CV] ..................................... n_neurons=12, total=  12.3s
[CV] n_neurons=13 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.5945 - accuracy: 0.8364 - val_loss: 0.3157 - val_accuracy: 0.9150
Epoch 2/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2984 - accuracy: 0.9157 - val_loss: 0.2722 - val_accuracy: 0.9266
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2624 - accuracy: 0.9261 - val_loss: 0.2536 - val_accuracy: 0.9308
Epoch 4/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2419 - accuracy: 0.9310 - val_loss: 0.2440 - val_accuracy: 0.9318
Epoch 5/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2265 - accuracy: 0.9356 - val_loss: 0.2352 - val_accuracy: 0.9360
Epoch 6/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.2148 - accuracy: 0.9392 - val_loss: 0.2296 - val_accuracy: 0.9354
Epoch 7/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.2062 - accuracy: 0.9414 - val_loss: 0.2271 - val_accuracy: 0.9362
Epoch 8/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1988 - accuracy: 0.9438 - val_loss: 0.2276 - val_accuracy: 0.9386
Epoch 9/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1925 - accuracy: 0.9446 - val_loss: 0.2202 - val_accuracy: 0.9388
Epoch 10/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1884 - accuracy: 0.9469 - val_loss: 0.2182 - val_accuracy: 0.9402
Epoch 11/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1835 - accuracy: 0.9476 - val_loss: 0.2261 - val_accuracy: 0.9384
Epoch 12/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1800 - accuracy: 0.9486 - val_loss: 0.2205 - val_accuracy: 0.9420
18334/18334 [==============================] - 0s 21us/sample - loss: 0.2316 - accuracy: 0.9359
[CV] ..................................... n_neurons=13, total=  17.0s
[CV] n_neurons=13 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.5250 - accuracy: 0.8547 - val_loss: 0.2993 - val_accuracy: 0.9148
Epoch 2/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2892 - accuracy: 0.9189 - val_loss: 0.2674 - val_accuracy: 0.9226
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2601 - accuracy: 0.9266 - val_loss: 0.2494 - val_accuracy: 0.9316
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2420 - accuracy: 0.9329 - val_loss: 0.2368 - val_accuracy: 0.9318
Epoch 5/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2279 - accuracy: 0.9375 - val_loss: 0.2299 - val_accuracy: 0.9390
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2175 - accuracy: 0.9406 - val_loss: 0.2270 - val_accuracy: 0.9394
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2083 - accuracy: 0.9419 - val_loss: 0.2189 - val_accuracy: 0.9372
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2021 - accuracy: 0.9443 - val_loss: 0.2178 - val_accuracy: 0.9404
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1965 - accuracy: 0.9462 - val_loss: 0.2224 - val_accuracy: 0.9382
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1923 - accuracy: 0.9465 - val_loss: 0.2172 - val_accuracy: 0.9424
Epoch 11/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1874 - accuracy: 0.9484 - val_loss: 0.2095 - val_accuracy: 0.9442
Epoch 12/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1836 - accuracy: 0.9500 - val_loss: 0.2158 - val_accuracy: 0.9404
Epoch 13/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1814 - accuracy: 0.9502 - val_loss: 0.2180 - val_accuracy: 0.9412
18333/18333 [==============================] - 0s 25us/sample - loss: 0.2347 - accuracy: 0.9363
[CV] ..................................... n_neurons=13, total=  17.3s
[CV] n_neurons=13 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.5813 - accuracy: 0.8402 - val_loss: 0.2994 - val_accuracy: 0.9150
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3010 - accuracy: 0.9135 - val_loss: 0.2578 - val_accuracy: 0.9268
Epoch 3/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2660 - accuracy: 0.9241 - val_loss: 0.2366 - val_accuracy: 0.9320
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2451 - accuracy: 0.9302 - val_loss: 0.2262 - val_accuracy: 0.9358
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2303 - accuracy: 0.9342 - val_loss: 0.2212 - val_accuracy: 0.9384
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2194 - accuracy: 0.9375 - val_loss: 0.2240 - val_accuracy: 0.9370
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2101 - accuracy: 0.9408 - val_loss: 0.2095 - val_accuracy: 0.9418
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2035 - accuracy: 0.9422 - val_loss: 0.2094 - val_accuracy: 0.9418
Epoch 9/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1980 - accuracy: 0.9432 - val_loss: 0.2069 - val_accuracy: 0.9418
Epoch 10/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1929 - accuracy: 0.9460 - val_loss: 0.2028 - val_accuracy: 0.9456
Epoch 11/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1884 - accuracy: 0.9480 - val_loss: 0.2059 - val_accuracy: 0.9432
Epoch 12/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.1843 - accuracy: 0.9486 - val_loss: 0.2004 - val_accuracy: 0.9466
Epoch 13/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1802 - accuracy: 0.9496 - val_loss: 0.1995 - val_accuracy: 0.9444
Epoch 14/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1765 - accuracy: 0.9510 - val_loss: 0.1995 - val_accuracy: 0.9454
Epoch 15/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1733 - accuracy: 0.9527 - val_loss: 0.1988 - val_accuracy: 0.9456
Epoch 16/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1712 - accuracy: 0.9524 - val_loss: 0.1973 - val_accuracy: 0.9454
Epoch 17/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1678 - accuracy: 0.9537 - val_loss: 0.1938 - val_accuracy: 0.9462
Epoch 18/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1649 - accuracy: 0.9543 - val_loss: 0.1944 - val_accuracy: 0.9480
Epoch 19/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1631 - accuracy: 0.9547 - val_loss: 0.2014 - val_accuracy: 0.9464
18333/18333 [==============================] - 0s 22us/sample - loss: 0.2199 - accuracy: 0.9404
[CV] ..................................... n_neurons=13, total=  25.2s
[CV] n_neurons=14 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.5167 - accuracy: 0.8606 - val_loss: 0.2926 - val_accuracy: 0.9182
Epoch 2/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2889 - accuracy: 0.9167 - val_loss: 0.2596 - val_accuracy: 0.9290
Epoch 3/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2576 - accuracy: 0.9262 - val_loss: 0.2375 - val_accuracy: 0.9330
Epoch 4/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2411 - accuracy: 0.9310 - val_loss: 0.2346 - val_accuracy: 0.9318
Epoch 5/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2300 - accuracy: 0.9340 - val_loss: 0.2243 - val_accuracy: 0.9372
Epoch 6/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2220 - accuracy: 0.9365 - val_loss: 0.2293 - val_accuracy: 0.9348
Epoch 7/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2156 - accuracy: 0.9386 - val_loss: 0.2222 - val_accuracy: 0.9356
Epoch 8/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2101 - accuracy: 0.9417 - val_loss: 0.2249 - val_accuracy: 0.9390
Epoch 9/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2043 - accuracy: 0.9410 - val_loss: 0.2166 - val_accuracy: 0.9356
Epoch 10/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2004 - accuracy: 0.9440 - val_loss: 0.2147 - val_accuracy: 0.9370
Epoch 11/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1959 - accuracy: 0.9441 - val_loss: 0.2292 - val_accuracy: 0.9350
Epoch 12/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1921 - accuracy: 0.9458 - val_loss: 0.2144 - val_accuracy: 0.9396
Epoch 13/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1895 - accuracy: 0.9462 - val_loss: 0.2146 - val_accuracy: 0.9402
Epoch 14/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1869 - accuracy: 0.9464 - val_loss: 0.2180 - val_accuracy: 0.9370
18334/18334 [==============================] - 0s 22us/sample - loss: 0.2487 - accuracy: 0.9308
[CV] ..................................... n_neurons=14, total=  18.0s
[CV] n_neurons=14 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.5333 - accuracy: 0.8536 - val_loss: 0.3197 - val_accuracy: 0.9094
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3025 - accuracy: 0.9158 - val_loss: 0.2741 - val_accuracy: 0.9216
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2712 - accuracy: 0.9247 - val_loss: 0.2532 - val_accuracy: 0.9280
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2507 - accuracy: 0.9310 - val_loss: 0.2375 - val_accuracy: 0.9342
Epoch 5/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2331 - accuracy: 0.9356 - val_loss: 0.2363 - val_accuracy: 0.9350
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2197 - accuracy: 0.9386 - val_loss: 0.2220 - val_accuracy: 0.9368
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2081 - accuracy: 0.9420 - val_loss: 0.2166 - val_accuracy: 0.9384
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1989 - accuracy: 0.9441 - val_loss: 0.2171 - val_accuracy: 0.9410
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1916 - accuracy: 0.9458 - val_loss: 0.2146 - val_accuracy: 0.9416
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1858 - accuracy: 0.9482 - val_loss: 0.2128 - val_accuracy: 0.9420
Epoch 11/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1808 - accuracy: 0.9489 - val_loss: 0.2062 - val_accuracy: 0.9438
Epoch 12/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1757 - accuracy: 0.9508 - val_loss: 0.2114 - val_accuracy: 0.9428
Epoch 13/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1717 - accuracy: 0.9517 - val_loss: 0.2113 - val_accuracy: 0.9418
18333/18333 [==============================] - 0s 22us/sample - loss: 0.2326 - accuracy: 0.9369
[CV] ..................................... n_neurons=14, total=  17.0s
[CV] n_neurons=14 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.5364 - accuracy: 0.8529 - val_loss: 0.3109 - val_accuracy: 0.9144
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.3006 - accuracy: 0.9135 - val_loss: 0.2685 - val_accuracy: 0.9228
Epoch 3/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2670 - accuracy: 0.9238 - val_loss: 0.2479 - val_accuracy: 0.9322
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2479 - accuracy: 0.9289 - val_loss: 0.2446 - val_accuracy: 0.9336
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2328 - accuracy: 0.9323 - val_loss: 0.2296 - val_accuracy: 0.9374
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2200 - accuracy: 0.9372 - val_loss: 0.2356 - val_accuracy: 0.9324
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2090 - accuracy: 0.9405 - val_loss: 0.2176 - val_accuracy: 0.9362
Epoch 8/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2003 - accuracy: 0.9420 - val_loss: 0.2110 - val_accuracy: 0.9412
Epoch 9/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1934 - accuracy: 0.9446 - val_loss: 0.2062 - val_accuracy: 0.9430
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1873 - accuracy: 0.9465 - val_loss: 0.2061 - val_accuracy: 0.9422
Epoch 11/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1810 - accuracy: 0.9483 - val_loss: 0.2029 - val_accuracy: 0.9452
Epoch 12/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1770 - accuracy: 0.9494 - val_loss: 0.1942 - val_accuracy: 0.9462
Epoch 13/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1735 - accuracy: 0.9504 - val_loss: 0.1919 - val_accuracy: 0.9480
Epoch 14/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1690 - accuracy: 0.9520 - val_loss: 0.1966 - val_accuracy: 0.9452
Epoch 15/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1666 - accuracy: 0.9533 - val_loss: 0.1973 - val_accuracy: 0.9452
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2202 - accuracy: 0.9396
[CV] ..................................... n_neurons=14, total=  19.2s
[CV] n_neurons=15 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.5089 - accuracy: 0.8583 - val_loss: 0.3024 - val_accuracy: 0.9192
Epoch 2/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2941 - accuracy: 0.9164 - val_loss: 0.2677 - val_accuracy: 0.9238
Epoch 3/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2613 - accuracy: 0.9259 - val_loss: 0.2487 - val_accuracy: 0.9316
Epoch 4/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2420 - accuracy: 0.9321 - val_loss: 0.2397 - val_accuracy: 0.9312
Epoch 5/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2269 - accuracy: 0.9355 - val_loss: 0.2284 - val_accuracy: 0.9344
Epoch 6/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2167 - accuracy: 0.9386 - val_loss: 0.2236 - val_accuracy: 0.9356
Epoch 7/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2075 - accuracy: 0.9419 - val_loss: 0.2167 - val_accuracy: 0.9390
Epoch 8/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1992 - accuracy: 0.9441 - val_loss: 0.2145 - val_accuracy: 0.9408
Epoch 9/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1920 - accuracy: 0.9457 - val_loss: 0.2090 - val_accuracy: 0.9398
Epoch 10/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1860 - accuracy: 0.9477 - val_loss: 0.2085 - val_accuracy: 0.9418
Epoch 11/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1795 - accuracy: 0.9506 - val_loss: 0.2113 - val_accuracy: 0.9436
Epoch 12/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1748 - accuracy: 0.9515 - val_loss: 0.2118 - val_accuracy: 0.9428
18334/18334 [==============================] - 0s 22us/sample - loss: 0.2327 - accuracy: 0.9363
[CV] ..................................... n_neurons=15, total=  15.6s
[CV] n_neurons=15 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.4914 - accuracy: 0.8645 - val_loss: 0.2928 - val_accuracy: 0.9160
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2775 - accuracy: 0.9211 - val_loss: 0.2500 - val_accuracy: 0.9270
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2438 - accuracy: 0.9310 - val_loss: 0.2331 - val_accuracy: 0.9328
Epoch 4/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2227 - accuracy: 0.9370 - val_loss: 0.2173 - val_accuracy: 0.9374
Epoch 5/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2078 - accuracy: 0.9414 - val_loss: 0.2177 - val_accuracy: 0.9376
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1975 - accuracy: 0.9440 - val_loss: 0.2046 - val_accuracy: 0.9448
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1889 - accuracy: 0.9451 - val_loss: 0.2012 - val_accuracy: 0.9454
Epoch 8/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1820 - accuracy: 0.9494 - val_loss: 0.1950 - val_accuracy: 0.9470
Epoch 9/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1766 - accuracy: 0.9513 - val_loss: 0.1976 - val_accuracy: 0.9462
Epoch 10/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1716 - accuracy: 0.9522 - val_loss: 0.1980 - val_accuracy: 0.9466
18333/18333 [==============================] - 0s 23us/sample - loss: 0.2278 - accuracy: 0.9365
[CV] ..................................... n_neurons=15, total=  13.2s
[CV] n_neurons=15 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.5268 - accuracy: 0.8560 - val_loss: 0.2954 - val_accuracy: 0.9166
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2907 - accuracy: 0.9179 - val_loss: 0.2542 - val_accuracy: 0.9266
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2593 - accuracy: 0.9249 - val_loss: 0.2350 - val_accuracy: 0.9330
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2377 - accuracy: 0.9309 - val_loss: 0.2330 - val_accuracy: 0.9326
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2227 - accuracy: 0.9366 - val_loss: 0.2172 - val_accuracy: 0.9396
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2122 - accuracy: 0.9392 - val_loss: 0.2292 - val_accuracy: 0.9348
Epoch 7/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2028 - accuracy: 0.9428 - val_loss: 0.2115 - val_accuracy: 0.9398
Epoch 8/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1947 - accuracy: 0.9448 - val_loss: 0.2119 - val_accuracy: 0.9412
Epoch 9/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1893 - accuracy: 0.9462 - val_loss: 0.2092 - val_accuracy: 0.9414
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1840 - accuracy: 0.9479 - val_loss: 0.2048 - val_accuracy: 0.9412
Epoch 11/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1782 - accuracy: 0.9499 - val_loss: 0.2056 - val_accuracy: 0.9410
Epoch 12/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1747 - accuracy: 0.9510 - val_loss: 0.2058 - val_accuracy: 0.9408
18333/18333 [==============================] - 0s 22us/sample - loss: 0.2198 - accuracy: 0.9401
[CV] ..................................... n_neurons=15, total=  15.7s
[CV] n_neurons=16 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.4822 - accuracy: 0.8685 - val_loss: 0.2832 - val_accuracy: 0.9214
Epoch 2/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.2715 - accuracy: 0.9213 - val_loss: 0.2340 - val_accuracy: 0.9314
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2326 - accuracy: 0.9329 - val_loss: 0.2112 - val_accuracy: 0.9382
Epoch 4/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2088 - accuracy: 0.9401 - val_loss: 0.1978 - val_accuracy: 0.9400
Epoch 5/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1920 - accuracy: 0.9439 - val_loss: 0.1899 - val_accuracy: 0.9438
Epoch 6/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1806 - accuracy: 0.9480 - val_loss: 0.1813 - val_accuracy: 0.9484
Epoch 7/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1730 - accuracy: 0.9500 - val_loss: 0.1754 - val_accuracy: 0.9492
Epoch 8/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1657 - accuracy: 0.9526 - val_loss: 0.1786 - val_accuracy: 0.9504
Epoch 9/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1593 - accuracy: 0.9538 - val_loss: 0.1706 - val_accuracy: 0.9516
Epoch 10/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1550 - accuracy: 0.9549 - val_loss: 0.1702 - val_accuracy: 0.9524
Epoch 11/30
36666/36666 [==============================] - 1s 33us/sample - loss: 0.1506 - accuracy: 0.9562 - val_loss: 0.1767 - val_accuracy: 0.9498
Epoch 12/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1461 - accuracy: 0.9584 - val_loss: 0.1751 - val_accuracy: 0.9512
18334/18334 [==============================] - 0s 21us/sample - loss: 0.2030 - accuracy: 0.9448
[CV] ..................................... n_neurons=16, total=  15.5s
[CV] n_neurons=16 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.5275 - accuracy: 0.8555 - val_loss: 0.3033 - val_accuracy: 0.9108
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2951 - accuracy: 0.9165 - val_loss: 0.2687 - val_accuracy: 0.9246
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2683 - accuracy: 0.9242 - val_loss: 0.2523 - val_accuracy: 0.9310
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2515 - accuracy: 0.9301 - val_loss: 0.2403 - val_accuracy: 0.9306
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2393 - accuracy: 0.9343 - val_loss: 0.2421 - val_accuracy: 0.9326
Epoch 6/30
36667/36667 [==============================] - 1s 32us/sample - loss: 0.2299 - accuracy: 0.9357 - val_loss: 0.2331 - val_accuracy: 0.9342
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2218 - accuracy: 0.9387 - val_loss: 0.2300 - val_accuracy: 0.9372
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2153 - accuracy: 0.9403 - val_loss: 0.2369 - val_accuracy: 0.9368- loss: 0.2185 - accuracy: 0.93 - ETA: 0s - loss: 0.2177 - accura
Epoch 9/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.2098 - accuracy: 0.9428 - val_loss: 0.2354 - val_accuracy: 0.9412
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2637 - accuracy: 0.9279
[CV] ..................................... n_neurons=16, total=  12.9s
[CV] n_neurons=16 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.5212 - accuracy: 0.8562 - val_loss: 0.2863 - val_accuracy: 0.9188
Epoch 2/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.2773 - accuracy: 0.9207 - val_loss: 0.2419 - val_accuracy: 0.9322
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2415 - accuracy: 0.9311 - val_loss: 0.2227 - val_accuracy: 0.9394
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2216 - accuracy: 0.9367 - val_loss: 0.2187 - val_accuracy: 0.9404
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2083 - accuracy: 0.9413 - val_loss: 0.2118 - val_accuracy: 0.9444
Epoch 6/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1985 - accuracy: 0.9440 - val_loss: 0.2213 - val_accuracy: 0.9414
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1905 - accuracy: 0.9468 - val_loss: 0.2035 - val_accuracy: 0.9466
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1832 - accuracy: 0.9484 - val_loss: 0.2024 - val_accuracy: 0.9468
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1775 - accuracy: 0.9505 - val_loss: 0.2050 - val_accuracy: 0.9468
Epoch 10/30
36667/36667 [==============================] - 1s 33us/sample - loss: 0.1722 - accuracy: 0.9524 - val_loss: 0.1984 - val_accuracy: 0.9464
Epoch 11/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1666 - accuracy: 0.9535 - val_loss: 0.2031 - val_accuracy: 0.9472
Epoch 12/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1626 - accuracy: 0.9545 - val_loss: 0.1998 - val_accuracy: 0.9480
18333/18333 [==============================] - 0s 22us/sample - loss: 0.2099 - accuracy: 0.9435
[CV] ..................................... n_neurons=16, total=  15.9s
[CV] n_neurons=17 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.4908 - accuracy: 0.8620 - val_loss: 0.2765 - val_accuracy: 0.9240
Epoch 2/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2643 - accuracy: 0.9240 - val_loss: 0.2366 - val_accuracy: 0.9350
Epoch 3/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2295 - accuracy: 0.9338 - val_loss: 0.2165 - val_accuracy: 0.9430
Epoch 4/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2096 - accuracy: 0.9396 - val_loss: 0.2072 - val_accuracy: 0.9414
Epoch 5/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1955 - accuracy: 0.9435 - val_loss: 0.2002 - val_accuracy: 0.9458
Epoch 6/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1848 - accuracy: 0.9483 - val_loss: 0.1976 - val_accuracy: 0.9462
Epoch 7/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1764 - accuracy: 0.9494 - val_loss: 0.1896 - val_accuracy: 0.9466
Epoch 8/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1708 - accuracy: 0.9515 - val_loss: 0.1920 - val_accuracy: 0.9482
Epoch 9/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1643 - accuracy: 0.9535 - val_loss: 0.1883 - val_accuracy: 0.9438
Epoch 10/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1601 - accuracy: 0.9551 - val_loss: 0.1820 - val_accuracy: 0.9508
Epoch 11/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1559 - accuracy: 0.9566 - val_loss: 0.1946 - val_accuracy: 0.9474
Epoch 12/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1522 - accuracy: 0.9579 - val_loss: 0.1880 - val_accuracy: 0.9486
18334/18334 [==============================] - 0s 21us/sample - loss: 0.2161 - accuracy: 0.9411
[CV] ..................................... n_neurons=17, total=  16.5s
[CV] n_neurons=17 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4945 - accuracy: 0.8631 - val_loss: 0.2867 - val_accuracy: 0.9158
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2714 - accuracy: 0.9229 - val_loss: 0.2455 - val_accuracy: 0.9322
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2377 - accuracy: 0.9323 - val_loss: 0.2304 - val_accuracy: 0.9366
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2168 - accuracy: 0.9384 - val_loss: 0.2150 - val_accuracy: 0.9402
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2011 - accuracy: 0.9435 - val_loss: 0.2141 - val_accuracy: 0.9398
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1904 - accuracy: 0.9466 - val_loss: 0.2023 - val_accuracy: 0.9444
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1800 - accuracy: 0.9483 - val_loss: 0.1947 - val_accuracy: 0.9488
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1727 - accuracy: 0.9506 - val_loss: 0.1966 - val_accuracy: 0.9484
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1661 - accuracy: 0.9521 - val_loss: 0.1954 - val_accuracy: 0.9468
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2220 - accuracy: 0.9379
[CV] ..................................... n_neurons=17, total=  12.8s
[CV] n_neurons=17 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4873 - accuracy: 0.8678 - val_loss: 0.2830 - val_accuracy: 0.9190
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2811 - accuracy: 0.9197 - val_loss: 0.2431 - val_accuracy: 0.9316
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2493 - accuracy: 0.9295 - val_loss: 0.2222 - val_accuracy: 0.9382
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2275 - accuracy: 0.9354 - val_loss: 0.2132 - val_accuracy: 0.9386
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2105 - accuracy: 0.9401 - val_loss: 0.2040 - val_accuracy: 0.9430
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1980 - accuracy: 0.9438 - val_loss: 0.2079 - val_accuracy: 0.9410
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1880 - accuracy: 0.9464 - val_loss: 0.1909 - val_accuracy: 0.9490
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1791 - accuracy: 0.9483 - val_loss: 0.1851 - val_accuracy: 0.9492
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1727 - accuracy: 0.9513 - val_loss: 0.1841 - val_accuracy: 0.9488
Epoch 10/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1669 - accuracy: 0.9525 - val_loss: 0.1840 - val_accuracy: 0.9486
Epoch 11/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1606 - accuracy: 0.9537 - val_loss: 0.1858 - val_accuracy: 0.9506
Epoch 12/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1571 - accuracy: 0.9549 - val_loss: 0.1760 - val_accuracy: 0.9500
Epoch 13/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1526 - accuracy: 0.9574 - val_loss: 0.1763 - val_accuracy: 0.9518
Epoch 14/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1483 - accuracy: 0.9581 - val_loss: 0.1759 - val_accuracy: 0.9540
Epoch 15/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1451 - accuracy: 0.9584 - val_loss: 0.1795 - val_accuracy: 0.9494
Epoch 16/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1426 - accuracy: 0.9605 - val_loss: 0.1918 - val_accuracy: 0.9508
18333/18333 [==============================] - 0s 22us/sample - loss: 0.2013 - accuracy: 0.9469
[CV] ..................................... n_neurons=17, total=  21.9s
[CV] n_neurons=18 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.4915 - accuracy: 0.8620 - val_loss: 0.2882 - val_accuracy: 0.9216
Epoch 2/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2824 - accuracy: 0.9195 - val_loss: 0.2565 - val_accuracy: 0.9274
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2505 - accuracy: 0.9290 - val_loss: 0.2288 - val_accuracy: 0.9346
Epoch 4/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2288 - accuracy: 0.9345 - val_loss: 0.2194 - val_accuracy: 0.9374
Epoch 5/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.2120 - accuracy: 0.9406 - val_loss: 0.2075 - val_accuracy: 0.9412
Epoch 6/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1988 - accuracy: 0.9440 - val_loss: 0.2048 - val_accuracy: 0.9398
Epoch 7/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1895 - accuracy: 0.9460 - val_loss: 0.1914 - val_accuracy: 0.9452
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1803 - accuracy: 0.9500 - val_loss: 0.1977 - val_accuracy: 0.9446
Epoch 9/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1734 - accuracy: 0.9510 - val_loss: 0.1862 - val_accuracy: 0.9476
Epoch 10/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1673 - accuracy: 0.9534 - val_loss: 0.1870 - val_accuracy: 0.9486
Epoch 11/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1610 - accuracy: 0.9546 - val_loss: 0.1862 - val_accuracy: 0.9500
18334/18334 [==============================] - 0s 21us/sample - loss: 0.2231 - accuracy: 0.9372
[CV] ..................................... n_neurons=18, total=  15.5s
[CV] n_neurons=18 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4743 - accuracy: 0.8718 - val_loss: 0.2838 - val_accuracy: 0.9186
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2729 - accuracy: 0.9244 - val_loss: 0.2405 - val_accuracy: 0.9316
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2352 - accuracy: 0.9341 - val_loss: 0.2189 - val_accuracy: 0.9404
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2105 - accuracy: 0.9428 - val_loss: 0.2039 - val_accuracy: 0.9442
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1926 - accuracy: 0.9473 - val_loss: 0.2029 - val_accuracy: 0.9484
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1790 - accuracy: 0.9509 - val_loss: 0.1878 - val_accuracy: 0.9516
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1686 - accuracy: 0.9539 - val_loss: 0.1836 - val_accuracy: 0.9504
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1594 - accuracy: 0.9569 - val_loss: 0.1823 - val_accuracy: 0.9522
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1532 - accuracy: 0.9585 - val_loss: 0.1824 - val_accuracy: 0.9514
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1471 - accuracy: 0.9599 - val_loss: 0.1825 - val_accuracy: 0.9526
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2025 - accuracy: 0.9435
[CV] ..................................... n_neurons=18, total=  13.9s
[CV] n_neurons=18 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.5054 - accuracy: 0.8618 - val_loss: 0.2878 - val_accuracy: 0.9184
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2731 - accuracy: 0.9228 - val_loss: 0.2403 - val_accuracy: 0.9318
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2374 - accuracy: 0.9316 - val_loss: 0.2177 - val_accuracy: 0.9400
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2170 - accuracy: 0.9380 - val_loss: 0.2115 - val_accuracy: 0.9408
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2004 - accuracy: 0.9427 - val_loss: 0.1976 - val_accuracy: 0.9466
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1871 - accuracy: 0.9471 - val_loss: 0.2053 - val_accuracy: 0.9436
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1767 - accuracy: 0.9498 - val_loss: 0.1869 - val_accuracy: 0.9480
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1679 - accuracy: 0.9525 - val_loss: 0.1849 - val_accuracy: 0.9490
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1612 - accuracy: 0.9546 - val_loss: 0.1862 - val_accuracy: 0.9506
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1562 - accuracy: 0.9565 - val_loss: 0.1878 - val_accuracy: 0.9504
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2109 - accuracy: 0.9431
[CV] ..................................... n_neurons=18, total=  13.7s
[CV] n_neurons=19 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.4929 - accuracy: 0.8651 - val_loss: 0.2716 - val_accuracy: 0.9268
Epoch 2/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2542 - accuracy: 0.9278 - val_loss: 0.2218 - val_accuracy: 0.9380
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2162 - accuracy: 0.9375 - val_loss: 0.1976 - val_accuracy: 0.9434
Epoch 4/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1934 - accuracy: 0.9444 - val_loss: 0.1889 - val_accuracy: 0.9452
Epoch 5/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1775 - accuracy: 0.9506 - val_loss: 0.1772 - val_accuracy: 0.9504
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1658 - accuracy: 0.9534 - val_loss: 0.1742 - val_accuracy: 0.9500
Epoch 7/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1560 - accuracy: 0.9560 - val_loss: 0.1679 - val_accuracy: 0.9530
Epoch 8/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.1487 - accuracy: 0.9584 - val_loss: 0.1757 - val_accuracy: 0.9510
Epoch 9/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1419 - accuracy: 0.9605 - val_loss: 0.1675 - val_accuracy: 0.9532
Epoch 10/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1370 - accuracy: 0.9620 - val_loss: 0.1663 - val_accuracy: 0.9538
Epoch 11/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1321 - accuracy: 0.9632 - val_loss: 0.1762 - val_accuracy: 0.9516
Epoch 12/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1273 - accuracy: 0.9645 - val_loss: 0.1662 - val_accuracy: 0.9536
Epoch 13/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1241 - accuracy: 0.9659 - val_loss: 0.1719 - val_accuracy: 0.9526
Epoch 14/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1207 - accuracy: 0.9662 - val_loss: 0.1689 - val_accuracy: 0.9516
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1937 - accuracy: 0.9469
[CV] ..................................... n_neurons=19, total=  20.7s
[CV] n_neurons=19 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.4678 - accuracy: 0.8721 - val_loss: 0.2779 - val_accuracy: 0.9188
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2684 - accuracy: 0.9231 - val_loss: 0.2373 - val_accuracy: 0.9320
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2343 - accuracy: 0.9349 - val_loss: 0.2186 - val_accuracy: 0.9390
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2119 - accuracy: 0.9411 - val_loss: 0.2047 - val_accuracy: 0.9416
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1959 - accuracy: 0.9457 - val_loss: 0.2035 - val_accuracy: 0.9452
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1822 - accuracy: 0.9485 - val_loss: 0.1937 - val_accuracy: 0.9462
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1720 - accuracy: 0.9518 - val_loss: 0.1869 - val_accuracy: 0.9468
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1640 - accuracy: 0.9546 - val_loss: 0.1870 - val_accuracy: 0.9490
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1562 - accuracy: 0.9563 - val_loss: 0.1935 - val_accuracy: 0.9476
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2204 - accuracy: 0.9386
[CV] ..................................... n_neurons=19, total=  12.4s
[CV] n_neurons=19 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.5099 - accuracy: 0.8598 - val_loss: 0.2731 - val_accuracy: 0.9230
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2662 - accuracy: 0.9244 - val_loss: 0.2337 - val_accuracy: 0.9326
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2282 - accuracy: 0.9339 - val_loss: 0.2074 - val_accuracy: 0.9402
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2055 - accuracy: 0.9415 - val_loss: 0.1959 - val_accuracy: 0.9444
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1895 - accuracy: 0.9451 - val_loss: 0.1938 - val_accuracy: 0.9444
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1781 - accuracy: 0.9483 - val_loss: 0.1890 - val_accuracy: 0.9460
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1684 - accuracy: 0.9518 - val_loss: 0.1736 - val_accuracy: 0.9494
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1602 - accuracy: 0.9536 - val_loss: 0.1729 - val_accuracy: 0.9502
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1543 - accuracy: 0.9549 - val_loss: 0.1732 - val_accuracy: 0.9518
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1492 - accuracy: 0.9576 - val_loss: 0.1712 - val_accuracy: 0.9522
Epoch 11/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1438 - accuracy: 0.9596 - val_loss: 0.1714 - val_accuracy: 0.9520
Epoch 12/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1405 - accuracy: 0.9593 - val_loss: 0.1686 - val_accuracy: 0.9524
Epoch 13/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1368 - accuracy: 0.9613 - val_loss: 0.1680 - val_accuracy: 0.9522
Epoch 14/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1326 - accuracy: 0.9627 - val_loss: 0.1669 - val_accuracy: 0.9546
Epoch 15/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1293 - accuracy: 0.9633 - val_loss: 0.1743 - val_accuracy: 0.9516
Epoch 16/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1272 - accuracy: 0.9643 - val_loss: 0.1842 - val_accuracy: 0.9500
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1880 - accuracy: 0.9512
[CV] ..................................... n_neurons=19, total=  21.1s
[CV] n_neurons=20 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.5136 - accuracy: 0.8551 - val_loss: 0.2880 - val_accuracy: 0.9202
Epoch 2/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2786 - accuracy: 0.9195 - val_loss: 0.2490 - val_accuracy: 0.9288
Epoch 3/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2379 - accuracy: 0.9319 - val_loss: 0.2175 - val_accuracy: 0.9386
Epoch 4/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2124 - accuracy: 0.9388 - val_loss: 0.2021 - val_accuracy: 0.9416
Epoch 5/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1946 - accuracy: 0.9444 - val_loss: 0.1951 - val_accuracy: 0.9442
Epoch 6/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1821 - accuracy: 0.9477 - val_loss: 0.1939 - val_accuracy: 0.9468
Epoch 7/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1726 - accuracy: 0.9502 - val_loss: 0.1853 - val_accuracy: 0.9476
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1633 - accuracy: 0.9536 - val_loss: 0.1983 - val_accuracy: 0.9456
Epoch 9/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1567 - accuracy: 0.9550 - val_loss: 0.1884 - val_accuracy: 0.9470
18334/18334 [==============================] - 0s 20us/sample - loss: 0.2097 - accuracy: 0.9413
[CV] ..................................... n_neurons=20, total=  12.2s
[CV] n_neurons=20 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.4645 - accuracy: 0.8730 - val_loss: 0.2799 - val_accuracy: 0.9198
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2689 - accuracy: 0.9231 - val_loss: 0.2360 - val_accuracy: 0.9296
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2352 - accuracy: 0.9330 - val_loss: 0.2229 - val_accuracy: 0.9328
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2135 - accuracy: 0.9387 - val_loss: 0.2116 - val_accuracy: 0.9378
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1975 - accuracy: 0.9439 - val_loss: 0.2063 - val_accuracy: 0.9426
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1833 - accuracy: 0.9472 - val_loss: 0.1931 - val_accuracy: 0.9468
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1709 - accuracy: 0.9509 - val_loss: 0.1863 - val_accuracy: 0.9496
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1601 - accuracy: 0.9545 - val_loss: 0.1823 - val_accuracy: 0.9528
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1514 - accuracy: 0.9562 - val_loss: 0.1774 - val_accuracy: 0.9514
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1449 - accuracy: 0.9590 - val_loss: 0.1794 - val_accuracy: 0.9518
Epoch 11/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1394 - accuracy: 0.9611 - val_loss: 0.1756 - val_accuracy: 0.9534
Epoch 12/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1337 - accuracy: 0.9618 - val_loss: 0.1747 - val_accuracy: 0.9524
Epoch 13/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1310 - accuracy: 0.9630 - val_loss: 0.1742 - val_accuracy: 0.9524
Epoch 14/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1258 - accuracy: 0.9651 - val_loss: 0.1722 - val_accuracy: 0.9548
Epoch 15/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1230 - accuracy: 0.9656 - val_loss: 0.1695 - val_accuracy: 0.9570
Epoch 16/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1201 - accuracy: 0.9672 - val_loss: 0.1720 - val_accuracy: 0.9550
Epoch 17/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1163 - accuracy: 0.9671 - val_loss: 0.1709 - val_accuracy: 0.9572
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1930 - accuracy: 0.9502
[CV] ..................................... n_neurons=20, total=  22.3s
[CV] n_neurons=20 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.4873 - accuracy: 0.8698 - val_loss: 0.2824 - val_accuracy: 0.9202
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2761 - accuracy: 0.9203 - val_loss: 0.2496 - val_accuracy: 0.9282
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2425 - accuracy: 0.9294 - val_loss: 0.2263 - val_accuracy: 0.9396
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2209 - accuracy: 0.9360 - val_loss: 0.2189 - val_accuracy: 0.9374
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2040 - accuracy: 0.9409 - val_loss: 0.2085 - val_accuracy: 0.9440
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1913 - accuracy: 0.9448 - val_loss: 0.2174 - val_accuracy: 0.9404
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1794 - accuracy: 0.9487 - val_loss: 0.1922 - val_accuracy: 0.9474
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1694 - accuracy: 0.9512 - val_loss: 0.1934 - val_accuracy: 0.9482
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1613 - accuracy: 0.9538 - val_loss: 0.1897 - val_accuracy: 0.9492
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1559 - accuracy: 0.9566 - val_loss: 0.1911 - val_accuracy: 0.9476
Epoch 11/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1498 - accuracy: 0.9575 - val_loss: 0.1858 - val_accuracy: 0.9514
Epoch 12/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1461 - accuracy: 0.9587 - val_loss: 0.1855 - val_accuracy: 0.9514
Epoch 13/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1414 - accuracy: 0.9603 - val_loss: 0.1928 - val_accuracy: 0.9480
Epoch 14/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1373 - accuracy: 0.9613 - val_loss: 0.1862 - val_accuracy: 0.9536
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2003 - accuracy: 0.9473
[CV] ..................................... n_neurons=20, total=  18.7s
[CV] n_neurons=21 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.4553 - accuracy: 0.8736 - val_loss: 0.2768 - val_accuracy: 0.9216
Epoch 2/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2685 - accuracy: 0.9233 - val_loss: 0.2372 - val_accuracy: 0.9330
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2296 - accuracy: 0.9342 - val_loss: 0.2109 - val_accuracy: 0.9388
Epoch 4/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2053 - accuracy: 0.9403 - val_loss: 0.2057 - val_accuracy: 0.9434
Epoch 5/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1897 - accuracy: 0.9460 - val_loss: 0.1904 - val_accuracy: 0.9480
Epoch 6/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1762 - accuracy: 0.9496 - val_loss: 0.1836 - val_accuracy: 0.9508
Epoch 7/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1670 - accuracy: 0.9531 - val_loss: 0.1765 - val_accuracy: 0.9532
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1586 - accuracy: 0.9545 - val_loss: 0.1868 - val_accuracy: 0.9496
Epoch 9/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1521 - accuracy: 0.9562 - val_loss: 0.1777 - val_accuracy: 0.9506
18334/18334 [==============================] - 0s 22us/sample - loss: 0.2082 - accuracy: 0.9418
[CV] ..................................... n_neurons=21, total=  12.4s
[CV] n_neurons=21 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.4662 - accuracy: 0.8718 - val_loss: 0.2808 - val_accuracy: 0.9184
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2661 - accuracy: 0.9242 - val_loss: 0.2381 - val_accuracy: 0.9296
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2316 - accuracy: 0.9341 - val_loss: 0.2122 - val_accuracy: 0.9402
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2092 - accuracy: 0.9414 - val_loss: 0.2005 - val_accuracy: 0.9402
Epoch 5/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1936 - accuracy: 0.9448 - val_loss: 0.2061 - val_accuracy: 0.9416
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1821 - accuracy: 0.9478 - val_loss: 0.1916 - val_accuracy: 0.9472
Epoch 7/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1722 - accuracy: 0.9505 - val_loss: 0.1841 - val_accuracy: 0.9456
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1643 - accuracy: 0.9523 - val_loss: 0.1837 - val_accuracy: 0.9482
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1583 - accuracy: 0.9549 - val_loss: 0.1865 - val_accuracy: 0.9472
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1529 - accuracy: 0.9566 - val_loss: 0.1829 - val_accuracy: 0.9516
Epoch 11/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1486 - accuracy: 0.9576 - val_loss: 0.1826 - val_accuracy: 0.9508
Epoch 12/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1434 - accuracy: 0.9596 - val_loss: 0.1892 - val_accuracy: 0.9510
Epoch 13/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1405 - accuracy: 0.9600 - val_loss: 0.1817 - val_accuracy: 0.9520
Epoch 14/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1365 - accuracy: 0.9612 - val_loss: 0.1835 - val_accuracy: 0.9524
Epoch 15/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1325 - accuracy: 0.9626 - val_loss: 0.1927 - val_accuracy: 0.9478
18333/18333 [==============================] - 0s 25us/sample - loss: 0.2118 - accuracy: 0.9433
[CV] ..................................... n_neurons=21, total=  21.1s
[CV] n_neurons=21 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.4735 - accuracy: 0.8694 - val_loss: 0.2855 - val_accuracy: 0.9206
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2847 - accuracy: 0.9188 - val_loss: 0.2478 - val_accuracy: 0.9278
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2496 - accuracy: 0.9290 - val_loss: 0.2229 - val_accuracy: 0.9378
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2252 - accuracy: 0.9354 - val_loss: 0.2113 - val_accuracy: 0.9418
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2062 - accuracy: 0.9419 - val_loss: 0.2004 - val_accuracy: 0.9460
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1910 - accuracy: 0.9465 - val_loss: 0.2049 - val_accuracy: 0.9436
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1794 - accuracy: 0.9501 - val_loss: 0.1853 - val_accuracy: 0.9494
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1698 - accuracy: 0.9518 - val_loss: 0.1812 - val_accuracy: 0.9506
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1635 - accuracy: 0.9540 - val_loss: 0.1826 - val_accuracy: 0.9506
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1573 - accuracy: 0.9558 - val_loss: 0.1822 - val_accuracy: 0.9494
18333/18333 [==============================] - 0s 21us/sample - loss: 0.2012 - accuracy: 0.9437
[CV] ..................................... n_neurons=21, total=  14.0s
[CV] n_neurons=22 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.4375 - accuracy: 0.8787 - val_loss: 0.2794 - val_accuracy: 0.9236
Epoch 2/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.2612 - accuracy: 0.9239 - val_loss: 0.2367 - val_accuracy: 0.9350
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2268 - accuracy: 0.9344 - val_loss: 0.2161 - val_accuracy: 0.9428
Epoch 4/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2055 - accuracy: 0.9402 - val_loss: 0.2014 - val_accuracy: 0.9458
Epoch 5/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1909 - accuracy: 0.9449 - val_loss: 0.2001 - val_accuracy: 0.9462
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1795 - accuracy: 0.9486 - val_loss: 0.1929 - val_accuracy: 0.9478
Epoch 7/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1710 - accuracy: 0.9499 - val_loss: 0.1914 - val_accuracy: 0.9466
Epoch 8/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1618 - accuracy: 0.9539 - val_loss: 0.1943 - val_accuracy: 0.9492
Epoch 9/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1549 - accuracy: 0.9558 - val_loss: 0.1826 - val_accuracy: 0.9504
Epoch 10/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1472 - accuracy: 0.9576 - val_loss: 0.1781 - val_accuracy: 0.9520
Epoch 11/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1398 - accuracy: 0.9602 - val_loss: 0.1886 - val_accuracy: 0.9490
Epoch 12/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1346 - accuracy: 0.9614 - val_loss: 0.1798 - val_accuracy: 0.9504
18334/18334 [==============================] - 0s 21us/sample - loss: 0.2016 - accuracy: 0.9453
[CV] ..................................... n_neurons=22, total=  17.0s
[CV] n_neurons=22 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4796 - accuracy: 0.8689 - val_loss: 0.2874 - val_accuracy: 0.9162
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2737 - accuracy: 0.9234 - val_loss: 0.2472 - val_accuracy: 0.9292
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2387 - accuracy: 0.9330 - val_loss: 0.2309 - val_accuracy: 0.9344
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2129 - accuracy: 0.9396 - val_loss: 0.2104 - val_accuracy: 0.9426
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1933 - accuracy: 0.9460 - val_loss: 0.2053 - val_accuracy: 0.9426
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1777 - accuracy: 0.9490 - val_loss: 0.1872 - val_accuracy: 0.9494
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1643 - accuracy: 0.9520 - val_loss: 0.1788 - val_accuracy: 0.9504
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1534 - accuracy: 0.9554 - val_loss: 0.1751 - val_accuracy: 0.9516
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1442 - accuracy: 0.9585 - val_loss: 0.1765 - val_accuracy: 0.9514
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1374 - accuracy: 0.9599 - val_loss: 0.1725 - val_accuracy: 0.9552
Epoch 11/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1317 - accuracy: 0.9615 - val_loss: 0.1697 - val_accuracy: 0.9542
Epoch 12/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1254 - accuracy: 0.9632 - val_loss: 0.1699 - val_accuracy: 0.9548
Epoch 13/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1214 - accuracy: 0.9645 - val_loss: 0.1693 - val_accuracy: 0.9542
Epoch 14/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1164 - accuracy: 0.9663 - val_loss: 0.1652 - val_accuracy: 0.9556
Epoch 15/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1127 - accuracy: 0.9670 - val_loss: 0.1675 - val_accuracy: 0.9554
Epoch 16/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1104 - accuracy: 0.9683 - val_loss: 0.1680 - val_accuracy: 0.9550
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1864 - accuracy: 0.9500
[CV] ..................................... n_neurons=22, total=  21.4s
[CV] n_neurons=22 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.4689 - accuracy: 0.8712 - val_loss: 0.2714 - val_accuracy: 0.9208
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2633 - accuracy: 0.9247 - val_loss: 0.2253 - val_accuracy: 0.9356
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2229 - accuracy: 0.9357 - val_loss: 0.1967 - val_accuracy: 0.9444
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1986 - accuracy: 0.9414 - val_loss: 0.1880 - val_accuracy: 0.9436
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1802 - accuracy: 0.9473 - val_loss: 0.1744 - val_accuracy: 0.9504
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1670 - accuracy: 0.9523 - val_loss: 0.1771 - val_accuracy: 0.9486
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1568 - accuracy: 0.9554 - val_loss: 0.1562 - val_accuracy: 0.9534
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1474 - accuracy: 0.9586 - val_loss: 0.1605 - val_accuracy: 0.9542
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1398 - accuracy: 0.9601 - val_loss: 0.1570 - val_accuracy: 0.9586
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1853 - accuracy: 0.9480
[CV] ..................................... n_neurons=22, total=  12.3s
[CV] n_neurons=23 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.4756 - accuracy: 0.8657 - val_loss: 0.2794 - val_accuracy: 0.9252
Epoch 2/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2564 - accuracy: 0.9260 - val_loss: 0.2268 - val_accuracy: 0.9336
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2132 - accuracy: 0.9387 - val_loss: 0.1975 - val_accuracy: 0.9444
Epoch 4/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1875 - accuracy: 0.9470 - val_loss: 0.1905 - val_accuracy: 0.9454
Epoch 5/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1714 - accuracy: 0.9507 - val_loss: 0.1804 - val_accuracy: 0.9512
Epoch 6/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1595 - accuracy: 0.9541 - val_loss: 0.1766 - val_accuracy: 0.9514
Epoch 7/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1500 - accuracy: 0.9573 - val_loss: 0.1724 - val_accuracy: 0.9520
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1417 - accuracy: 0.9595 - val_loss: 0.1664 - val_accuracy: 0.9560
Epoch 9/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1340 - accuracy: 0.9623 - val_loss: 0.1634 - val_accuracy: 0.9542
Epoch 10/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1279 - accuracy: 0.9634 - val_loss: 0.1575 - val_accuracy: 0.9570
Epoch 11/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1217 - accuracy: 0.9654 - val_loss: 0.1705 - val_accuracy: 0.9564
Epoch 12/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1166 - accuracy: 0.9676 - val_loss: 0.1662 - val_accuracy: 0.9572
18334/18334 [==============================] - 0s 21us/sample - loss: 0.1778 - accuracy: 0.9512
[CV] ..................................... n_neurons=23, total=  16.2s
[CV] n_neurons=23 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4752 - accuracy: 0.8685 - val_loss: 0.2805 - val_accuracy: 0.9170
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2672 - accuracy: 0.9243 - val_loss: 0.2392 - val_accuracy: 0.9314
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2303 - accuracy: 0.9352 - val_loss: 0.2110 - val_accuracy: 0.9414
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2045 - accuracy: 0.9420 - val_loss: 0.1971 - val_accuracy: 0.9422
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1846 - accuracy: 0.9480 - val_loss: 0.1930 - val_accuracy: 0.9424
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1706 - accuracy: 0.9521 - val_loss: 0.1761 - val_accuracy: 0.9472
Epoch 7/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1581 - accuracy: 0.9550 - val_loss: 0.1707 - val_accuracy: 0.9484
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1484 - accuracy: 0.9582 - val_loss: 0.1697 - val_accuracy: 0.9514
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1407 - accuracy: 0.9594 - val_loss: 0.1677 - val_accuracy: 0.9500
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1343 - accuracy: 0.9622 - val_loss: 0.1676 - val_accuracy: 0.9530
Epoch 11/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1287 - accuracy: 0.9632 - val_loss: 0.1590 - val_accuracy: 0.9518
Epoch 12/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1232 - accuracy: 0.9643 - val_loss: 0.1678 - val_accuracy: 0.9504
Epoch 13/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1197 - accuracy: 0.9662 - val_loss: 0.1602 - val_accuracy: 0.9530
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1860 - accuracy: 0.9481
[CV] ..................................... n_neurons=23, total=  17.4s
[CV] n_neurons=23 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4762 - accuracy: 0.8683 - val_loss: 0.2724 - val_accuracy: 0.9222
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2660 - accuracy: 0.9239 - val_loss: 0.2230 - val_accuracy: 0.9378
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2265 - accuracy: 0.9352 - val_loss: 0.1979 - val_accuracy: 0.9436
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2027 - accuracy: 0.9408 - val_loss: 0.1862 - val_accuracy: 0.9486
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1856 - accuracy: 0.9480 - val_loss: 0.1788 - val_accuracy: 0.9504
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1723 - accuracy: 0.9502 - val_loss: 0.1777 - val_accuracy: 0.9508
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1619 - accuracy: 0.9537 - val_loss: 0.1651 - val_accuracy: 0.9516
Epoch 8/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1523 - accuracy: 0.9557 - val_loss: 0.1707 - val_accuracy: 0.9534
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1460 - accuracy: 0.9589 - val_loss: 0.1645 - val_accuracy: 0.9518
Epoch 10/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1390 - accuracy: 0.9608 - val_loss: 0.1612 - val_accuracy: 0.9536
Epoch 11/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1337 - accuracy: 0.9620 - val_loss: 0.1649 - val_accuracy: 0.9536
Epoch 12/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1292 - accuracy: 0.9631 - val_loss: 0.1590 - val_accuracy: 0.9540
Epoch 13/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1252 - accuracy: 0.9650 - val_loss: 0.1643 - val_accuracy: 0.9540
Epoch 14/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1207 - accuracy: 0.9660 - val_loss: 0.1597 - val_accuracy: 0.9572
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1952 - accuracy: 0.9503
[CV] ..................................... n_neurons=23, total=  19.1s
[CV] n_neurons=24 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.4406 - accuracy: 0.8793 - val_loss: 0.2605 - val_accuracy: 0.9314
Epoch 2/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.2452 - accuracy: 0.9302 - val_loss: 0.2162 - val_accuracy: 0.9378
Epoch 3/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.2071 - accuracy: 0.9404 - val_loss: 0.1879 - val_accuracy: 0.9482
Epoch 4/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1842 - accuracy: 0.9480 - val_loss: 0.1768 - val_accuracy: 0.9486
Epoch 5/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1679 - accuracy: 0.9521 - val_loss: 0.1639 - val_accuracy: 0.9562
Epoch 6/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1554 - accuracy: 0.9556 - val_loss: 0.1638 - val_accuracy: 0.9540
Epoch 7/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1443 - accuracy: 0.9596 - val_loss: 0.1581 - val_accuracy: 0.9548
Epoch 8/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1363 - accuracy: 0.9619 - val_loss: 0.1617 - val_accuracy: 0.9546
Epoch 9/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1285 - accuracy: 0.9635 - val_loss: 0.1577 - val_accuracy: 0.9548
Epoch 10/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1221 - accuracy: 0.9647 - val_loss: 0.1566 - val_accuracy: 0.9550
Epoch 11/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1174 - accuracy: 0.9667 - val_loss: 0.1554 - val_accuracy: 0.9554
Epoch 12/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1125 - accuracy: 0.9686 - val_loss: 0.1512 - val_accuracy: 0.9564
Epoch 13/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1081 - accuracy: 0.9703 - val_loss: 0.1597 - val_accuracy: 0.9576
Epoch 14/30
36666/36666 [==============================] - 1s 34us/sample - loss: 0.1045 - accuracy: 0.9708 - val_loss: 0.1561 - val_accuracy: 0.9564
18334/18334 [==============================] - 0s 21us/sample - loss: 0.1760 - accuracy: 0.9505
[CV] ..................................... n_neurons=24, total=  19.9s
[CV] n_neurons=24 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.4348 - accuracy: 0.8832 - val_loss: 0.2519 - val_accuracy: 0.9262
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2386 - accuracy: 0.9328 - val_loss: 0.2115 - val_accuracy: 0.9398
Epoch 3/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2028 - accuracy: 0.9434 - val_loss: 0.1944 - val_accuracy: 0.9462
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1788 - accuracy: 0.9498 - val_loss: 0.1766 - val_accuracy: 0.9520
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1608 - accuracy: 0.9548 - val_loss: 0.1712 - val_accuracy: 0.9532
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1459 - accuracy: 0.9584 - val_loss: 0.1601 - val_accuracy: 0.9586
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1348 - accuracy: 0.9615 - val_loss: 0.1523 - val_accuracy: 0.9600
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1255 - accuracy: 0.9645 - val_loss: 0.1507 - val_accuracy: 0.9594
Epoch 9/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1194 - accuracy: 0.9658 - val_loss: 0.1513 - val_accuracy: 0.9590
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1132 - accuracy: 0.9674 - val_loss: 0.1509 - val_accuracy: 0.9602
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1769 - accuracy: 0.9524
[CV] ..................................... n_neurons=24, total=  13.5s
[CV] n_neurons=24 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.4625 - accuracy: 0.8727 - val_loss: 0.2473 - val_accuracy: 0.9270
Epoch 2/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.2403 - accuracy: 0.9303 - val_loss: 0.2100 - val_accuracy: 0.9376
Epoch 3/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2037 - accuracy: 0.9402 - val_loss: 0.1864 - val_accuracy: 0.9482
Epoch 4/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1830 - accuracy: 0.9469 - val_loss: 0.1765 - val_accuracy: 0.9500
Epoch 5/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1670 - accuracy: 0.9521 - val_loss: 0.1705 - val_accuracy: 0.9530
Epoch 6/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1550 - accuracy: 0.9554 - val_loss: 0.1677 - val_accuracy: 0.9548
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1447 - accuracy: 0.9587 - val_loss: 0.1596 - val_accuracy: 0.9574
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1370 - accuracy: 0.9609 - val_loss: 0.1575 - val_accuracy: 0.9572
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1294 - accuracy: 0.9633 - val_loss: 0.1586 - val_accuracy: 0.9564
Epoch 10/30
36667/36667 [==============================] - 1s 34us/sample - loss: 0.1247 - accuracy: 0.9649 - val_loss: 0.1642 - val_accuracy: 0.9536
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1749 - accuracy: 0.9517
[CV] ..................................... n_neurons=24, total=  14.0s
[CV] n_neurons=25 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.4358 - accuracy: 0.8817 - val_loss: 0.2503 - val_accuracy: 0.9308
Epoch 2/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2341 - accuracy: 0.9330 - val_loss: 0.2107 - val_accuracy: 0.9414
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1970 - accuracy: 0.9430 - val_loss: 0.1804 - val_accuracy: 0.9480
Epoch 4/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.1760 - accuracy: 0.9495 - val_loss: 0.1708 - val_accuracy: 0.9542
Epoch 5/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1603 - accuracy: 0.9541 - val_loss: 0.1627 - val_accuracy: 0.9572
Epoch 6/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1486 - accuracy: 0.9588 - val_loss: 0.1543 - val_accuracy: 0.9596
Epoch 7/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1391 - accuracy: 0.9609 - val_loss: 0.1537 - val_accuracy: 0.9584
Epoch 8/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1315 - accuracy: 0.9626 - val_loss: 0.1613 - val_accuracy: 0.9562
Epoch 9/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1239 - accuracy: 0.9653 - val_loss: 0.1467 - val_accuracy: 0.9610
Epoch 10/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1181 - accuracy: 0.9673 - val_loss: 0.1458 - val_accuracy: 0.9604
Epoch 11/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1119 - accuracy: 0.9698 - val_loss: 0.1522 - val_accuracy: 0.9598
Epoch 12/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1070 - accuracy: 0.9711 - val_loss: 0.1490 - val_accuracy: 0.9622
18334/18334 [==============================] - 0s 21us/sample - loss: 0.1631 - accuracy: 0.9566
[CV] ..................................... n_neurons=25, total=  17.8s
[CV] n_neurons=25 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4302 - accuracy: 0.8812 - val_loss: 0.2698 - val_accuracy: 0.9218
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2548 - accuracy: 0.9275 - val_loss: 0.2250 - val_accuracy: 0.9348
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2144 - accuracy: 0.9392 - val_loss: 0.2049 - val_accuracy: 0.9446
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1890 - accuracy: 0.9466 - val_loss: 0.1863 - val_accuracy: 0.9488
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1712 - accuracy: 0.9515 - val_loss: 0.1903 - val_accuracy: 0.9492
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1570 - accuracy: 0.9546 - val_loss: 0.1753 - val_accuracy: 0.9498
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1447 - accuracy: 0.9584 - val_loss: 0.1641 - val_accuracy: 0.9540
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1361 - accuracy: 0.9611 - val_loss: 0.1629 - val_accuracy: 0.9522
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1287 - accuracy: 0.9638 - val_loss: 0.1630 - val_accuracy: 0.9542
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1226 - accuracy: 0.9654 - val_loss: 0.1620 - val_accuracy: 0.9544
Epoch 11/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1182 - accuracy: 0.9673 - val_loss: 0.1580 - val_accuracy: 0.9560
Epoch 12/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1127 - accuracy: 0.9683 - val_loss: 0.1607 - val_accuracy: 0.9558
Epoch 13/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1093 - accuracy: 0.9701 - val_loss: 0.1619 - val_accuracy: 0.9564
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1795 - accuracy: 0.9500
[CV] ..................................... n_neurons=25, total=  17.9s
[CV] n_neurons=25 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4566 - accuracy: 0.8730 - val_loss: 0.2637 - val_accuracy: 0.9228
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2489 - accuracy: 0.9284 - val_loss: 0.2112 - val_accuracy: 0.9416
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2041 - accuracy: 0.9414 - val_loss: 0.1836 - val_accuracy: 0.9510
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1778 - accuracy: 0.9489 - val_loss: 0.1758 - val_accuracy: 0.9514
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1604 - accuracy: 0.9551 - val_loss: 0.1637 - val_accuracy: 0.9576
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1471 - accuracy: 0.9588 - val_loss: 0.1631 - val_accuracy: 0.9560
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1364 - accuracy: 0.9615 - val_loss: 0.1517 - val_accuracy: 0.9594
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1281 - accuracy: 0.9632 - val_loss: 0.1518 - val_accuracy: 0.9614
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1221 - accuracy: 0.9657 - val_loss: 0.1474 - val_accuracy: 0.9608
Epoch 10/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1155 - accuracy: 0.9677 - val_loss: 0.1499 - val_accuracy: 0.9612
Epoch 11/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1095 - accuracy: 0.9696 - val_loss: 0.1509 - val_accuracy: 0.9604
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1696 - accuracy: 0.9524
[CV] ..................................... n_neurons=25, total=  15.4s
[CV] n_neurons=26 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.4482 - accuracy: 0.8775 - val_loss: 0.2631 - val_accuracy: 0.9280
Epoch 2/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2517 - accuracy: 0.9274 - val_loss: 0.2146 - val_accuracy: 0.9392
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2084 - accuracy: 0.9401 - val_loss: 0.1899 - val_accuracy: 0.9464
Epoch 4/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1840 - accuracy: 0.9463 - val_loss: 0.1853 - val_accuracy: 0.9478
Epoch 5/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1677 - accuracy: 0.9526 - val_loss: 0.1736 - val_accuracy: 0.9516
Epoch 6/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1559 - accuracy: 0.9563 - val_loss: 0.1718 - val_accuracy: 0.9526
Epoch 7/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1461 - accuracy: 0.9589 - val_loss: 0.1642 - val_accuracy: 0.9532
Epoch 8/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1380 - accuracy: 0.9609 - val_loss: 0.1703 - val_accuracy: 0.9518
Epoch 9/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1316 - accuracy: 0.9622 - val_loss: 0.1668 - val_accuracy: 0.9544
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1879 - accuracy: 0.9480
[CV] ..................................... n_neurons=26, total=  12.7s
[CV] n_neurons=26 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4346 - accuracy: 0.8795 - val_loss: 0.2701 - val_accuracy: 0.9208
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2569 - accuracy: 0.9290 - val_loss: 0.2294 - val_accuracy: 0.9360
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2211 - accuracy: 0.9381 - val_loss: 0.2060 - val_accuracy: 0.9434
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1963 - accuracy: 0.9452 - val_loss: 0.1870 - val_accuracy: 0.9492
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1775 - accuracy: 0.9499 - val_loss: 0.2042 - val_accuracy: 0.9462
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1640 - accuracy: 0.9532 - val_loss: 0.1729 - val_accuracy: 0.9536
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1513 - accuracy: 0.9567 - val_loss: 0.1683 - val_accuracy: 0.9532
Epoch 8/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1420 - accuracy: 0.9606 - val_loss: 0.1645 - val_accuracy: 0.9566
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1342 - accuracy: 0.9625 - val_loss: 0.1637 - val_accuracy: 0.9570
Epoch 10/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1283 - accuracy: 0.9639 - val_loss: 0.1628 - val_accuracy: 0.9570
Epoch 11/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1220 - accuracy: 0.9656 - val_loss: 0.1595 - val_accuracy: 0.9572
Epoch 12/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1167 - accuracy: 0.9666 - val_loss: 0.1611 - val_accuracy: 0.9570
Epoch 13/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1135 - accuracy: 0.9681 - val_loss: 0.1652 - val_accuracy: 0.9534
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1826 - accuracy: 0.9499
[CV] ..................................... n_neurons=26, total=  18.8s
[CV] n_neurons=26 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4519 - accuracy: 0.8755 - val_loss: 0.2817 - val_accuracy: 0.9184
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2736 - accuracy: 0.9212 - val_loss: 0.2390 - val_accuracy: 0.9348
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2278 - accuracy: 0.9349 - val_loss: 0.2054 - val_accuracy: 0.9410
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1967 - accuracy: 0.9442 - val_loss: 0.1910 - val_accuracy: 0.9462
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1755 - accuracy: 0.9506 - val_loss: 0.1786 - val_accuracy: 0.9502
Epoch 6/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1602 - accuracy: 0.9536 - val_loss: 0.1763 - val_accuracy: 0.9488
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1473 - accuracy: 0.9577 - val_loss: 0.1590 - val_accuracy: 0.9530
Epoch 8/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1364 - accuracy: 0.9606 - val_loss: 0.1624 - val_accuracy: 0.9530
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1287 - accuracy: 0.9627 - val_loss: 0.1523 - val_accuracy: 0.9560
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1215 - accuracy: 0.9658 - val_loss: 0.1595 - val_accuracy: 0.9542
Epoch 11/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1147 - accuracy: 0.9676 - val_loss: 0.1570 - val_accuracy: 0.9556
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1829 - accuracy: 0.9512
[CV] ..................................... n_neurons=26, total=  15.4s
[CV] n_neurons=27 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.4295 - accuracy: 0.8811 - val_loss: 0.2584 - val_accuracy: 0.9304
Epoch 2/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2424 - accuracy: 0.9305 - val_loss: 0.2056 - val_accuracy: 0.9414
Epoch 3/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1957 - accuracy: 0.9442 - val_loss: 0.1824 - val_accuracy: 0.9476
Epoch 4/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1689 - accuracy: 0.9512 - val_loss: 0.1670 - val_accuracy: 0.9524
Epoch 5/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1515 - accuracy: 0.9565 - val_loss: 0.1592 - val_accuracy: 0.9560
Epoch 6/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1386 - accuracy: 0.9598 - val_loss: 0.1553 - val_accuracy: 0.9580
Epoch 7/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1285 - accuracy: 0.9632 - val_loss: 0.1560 - val_accuracy: 0.9574
Epoch 8/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1212 - accuracy: 0.9654 - val_loss: 0.1520 - val_accuracy: 0.9584
Epoch 9/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1145 - accuracy: 0.9675 - val_loss: 0.1528 - val_accuracy: 0.9564
Epoch 10/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1092 - accuracy: 0.9689 - val_loss: 0.1510 - val_accuracy: 0.9596
Epoch 11/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1047 - accuracy: 0.9701 - val_loss: 0.1526 - val_accuracy: 0.9598
Epoch 12/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.0998 - accuracy: 0.9714 - val_loss: 0.1579 - val_accuracy: 0.9590
18334/18334 [==============================] - 0s 21us/sample - loss: 0.1773 - accuracy: 0.9539
[CV] ..................................... n_neurons=27, total=  16.6s
[CV] n_neurons=27 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4331 - accuracy: 0.8829 - val_loss: 0.2598 - val_accuracy: 0.9238
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2448 - accuracy: 0.9312 - val_loss: 0.2154 - val_accuracy: 0.9380
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2037 - accuracy: 0.9425 - val_loss: 0.1936 - val_accuracy: 0.9442
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1776 - accuracy: 0.9503 - val_loss: 0.1801 - val_accuracy: 0.9492
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1572 - accuracy: 0.9553 - val_loss: 0.1718 - val_accuracy: 0.9498
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1412 - accuracy: 0.9595 - val_loss: 0.1611 - val_accuracy: 0.9554
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1300 - accuracy: 0.9624 - val_loss: 0.1566 - val_accuracy: 0.9580
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1205 - accuracy: 0.9658 - val_loss: 0.1570 - val_accuracy: 0.9578
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1124 - accuracy: 0.9680 - val_loss: 0.1539 - val_accuracy: 0.9564
Epoch 10/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1074 - accuracy: 0.9698 - val_loss: 0.1579 - val_accuracy: 0.9592
Epoch 11/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1016 - accuracy: 0.9710 - val_loss: 0.1554 - val_accuracy: 0.9578
18333/18333 [==============================] - 1s 42us/sample - loss: 0.1706 - accuracy: 0.9540
[CV] ..................................... n_neurons=27, total=  15.5s
[CV] n_neurons=27 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4652 - accuracy: 0.8691 - val_loss: 0.2697 - val_accuracy: 0.9220
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2521 - accuracy: 0.9264 - val_loss: 0.2259 - val_accuracy: 0.9388
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2136 - accuracy: 0.9386 - val_loss: 0.2015 - val_accuracy: 0.9450
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1896 - accuracy: 0.9455 - val_loss: 0.1929 - val_accuracy: 0.9476
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1709 - accuracy: 0.9511 - val_loss: 0.1830 - val_accuracy: 0.9530
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1558 - accuracy: 0.9552 - val_loss: 0.1756 - val_accuracy: 0.9538
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1412 - accuracy: 0.9598 - val_loss: 0.1651 - val_accuracy: 0.9548
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1320 - accuracy: 0.9626 - val_loss: 0.1631 - val_accuracy: 0.9558
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1233 - accuracy: 0.9652 - val_loss: 0.1619 - val_accuracy: 0.9564
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1171 - accuracy: 0.9673 - val_loss: 0.1611 - val_accuracy: 0.9568
Epoch 11/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1101 - accuracy: 0.9695 - val_loss: 0.1562 - val_accuracy: 0.9596
Epoch 12/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1055 - accuracy: 0.9701 - val_loss: 0.1520 - val_accuracy: 0.9608
Epoch 13/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1004 - accuracy: 0.9717 - val_loss: 0.1568 - val_accuracy: 0.9576
Epoch 14/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.0961 - accuracy: 0.9737 - val_loss: 0.1550 - val_accuracy: 0.9612
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1652 - accuracy: 0.9573
[CV] ..................................... n_neurons=27, total=  19.2s
[CV] n_neurons=28 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.4252 - accuracy: 0.8844 - val_loss: 0.2627 - val_accuracy: 0.9256
Epoch 2/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2502 - accuracy: 0.9283 - val_loss: 0.2198 - val_accuracy: 0.9384
Epoch 3/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2057 - accuracy: 0.9410 - val_loss: 0.1871 - val_accuracy: 0.9484
Epoch 4/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1780 - accuracy: 0.9494 - val_loss: 0.1784 - val_accuracy: 0.9496
Epoch 5/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1604 - accuracy: 0.9545 - val_loss: 0.1725 - val_accuracy: 0.9518
Epoch 6/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1464 - accuracy: 0.9578 - val_loss: 0.1652 - val_accuracy: 0.9540
Epoch 7/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1351 - accuracy: 0.9605 - val_loss: 0.1607 - val_accuracy: 0.9534
Epoch 8/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1267 - accuracy: 0.9641 - val_loss: 0.1620 - val_accuracy: 0.9544
Epoch 9/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1195 - accuracy: 0.9655 - val_loss: 0.1565 - val_accuracy: 0.9562
Epoch 10/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1139 - accuracy: 0.9669 - val_loss: 0.1538 - val_accuracy: 0.9560
Epoch 11/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1077 - accuracy: 0.9696 - val_loss: 0.1566 - val_accuracy: 0.9560
Epoch 12/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1034 - accuracy: 0.9709 - val_loss: 0.1544 - val_accuracy: 0.9572
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1800 - accuracy: 0.9507
[CV] ..................................... n_neurons=28, total=  16.5s
[CV] n_neurons=28 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4189 - accuracy: 0.8845 - val_loss: 0.2529 - val_accuracy: 0.9248
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2368 - accuracy: 0.9327 - val_loss: 0.2043 - val_accuracy: 0.9396
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1933 - accuracy: 0.9448 - val_loss: 0.1815 - val_accuracy: 0.9454
Epoch 4/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1665 - accuracy: 0.9521 - val_loss: 0.1637 - val_accuracy: 0.9522
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1484 - accuracy: 0.9577 - val_loss: 0.1616 - val_accuracy: 0.9548
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1349 - accuracy: 0.9615 - val_loss: 0.1507 - val_accuracy: 0.9564
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1251 - accuracy: 0.9634 - val_loss: 0.1466 - val_accuracy: 0.9584
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1162 - accuracy: 0.9669 - val_loss: 0.1457 - val_accuracy: 0.9590
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1106 - accuracy: 0.9681 - val_loss: 0.1467 - val_accuracy: 0.9586
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1046 - accuracy: 0.9708 - val_loss: 0.1482 - val_accuracy: 0.9586
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1669 - accuracy: 0.9534
[CV] ..................................... n_neurons=28, total=  14.0s
[CV] n_neurons=28 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.4426 - accuracy: 0.8750 - val_loss: 0.2606 - val_accuracy: 0.9232
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2488 - accuracy: 0.9284 - val_loss: 0.2113 - val_accuracy: 0.9382
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2071 - accuracy: 0.9394 - val_loss: 0.1833 - val_accuracy: 0.9478
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1821 - accuracy: 0.9472 - val_loss: 0.1772 - val_accuracy: 0.9488
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1632 - accuracy: 0.9534 - val_loss: 0.1677 - val_accuracy: 0.9514
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1496 - accuracy: 0.9569 - val_loss: 0.1615 - val_accuracy: 0.9540
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1371 - accuracy: 0.9601 - val_loss: 0.1550 - val_accuracy: 0.9572
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1275 - accuracy: 0.9632 - val_loss: 0.1474 - val_accuracy: 0.9584
Epoch 9/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1202 - accuracy: 0.9659 - val_loss: 0.1450 - val_accuracy: 0.9602
Epoch 10/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1135 - accuracy: 0.9681 - val_loss: 0.1472 - val_accuracy: 0.9606
Epoch 11/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1070 - accuracy: 0.9705 - val_loss: 0.1438 - val_accuracy: 0.9596
Epoch 12/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1022 - accuracy: 0.9709 - val_loss: 0.1448 - val_accuracy: 0.9606
Epoch 13/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0985 - accuracy: 0.9723 - val_loss: 0.1436 - val_accuracy: 0.9604
Epoch 14/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0946 - accuracy: 0.9735 - val_loss: 0.1483 - val_accuracy: 0.9602
Epoch 15/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.0909 - accuracy: 0.9744 - val_loss: 0.1500 - val_accuracy: 0.9590
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1670 - accuracy: 0.9550
[CV] ..................................... n_neurons=28, total=  21.3s
[CV] n_neurons=29 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.4139 - accuracy: 0.8847 - val_loss: 0.2579 - val_accuracy: 0.9326
Epoch 2/30
36666/36666 [==============================] - 3s 85us/sample - loss: 0.2384 - accuracy: 0.9312 - val_loss: 0.2094 - val_accuracy: 0.9416
Epoch 3/30
36666/36666 [==============================] - 3s 91us/sample - loss: 0.1956 - accuracy: 0.9437 - val_loss: 0.1851 - val_accuracy: 0.9478
Epoch 4/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1703 - accuracy: 0.9506 - val_loss: 0.1756 - val_accuracy: 0.9466
Epoch 5/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1523 - accuracy: 0.9568 - val_loss: 0.1649 - val_accuracy: 0.9536
Epoch 6/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.1393 - accuracy: 0.9603 - val_loss: 0.1692 - val_accuracy: 0.9544
Epoch 7/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1285 - accuracy: 0.9626 - val_loss: 0.1591 - val_accuracy: 0.9534
Epoch 8/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1206 - accuracy: 0.9659 - val_loss: 0.1595 - val_accuracy: 0.9546
Epoch 9/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1122 - accuracy: 0.9678 - val_loss: 0.1572 - val_accuracy: 0.9536
Epoch 10/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1071 - accuracy: 0.9692 - val_loss: 0.1533 - val_accuracy: 0.9572
Epoch 11/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1005 - accuracy: 0.9713 - val_loss: 0.1591 - val_accuracy: 0.9558
Epoch 12/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0959 - accuracy: 0.9723 - val_loss: 0.1568 - val_accuracy: 0.9558
18334/18334 [==============================] - 0s 27us/sample - loss: 0.1692 - accuracy: 0.9555
[CV] ..................................... n_neurons=29, total=  22.6s
[CV] n_neurons=29 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.4431 - accuracy: 0.8760 - val_loss: 0.2673 - val_accuracy: 0.9210
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2532 - accuracy: 0.9293 - val_loss: 0.2169 - val_accuracy: 0.9368
Epoch 3/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.2075 - accuracy: 0.9408 - val_loss: 0.1964 - val_accuracy: 0.9448
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1804 - accuracy: 0.9496 - val_loss: 0.1776 - val_accuracy: 0.9488
Epoch 5/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1619 - accuracy: 0.9539 - val_loss: 0.1787 - val_accuracy: 0.9496
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1463 - accuracy: 0.9570 - val_loss: 0.1603 - val_accuracy: 0.9544
Epoch 7/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1340 - accuracy: 0.9611 - val_loss: 0.1552 - val_accuracy: 0.9580
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1242 - accuracy: 0.9635 - val_loss: 0.1535 - val_accuracy: 0.9558
Epoch 9/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1175 - accuracy: 0.9665 - val_loss: 0.1516 - val_accuracy: 0.9576
Epoch 10/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1102 - accuracy: 0.9686 - val_loss: 0.1539 - val_accuracy: 0.9574
Epoch 11/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1048 - accuracy: 0.9700 - val_loss: 0.1488 - val_accuracy: 0.9580
Epoch 12/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0978 - accuracy: 0.9718 - val_loss: 0.1553 - val_accuracy: 0.9572
Epoch 13/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0946 - accuracy: 0.9731 - val_loss: 0.1596 - val_accuracy: 0.9564
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1718 - accuracy: 0.9529
[CV] ..................................... n_neurons=29, total=  19.5s
[CV] n_neurons=29 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.4347 - accuracy: 0.8787 - val_loss: 0.2604 - val_accuracy: 0.9240
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2526 - accuracy: 0.9269 - val_loss: 0.2143 - val_accuracy: 0.9374
Epoch 3/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2086 - accuracy: 0.9393 - val_loss: 0.1852 - val_accuracy: 0.9492
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1793 - accuracy: 0.9483 - val_loss: 0.1688 - val_accuracy: 0.9532
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1587 - accuracy: 0.9538 - val_loss: 0.1612 - val_accuracy: 0.9578
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1445 - accuracy: 0.9588 - val_loss: 0.1640 - val_accuracy: 0.9528
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1331 - accuracy: 0.9620 - val_loss: 0.1435 - val_accuracy: 0.9602
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1232 - accuracy: 0.9646 - val_loss: 0.1403 - val_accuracy: 0.9618
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1161 - accuracy: 0.9671 - val_loss: 0.1484 - val_accuracy: 0.9592
Epoch 10/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1103 - accuracy: 0.9695 - val_loss: 0.1440 - val_accuracy: 0.9580
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1728 - accuracy: 0.9517
[CV] ..................................... n_neurons=29, total=  14.3s
[CV] n_neurons=30 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.4316 - accuracy: 0.8795 - val_loss: 0.2618 - val_accuracy: 0.9290
Epoch 2/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2468 - accuracy: 0.9296 - val_loss: 0.2132 - val_accuracy: 0.9390
Epoch 3/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1989 - accuracy: 0.9424 - val_loss: 0.1799 - val_accuracy: 0.9512
Epoch 4/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1703 - accuracy: 0.9506 - val_loss: 0.1671 - val_accuracy: 0.9518
Epoch 5/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1509 - accuracy: 0.9570 - val_loss: 0.1544 - val_accuracy: 0.9558
Epoch 6/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1377 - accuracy: 0.9607 - val_loss: 0.1544 - val_accuracy: 0.9544
Epoch 7/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1274 - accuracy: 0.9629 - val_loss: 0.1517 - val_accuracy: 0.9556
Epoch 8/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1190 - accuracy: 0.9656 - val_loss: 0.1546 - val_accuracy: 0.9538
Epoch 9/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1114 - accuracy: 0.9685 - val_loss: 0.1483 - val_accuracy: 0.9566
Epoch 10/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1054 - accuracy: 0.9700 - val_loss: 0.1459 - val_accuracy: 0.9576
Epoch 11/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1006 - accuracy: 0.9716 - val_loss: 0.1523 - val_accuracy: 0.9574
Epoch 12/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.0945 - accuracy: 0.9727 - val_loss: 0.1504 - val_accuracy: 0.9586
18334/18334 [==============================] - 0s 21us/sample - loss: 0.1669 - accuracy: 0.9561
[CV] ..................................... n_neurons=30, total=  16.9s
[CV] n_neurons=30 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.3958 - accuracy: 0.8924 - val_loss: 0.2336 - val_accuracy: 0.9300
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2158 - accuracy: 0.9375 - val_loss: 0.1866 - val_accuracy: 0.9490
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1764 - accuracy: 0.9494 - val_loss: 0.1684 - val_accuracy: 0.9498
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1527 - accuracy: 0.9557 - val_loss: 0.1535 - val_accuracy: 0.9548
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1365 - accuracy: 0.9607 - val_loss: 0.1524 - val_accuracy: 0.9560
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1232 - accuracy: 0.9648 - val_loss: 0.1396 - val_accuracy: 0.9596
Epoch 7/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1128 - accuracy: 0.9668 - val_loss: 0.1350 - val_accuracy: 0.9604
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1044 - accuracy: 0.9705 - val_loss: 0.1378 - val_accuracy: 0.9596
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.0997 - accuracy: 0.9712 - val_loss: 0.1321 - val_accuracy: 0.9632
Epoch 10/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0942 - accuracy: 0.9725 - val_loss: 0.1354 - val_accuracy: 0.9642
Epoch 11/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.0894 - accuracy: 0.9743 - val_loss: 0.1334 - val_accuracy: 0.9618
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1541 - accuracy: 0.9581
[CV] ..................................... n_neurons=30, total=  16.0s
[CV] n_neurons=30 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.4242 - accuracy: 0.8843 - val_loss: 0.2544 - val_accuracy: 0.9274
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2459 - accuracy: 0.9303 - val_loss: 0.2093 - val_accuracy: 0.9430
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2022 - accuracy: 0.9422 - val_loss: 0.1850 - val_accuracy: 0.9496
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1757 - accuracy: 0.9492 - val_loss: 0.1776 - val_accuracy: 0.9512
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1561 - accuracy: 0.9554 - val_loss: 0.1666 - val_accuracy: 0.9546
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1425 - accuracy: 0.9587 - val_loss: 0.1672 - val_accuracy: 0.9552
Epoch 7/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1305 - accuracy: 0.9627 - val_loss: 0.1508 - val_accuracy: 0.9586
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1219 - accuracy: 0.9657 - val_loss: 0.1530 - val_accuracy: 0.9598
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1150 - accuracy: 0.9678 - val_loss: 0.1500 - val_accuracy: 0.9610
Epoch 10/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1093 - accuracy: 0.9692 - val_loss: 0.1525 - val_accuracy: 0.9586
Epoch 11/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1030 - accuracy: 0.9721 - val_loss: 0.1540 - val_accuracy: 0.9594
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1677 - accuracy: 0.9546s - loss: 0.1790 - accuracy: 0.
[CV] ..................................... n_neurons=30, total=  15.9s
[CV] n_neurons=31 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.4276 - accuracy: 0.8803 - val_loss: 0.2679 - val_accuracy: 0.9304
Epoch 2/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.2473 - accuracy: 0.9299 - val_loss: 0.2185 - val_accuracy: 0.9392
Epoch 3/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.2029 - accuracy: 0.9413 - val_loss: 0.1926 - val_accuracy: 0.9470
Epoch 4/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1746 - accuracy: 0.9504 - val_loss: 0.1803 - val_accuracy: 0.9482
Epoch 5/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1544 - accuracy: 0.9559 - val_loss: 0.1665 - val_accuracy: 0.9536
Epoch 6/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1387 - accuracy: 0.9604 - val_loss: 0.1614 - val_accuracy: 0.9540
Epoch 7/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1274 - accuracy: 0.9639 - val_loss: 0.1589 - val_accuracy: 0.9546
Epoch 8/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1185 - accuracy: 0.9662 - val_loss: 0.1646 - val_accuracy: 0.9538
Epoch 9/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1099 - accuracy: 0.9693 - val_loss: 0.1547 - val_accuracy: 0.9590
Epoch 10/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1046 - accuracy: 0.9706 - val_loss: 0.1550 - val_accuracy: 0.9580
Epoch 11/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.0980 - accuracy: 0.9727 - val_loss: 0.1615 - val_accuracy: 0.9574
18334/18334 [==============================] - 0s 21us/sample - loss: 0.1724 - accuracy: 0.9526
[CV] ..................................... n_neurons=31, total=  15.7s
[CV] n_neurons=31 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4186 - accuracy: 0.8849 - val_loss: 0.2542 - val_accuracy: 0.9250
Epoch 2/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.2348 - accuracy: 0.9331 - val_loss: 0.2020 - val_accuracy: 0.9420
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1919 - accuracy: 0.9453 - val_loss: 0.1793 - val_accuracy: 0.9462
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1668 - accuracy: 0.9518 - val_loss: 0.1620 - val_accuracy: 0.9508
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1507 - accuracy: 0.9564 - val_loss: 0.1639 - val_accuracy: 0.9514
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1369 - accuracy: 0.9602 - val_loss: 0.1504 - val_accuracy: 0.9560
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1265 - accuracy: 0.9623 - val_loss: 0.1489 - val_accuracy: 0.9552
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1180 - accuracy: 0.9655 - val_loss: 0.1434 - val_accuracy: 0.9566
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1119 - accuracy: 0.9674 - val_loss: 0.1432 - val_accuracy: 0.9572
Epoch 10/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1062 - accuracy: 0.9693 - val_loss: 0.1419 - val_accuracy: 0.9604
Epoch 11/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1013 - accuracy: 0.9710 - val_loss: 0.1435 - val_accuracy: 0.9576
Epoch 12/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0955 - accuracy: 0.9727 - val_loss: 0.1487 - val_accuracy: 0.9602
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1796 - accuracy: 0.9518
[CV] ..................................... n_neurons=31, total=  16.9s
[CV] n_neurons=31 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4251 - accuracy: 0.8815 - val_loss: 0.2482 - val_accuracy: 0.9286
Epoch 2/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.2356 - accuracy: 0.9316 - val_loss: 0.2000 - val_accuracy: 0.9458
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1938 - accuracy: 0.9438 - val_loss: 0.1763 - val_accuracy: 0.9500
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1698 - accuracy: 0.9510 - val_loss: 0.1686 - val_accuracy: 0.9516
Epoch 5/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.1511 - accuracy: 0.9574 - val_loss: 0.1583 - val_accuracy: 0.9562
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1381 - accuracy: 0.9604 - val_loss: 0.1570 - val_accuracy: 0.9552
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1267 - accuracy: 0.9644 - val_loss: 0.1428 - val_accuracy: 0.9596
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1178 - accuracy: 0.9665 - val_loss: 0.1404 - val_accuracy: 0.9632
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1114 - accuracy: 0.9686 - val_loss: 0.1447 - val_accuracy: 0.9594
Epoch 10/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1055 - accuracy: 0.9701 - val_loss: 0.1402 - val_accuracy: 0.9608
Epoch 11/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.0995 - accuracy: 0.9724 - val_loss: 0.1426 - val_accuracy: 0.9628
Epoch 12/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.0956 - accuracy: 0.9737 - val_loss: 0.1346 - val_accuracy: 0.9652
Epoch 13/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0909 - accuracy: 0.9743 - val_loss: 0.1466 - val_accuracy: 0.9592
Epoch 14/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.0859 - accuracy: 0.9759 - val_loss: 0.1450 - val_accuracy: 0.9638
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1542 - accuracy: 0.9588
[CV] ..................................... n_neurons=31, total=  19.4s
[CV] n_neurons=32 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.4214 - accuracy: 0.8827 - val_loss: 0.2608 - val_accuracy: 0.9308
Epoch 2/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.2429 - accuracy: 0.9309 - val_loss: 0.2130 - val_accuracy: 0.9400
Epoch 3/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1986 - accuracy: 0.9426 - val_loss: 0.1803 - val_accuracy: 0.9498
Epoch 4/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1716 - accuracy: 0.9500 - val_loss: 0.1689 - val_accuracy: 0.9540
Epoch 5/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1526 - accuracy: 0.9562 - val_loss: 0.1551 - val_accuracy: 0.9570
Epoch 6/30
36666/36666 [==============================] - 1s 35us/sample - loss: 0.1374 - accuracy: 0.9601 - val_loss: 0.1467 - val_accuracy: 0.9598
Epoch 7/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1261 - accuracy: 0.9641 - val_loss: 0.1462 - val_accuracy: 0.9590
Epoch 8/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1169 - accuracy: 0.9666 - val_loss: 0.1488 - val_accuracy: 0.9552
Epoch 9/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1087 - accuracy: 0.9686 - val_loss: 0.1413 - val_accuracy: 0.9580
Epoch 10/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1031 - accuracy: 0.9699 - val_loss: 0.1434 - val_accuracy: 0.9606
Epoch 11/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.0959 - accuracy: 0.9722 - val_loss: 0.1501 - val_accuracy: 0.9586
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1591 - accuracy: 0.9558
[CV] ..................................... n_neurons=32, total=  16.5s
[CV] n_neurons=32 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.4353 - accuracy: 0.8796 - val_loss: 0.2626 - val_accuracy: 0.9222
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.2459 - accuracy: 0.9296 - val_loss: 0.2155 - val_accuracy: 0.9390s - loss: 0.2466 - accuracy: 0.92
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2041 - accuracy: 0.9423 - val_loss: 0.1885 - val_accuracy: 0.9466
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1754 - accuracy: 0.9495 - val_loss: 0.1716 - val_accuracy: 0.9494
Epoch 5/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1557 - accuracy: 0.9550 - val_loss: 0.1719 - val_accuracy: 0.9484
Epoch 6/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1399 - accuracy: 0.9594 - val_loss: 0.1538 - val_accuracy: 0.9582
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1280 - accuracy: 0.9634 - val_loss: 0.1459 - val_accuracy: 0.9564
Epoch 8/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1176 - accuracy: 0.9660 - val_loss: 0.1440 - val_accuracy: 0.9596
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1104 - accuracy: 0.9686 - val_loss: 0.1411 - val_accuracy: 0.9604
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1032 - accuracy: 0.9708 - val_loss: 0.1383 - val_accuracy: 0.9622
Epoch 11/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0967 - accuracy: 0.9725 - val_loss: 0.1364 - val_accuracy: 0.9604
Epoch 12/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0913 - accuracy: 0.9733 - val_loss: 0.1420 - val_accuracy: 0.9610
Epoch 13/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0877 - accuracy: 0.9749 - val_loss: 0.1352 - val_accuracy: 0.9614
Epoch 14/30
36667/36667 [==============================] - 1s 35us/sample - loss: 0.0818 - accuracy: 0.9763 - val_loss: 0.1292 - val_accuracy: 0.9618
Epoch 15/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0783 - accuracy: 0.9776 - val_loss: 0.1352 - val_accuracy: 0.9624
Epoch 16/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0754 - accuracy: 0.9785 - val_loss: 0.1313 - val_accuracy: 0.9612
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1691 - accuracy: 0.9561
[CV] ..................................... n_neurons=32, total=  22.9s
[CV] n_neurons=32 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.4208 - accuracy: 0.8824 - val_loss: 0.2456 - val_accuracy: 0.9352
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.2297 - accuracy: 0.9334 - val_loss: 0.2019 - val_accuracy: 0.9394
Epoch 3/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1843 - accuracy: 0.9464 - val_loss: 0.1712 - val_accuracy: 0.9534
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1596 - accuracy: 0.9543 - val_loss: 0.1694 - val_accuracy: 0.9524
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1426 - accuracy: 0.9599 - val_loss: 0.1552 - val_accuracy: 0.9582
Epoch 6/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1292 - accuracy: 0.9635 - val_loss: 0.1571 - val_accuracy: 0.9570
Epoch 7/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1189 - accuracy: 0.9664 - val_loss: 0.1387 - val_accuracy: 0.9622
Epoch 8/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1096 - accuracy: 0.9691 - val_loss: 0.1450 - val_accuracy: 0.9614
Epoch 9/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1031 - accuracy: 0.9708 - val_loss: 0.1433 - val_accuracy: 0.9612
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1513 - accuracy: 0.9561
[CV] ..................................... n_neurons=32, total=  13.8s
[CV] n_neurons=33 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.4033 - accuracy: 0.8868 - val_loss: 0.2450 - val_accuracy: 0.9354
Epoch 2/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.2282 - accuracy: 0.9344 - val_loss: 0.2019 - val_accuracy: 0.9456
Epoch 3/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1880 - accuracy: 0.9449 - val_loss: 0.1757 - val_accuracy: 0.9502
Epoch 4/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1609 - accuracy: 0.9533 - val_loss: 0.1711 - val_accuracy: 0.9524
Epoch 5/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1441 - accuracy: 0.9583 - val_loss: 0.1573 - val_accuracy: 0.9582
Epoch 6/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1304 - accuracy: 0.9626 - val_loss: 0.1531 - val_accuracy: 0.9582
Epoch 7/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1194 - accuracy: 0.9655 - val_loss: 0.1536 - val_accuracy: 0.9596
Epoch 8/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1116 - accuracy: 0.9680 - val_loss: 0.1467 - val_accuracy: 0.9622
Epoch 9/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1039 - accuracy: 0.9707 - val_loss: 0.1470 - val_accuracy: 0.9606
Epoch 10/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0976 - accuracy: 0.9716 - val_loss: 0.1426 - val_accuracy: 0.9630
Epoch 11/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0917 - accuracy: 0.9741 - val_loss: 0.1450 - val_accuracy: 0.9628
Epoch 12/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0856 - accuracy: 0.9757 - val_loss: 0.1501 - val_accuracy: 0.9602
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1635 - accuracy: 0.9567
[CV] ..................................... n_neurons=33, total=  17.7s
[CV] n_neurons=33 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.4058 - accuracy: 0.8882 - val_loss: 0.2458 - val_accuracy: 0.9318
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2370 - accuracy: 0.9337 - val_loss: 0.2040 - val_accuracy: 0.9440
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1957 - accuracy: 0.9452 - val_loss: 0.1799 - val_accuracy: 0.9512
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1689 - accuracy: 0.9533 - val_loss: 0.1680 - val_accuracy: 0.9526
Epoch 5/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1506 - accuracy: 0.9582 - val_loss: 0.1685 - val_accuracy: 0.9540
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1353 - accuracy: 0.9623 - val_loss: 0.1521 - val_accuracy: 0.9554
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1232 - accuracy: 0.9650 - val_loss: 0.1471 - val_accuracy: 0.9576
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1138 - accuracy: 0.9687 - val_loss: 0.1447 - val_accuracy: 0.9606
Epoch 9/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1055 - accuracy: 0.9701 - val_loss: 0.1470 - val_accuracy: 0.9608
Epoch 10/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0993 - accuracy: 0.9724 - val_loss: 0.1482 - val_accuracy: 0.9608
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1600 - accuracy: 0.9565
[CV] ..................................... n_neurons=33, total=  16.0s
[CV] n_neurons=33 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4079 - accuracy: 0.8867 - val_loss: 0.2264 - val_accuracy: 0.9334
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2189 - accuracy: 0.9361 - val_loss: 0.1888 - val_accuracy: 0.9474
Epoch 3/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1772 - accuracy: 0.9475 - val_loss: 0.1586 - val_accuracy: 0.9532
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1531 - accuracy: 0.9561 - val_loss: 0.1480 - val_accuracy: 0.9572
Epoch 5/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1350 - accuracy: 0.9611 - val_loss: 0.1440 - val_accuracy: 0.9592
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1224 - accuracy: 0.9645 - val_loss: 0.1388 - val_accuracy: 0.9624
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1120 - accuracy: 0.9681 - val_loss: 0.1360 - val_accuracy: 0.9612
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1029 - accuracy: 0.9699 - val_loss: 0.1331 - val_accuracy: 0.9642
Epoch 9/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0966 - accuracy: 0.9722 - val_loss: 0.1318 - val_accuracy: 0.9656
Epoch 10/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0910 - accuracy: 0.9741 - val_loss: 0.1360 - val_accuracy: 0.9632
Epoch 11/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0845 - accuracy: 0.9761 - val_loss: 0.1323 - val_accuracy: 0.9644
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1451 - accuracy: 0.9613
[CV] ..................................... n_neurons=33, total=  16.6s
[CV] n_neurons=34 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 61us/sample - loss: 0.4106 - accuracy: 0.8844 - val_loss: 0.2437 - val_accuracy: 0.9338
Epoch 2/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.2303 - accuracy: 0.9324 - val_loss: 0.1981 - val_accuracy: 0.9464
Epoch 3/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1877 - accuracy: 0.9445 - val_loss: 0.1739 - val_accuracy: 0.9506
Epoch 4/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1614 - accuracy: 0.9529 - val_loss: 0.1649 - val_accuracy: 0.9498
Epoch 5/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1439 - accuracy: 0.9587 - val_loss: 0.1552 - val_accuracy: 0.9560
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1313 - accuracy: 0.9630 - val_loss: 0.1558 - val_accuracy: 0.9536
Epoch 7/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1209 - accuracy: 0.9648 - val_loss: 0.1432 - val_accuracy: 0.9582
Epoch 8/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1116 - accuracy: 0.9681 - val_loss: 0.1486 - val_accuracy: 0.9578
Epoch 9/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1052 - accuracy: 0.9694 - val_loss: 0.1410 - val_accuracy: 0.9626
Epoch 10/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.0996 - accuracy: 0.9712 - val_loss: 0.1421 - val_accuracy: 0.9622
Epoch 11/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.0932 - accuracy: 0.9731 - val_loss: 0.1533 - val_accuracy: 0.9576
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1695 - accuracy: 0.9551
[CV] ..................................... n_neurons=34, total=  16.9s
[CV] n_neurons=34 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4025 - accuracy: 0.8903 - val_loss: 0.2509 - val_accuracy: 0.9278
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2347 - accuracy: 0.9333 - val_loss: 0.1986 - val_accuracy: 0.9446
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1912 - accuracy: 0.9451 - val_loss: 0.1777 - val_accuracy: 0.9502
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1644 - accuracy: 0.9538 - val_loss: 0.1570 - val_accuracy: 0.9534
Epoch 5/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1460 - accuracy: 0.9587 - val_loss: 0.1609 - val_accuracy: 0.9542
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1314 - accuracy: 0.9624 - val_loss: 0.1493 - val_accuracy: 0.9578
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1196 - accuracy: 0.9658 - val_loss: 0.1437 - val_accuracy: 0.9570
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1103 - accuracy: 0.9691 - val_loss: 0.1446 - val_accuracy: 0.9588
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1034 - accuracy: 0.9714 - val_loss: 0.1417 - val_accuracy: 0.9596
Epoch 10/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0958 - accuracy: 0.9729 - val_loss: 0.1454 - val_accuracy: 0.9590
Epoch 11/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0902 - accuracy: 0.9755 - val_loss: 0.1426 - val_accuracy: 0.9572
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1649 - accuracy: 0.9558
[CV] ..................................... n_neurons=34, total=  16.0s
[CV] n_neurons=34 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4096 - accuracy: 0.8874 - val_loss: 0.2431 - val_accuracy: 0.9282
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2304 - accuracy: 0.9328 - val_loss: 0.1896 - val_accuracy: 0.9448
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1875 - accuracy: 0.9456 - val_loss: 0.1650 - val_accuracy: 0.9532
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1619 - accuracy: 0.9524 - val_loss: 0.1505 - val_accuracy: 0.9566
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1434 - accuracy: 0.9584 - val_loss: 0.1488 - val_accuracy: 0.9576
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1295 - accuracy: 0.9624 - val_loss: 0.1430 - val_accuracy: 0.9584
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1180 - accuracy: 0.9659 - val_loss: 0.1368 - val_accuracy: 0.9608
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1100 - accuracy: 0.9687 - val_loss: 0.1332 - val_accuracy: 0.9624
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1025 - accuracy: 0.9709 - val_loss: 0.1262 - val_accuracy: 0.9654
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0969 - accuracy: 0.9727 - val_loss: 0.1372 - val_accuracy: 0.9610
Epoch 11/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0907 - accuracy: 0.9749 - val_loss: 0.1336 - val_accuracy: 0.9612
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1575 - accuracy: 0.9585
[CV] ..................................... n_neurons=34, total=  16.0s
[CV] n_neurons=35 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.4175 - accuracy: 0.8842 - val_loss: 0.2457 - val_accuracy: 0.9328
Epoch 2/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.2292 - accuracy: 0.9340 - val_loss: 0.2010 - val_accuracy: 0.9434
Epoch 3/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1870 - accuracy: 0.9465 - val_loss: 0.1744 - val_accuracy: 0.9506
Epoch 4/30
36666/36666 [==============================] - 1s 36us/sample - loss: 0.1605 - accuracy: 0.9528 - val_loss: 0.1697 - val_accuracy: 0.9502
Epoch 5/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1421 - accuracy: 0.9594 - val_loss: 0.1569 - val_accuracy: 0.9558
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1272 - accuracy: 0.9632 - val_loss: 0.1444 - val_accuracy: 0.9598
Epoch 7/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1164 - accuracy: 0.9666 - val_loss: 0.1392 - val_accuracy: 0.9626
Epoch 8/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1067 - accuracy: 0.9700 - val_loss: 0.1432 - val_accuracy: 0.9600
Epoch 9/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0983 - accuracy: 0.9724 - val_loss: 0.1353 - val_accuracy: 0.9624
Epoch 10/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.0917 - accuracy: 0.9742 - val_loss: 0.1398 - val_accuracy: 0.9628
Epoch 11/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.0871 - accuracy: 0.9759 - val_loss: 0.1439 - val_accuracy: 0.9642
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1526 - accuracy: 0.9595
[CV] ..................................... n_neurons=35, total=  16.1s
[CV] n_neurons=35 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.4041 - accuracy: 0.8868 - val_loss: 0.2506 - val_accuracy: 0.9248
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2257 - accuracy: 0.9365 - val_loss: 0.2001 - val_accuracy: 0.9424
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1802 - accuracy: 0.9494 - val_loss: 0.1741 - val_accuracy: 0.9510
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1529 - accuracy: 0.9566 - val_loss: 0.1519 - val_accuracy: 0.9558
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1339 - accuracy: 0.9621 - val_loss: 0.1526 - val_accuracy: 0.9560
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1203 - accuracy: 0.9650 - val_loss: 0.1348 - val_accuracy: 0.9618
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1091 - accuracy: 0.9689 - val_loss: 0.1295 - val_accuracy: 0.9634
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1004 - accuracy: 0.9718 - val_loss: 0.1313 - val_accuracy: 0.9624
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0949 - accuracy: 0.9732 - val_loss: 0.1329 - val_accuracy: 0.9628
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1577 - accuracy: 0.9555
[CV] ..................................... n_neurons=35, total=  13.2s
[CV] n_neurons=35 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.4232 - accuracy: 0.8838 - val_loss: 0.2474 - val_accuracy: 0.9288
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.2339 - accuracy: 0.9330 - val_loss: 0.1962 - val_accuracy: 0.9432
Epoch 3/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1903 - accuracy: 0.9442 - val_loss: 0.1727 - val_accuracy: 0.9494
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1626 - accuracy: 0.9528 - val_loss: 0.1640 - val_accuracy: 0.9552
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1452 - accuracy: 0.9578 - val_loss: 0.1567 - val_accuracy: 0.9590
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1312 - accuracy: 0.9620 - val_loss: 0.1559 - val_accuracy: 0.9594
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1212 - accuracy: 0.9650 - val_loss: 0.1423 - val_accuracy: 0.9574
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1115 - accuracy: 0.9669 - val_loss: 0.1450 - val_accuracy: 0.9620
Epoch 9/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1055 - accuracy: 0.9695 - val_loss: 0.1449 - val_accuracy: 0.9626
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1676 - accuracy: 0.9534
[CV] ..................................... n_neurons=35, total=  13.8s
[CV] n_neurons=36 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.4129 - accuracy: 0.8853 - val_loss: 0.2341 - val_accuracy: 0.9386
Epoch 2/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.2147 - accuracy: 0.9380 - val_loss: 0.1873 - val_accuracy: 0.9474
Epoch 3/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1721 - accuracy: 0.9507 - val_loss: 0.1552 - val_accuracy: 0.9566
Epoch 4/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1471 - accuracy: 0.9580 - val_loss: 0.1477 - val_accuracy: 0.9588
Epoch 5/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1310 - accuracy: 0.9630 - val_loss: 0.1390 - val_accuracy: 0.9616
Epoch 6/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1188 - accuracy: 0.9655 - val_loss: 0.1423 - val_accuracy: 0.9602
Epoch 7/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1093 - accuracy: 0.9691 - val_loss: 0.1324 - val_accuracy: 0.9634
Epoch 8/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1013 - accuracy: 0.9712 - val_loss: 0.1336 - val_accuracy: 0.9614
Epoch 9/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0939 - accuracy: 0.9730 - val_loss: 0.1283 - val_accuracy: 0.9644
Epoch 10/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0877 - accuracy: 0.9743 - val_loss: 0.1353 - val_accuracy: 0.9630
Epoch 11/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0829 - accuracy: 0.9764 - val_loss: 0.1390 - val_accuracy: 0.9624
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1493 - accuracy: 0.9590
[CV] ..................................... n_neurons=36, total=  16.9s
[CV] n_neurons=36 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.3976 - accuracy: 0.8895 - val_loss: 0.2307 - val_accuracy: 0.9314
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2092 - accuracy: 0.9403 - val_loss: 0.1803 - val_accuracy: 0.9478
Epoch 3/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1673 - accuracy: 0.9518 - val_loss: 0.1545 - val_accuracy: 0.9544
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1441 - accuracy: 0.9589 - val_loss: 0.1388 - val_accuracy: 0.9584
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1282 - accuracy: 0.9629 - val_loss: 0.1454 - val_accuracy: 0.9588
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1159 - accuracy: 0.9668 - val_loss: 0.1316 - val_accuracy: 0.9624
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1065 - accuracy: 0.9690 - val_loss: 0.1256 - val_accuracy: 0.9640
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0977 - accuracy: 0.9724 - val_loss: 0.1290 - val_accuracy: 0.9640
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0919 - accuracy: 0.9742 - val_loss: 0.1234 - val_accuracy: 0.9666
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0863 - accuracy: 0.9758 - val_loss: 0.1302 - val_accuracy: 0.9630
Epoch 11/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.0813 - accuracy: 0.9775 - val_loss: 0.1234 - val_accuracy: 0.9644
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1525 - accuracy: 0.9602
[CV] ..................................... n_neurons=36, total=  15.9s
[CV] n_neurons=36 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.4228 - accuracy: 0.8831 - val_loss: 0.2385 - val_accuracy: 0.9328
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2296 - accuracy: 0.9348 - val_loss: 0.1943 - val_accuracy: 0.9478
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1880 - accuracy: 0.9459 - val_loss: 0.1682 - val_accuracy: 0.9546
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1628 - accuracy: 0.9536 - val_loss: 0.1578 - val_accuracy: 0.9572
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1427 - accuracy: 0.9594 - val_loss: 0.1486 - val_accuracy: 0.9622
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1284 - accuracy: 0.9631 - val_loss: 0.1534 - val_accuracy: 0.9606
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1170 - accuracy: 0.9661 - val_loss: 0.1308 - val_accuracy: 0.9658
Epoch 8/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1071 - accuracy: 0.9696 - val_loss: 0.1305 - val_accuracy: 0.9658
Epoch 9/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1008 - accuracy: 0.9711 - val_loss: 0.1295 - val_accuracy: 0.9670
Epoch 10/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0955 - accuracy: 0.9734 - val_loss: 0.1275 - val_accuracy: 0.9640
Epoch 11/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0884 - accuracy: 0.9753 - val_loss: 0.1374 - val_accuracy: 0.9642
Epoch 12/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0848 - accuracy: 0.9761 - val_loss: 0.1260 - val_accuracy: 0.9666
Epoch 13/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0810 - accuracy: 0.9767 - val_loss: 0.1306 - val_accuracy: 0.9626
Epoch 14/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0763 - accuracy: 0.9785 - val_loss: 0.1343 - val_accuracy: 0.9664
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1489 - accuracy: 0.9624
[CV] ..................................... n_neurons=36, total=  20.5s
[CV] n_neurons=37 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.4123 - accuracy: 0.8859 - val_loss: 0.2462 - val_accuracy: 0.9326
Epoch 2/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.2264 - accuracy: 0.9344 - val_loss: 0.2043 - val_accuracy: 0.9416
Epoch 3/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1826 - accuracy: 0.9470 - val_loss: 0.1692 - val_accuracy: 0.9532
Epoch 4/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1539 - accuracy: 0.9549 - val_loss: 0.1599 - val_accuracy: 0.9538
Epoch 5/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1354 - accuracy: 0.9608 - val_loss: 0.1550 - val_accuracy: 0.9552
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1221 - accuracy: 0.9642 - val_loss: 0.1490 - val_accuracy: 0.9608
Epoch 7/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1102 - accuracy: 0.9675 - val_loss: 0.1461 - val_accuracy: 0.9584
Epoch 8/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1023 - accuracy: 0.9698 - val_loss: 0.1460 - val_accuracy: 0.9598
Epoch 9/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.0950 - accuracy: 0.9720 - val_loss: 0.1407 - val_accuracy: 0.9610
Epoch 10/30
36666/36666 [==============================] - 2s 66us/sample - loss: 0.0893 - accuracy: 0.9745 - val_loss: 0.1419 - val_accuracy: 0.9618
Epoch 11/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0839 - accuracy: 0.9756 - val_loss: 0.1478 - val_accuracy: 0.9618
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1557 - accuracy: 0.9566
[CV] ..................................... n_neurons=37, total=  17.2s
[CV] n_neurons=37 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.4128 - accuracy: 0.8857 - val_loss: 0.2525 - val_accuracy: 0.9254
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2310 - accuracy: 0.9349 - val_loss: 0.1991 - val_accuracy: 0.9424
Epoch 3/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1863 - accuracy: 0.9473 - val_loss: 0.1726 - val_accuracy: 0.9514
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1575 - accuracy: 0.9549 - val_loss: 0.1564 - val_accuracy: 0.9538
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1374 - accuracy: 0.9606 - val_loss: 0.1588 - val_accuracy: 0.9550
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1226 - accuracy: 0.9647 - val_loss: 0.1447 - val_accuracy: 0.9618
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1121 - accuracy: 0.9677 - val_loss: 0.1394 - val_accuracy: 0.9598
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1031 - accuracy: 0.9702 - val_loss: 0.1360 - val_accuracy: 0.9638
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0970 - accuracy: 0.9722 - val_loss: 0.1412 - val_accuracy: 0.9618
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0903 - accuracy: 0.9748 - val_loss: 0.1381 - val_accuracy: 0.9632
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1548 - accuracy: 0.9571
[CV] ..................................... n_neurons=37, total=  14.9s
[CV] n_neurons=37 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.4087 - accuracy: 0.8856 - val_loss: 0.2437 - val_accuracy: 0.9298
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2208 - accuracy: 0.9358 - val_loss: 0.1839 - val_accuracy: 0.9466
Epoch 3/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1707 - accuracy: 0.9495 - val_loss: 0.1548 - val_accuracy: 0.9566
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1425 - accuracy: 0.9589 - val_loss: 0.1443 - val_accuracy: 0.9596
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1235 - accuracy: 0.9641 - val_loss: 0.1348 - val_accuracy: 0.9598
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1102 - accuracy: 0.9684 - val_loss: 0.1463 - val_accuracy: 0.9582
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1004 - accuracy: 0.9711 - val_loss: 0.1234 - val_accuracy: 0.9618
Epoch 8/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0913 - accuracy: 0.9738 - val_loss: 0.1216 - val_accuracy: 0.9644
Epoch 9/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0856 - accuracy: 0.9764 - val_loss: 0.1246 - val_accuracy: 0.9616
Epoch 10/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0801 - accuracy: 0.9778 - val_loss: 0.1321 - val_accuracy: 0.9626
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1451 - accuracy: 0.9615
[CV] ..................................... n_neurons=37, total=  14.6s
[CV] n_neurons=38 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.4065 - accuracy: 0.8859 - val_loss: 0.2638 - val_accuracy: 0.9278
Epoch 2/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.2437 - accuracy: 0.9300 - val_loss: 0.2116 - val_accuracy: 0.9378
Epoch 3/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1930 - accuracy: 0.9442 - val_loss: 0.1766 - val_accuracy: 0.9504
Epoch 4/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1625 - accuracy: 0.9528 - val_loss: 0.1633 - val_accuracy: 0.9518
Epoch 5/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1418 - accuracy: 0.9582 - val_loss: 0.1486 - val_accuracy: 0.9604
Epoch 6/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1269 - accuracy: 0.9629 - val_loss: 0.1508 - val_accuracy: 0.9554
Epoch 7/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1162 - accuracy: 0.9651 - val_loss: 0.1344 - val_accuracy: 0.9622
Epoch 8/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1063 - accuracy: 0.9697 - val_loss: 0.1354 - val_accuracy: 0.9604
Epoch 9/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0979 - accuracy: 0.9724 - val_loss: 0.1351 - val_accuracy: 0.9606
18334/18334 [==============================] - 0s 21us/sample - loss: 0.1592 - accuracy: 0.9578
[CV] ..................................... n_neurons=38, total=  13.1s
[CV] n_neurons=38 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.4042 - accuracy: 0.8883 - val_loss: 0.2521 - val_accuracy: 0.9286
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2284 - accuracy: 0.9349 - val_loss: 0.1970 - val_accuracy: 0.9418
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1811 - accuracy: 0.9482 - val_loss: 0.1713 - val_accuracy: 0.9508
Epoch 4/30
36667/36667 [==============================] - 1s 36us/sample - loss: 0.1544 - accuracy: 0.9555 - val_loss: 0.1519 - val_accuracy: 0.9570
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1358 - accuracy: 0.9609 - val_loss: 0.1603 - val_accuracy: 0.9564
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1205 - accuracy: 0.9651 - val_loss: 0.1430 - val_accuracy: 0.9592
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1101 - accuracy: 0.9677 - val_loss: 0.1379 - val_accuracy: 0.9606
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1014 - accuracy: 0.9711 - val_loss: 0.1343 - val_accuracy: 0.9634
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0948 - accuracy: 0.9728 - val_loss: 0.1341 - val_accuracy: 0.9628
Epoch 10/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0882 - accuracy: 0.9744 - val_loss: 0.1368 - val_accuracy: 0.9626
Epoch 11/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0835 - accuracy: 0.9757 - val_loss: 0.1300 - val_accuracy: 0.9644
Epoch 12/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0772 - accuracy: 0.9773 - val_loss: 0.1328 - val_accuracy: 0.9640
Epoch 13/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0739 - accuracy: 0.9792 - val_loss: 0.1301 - val_accuracy: 0.9654
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1548 - accuracy: 0.9579
[CV] ..................................... n_neurons=38, total=  18.6s
[CV] n_neurons=38 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.4012 - accuracy: 0.8907 - val_loss: 0.2384 - val_accuracy: 0.9308
Epoch 2/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.2232 - accuracy: 0.9358 - val_loss: 0.1938 - val_accuracy: 0.9450
Epoch 3/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1792 - accuracy: 0.9477 - val_loss: 0.1592 - val_accuracy: 0.9528
Epoch 4/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1521 - accuracy: 0.9559 - val_loss: 0.1554 - val_accuracy: 0.9554
Epoch 5/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1337 - accuracy: 0.9612 - val_loss: 0.1477 - val_accuracy: 0.9570
Epoch 6/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1199 - accuracy: 0.9658 - val_loss: 0.1419 - val_accuracy: 0.9584
Epoch 7/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1086 - accuracy: 0.9683 - val_loss: 0.1417 - val_accuracy: 0.9592
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0995 - accuracy: 0.9717 - val_loss: 0.1348 - val_accuracy: 0.9628
Epoch 9/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0939 - accuracy: 0.9732 - val_loss: 0.1377 - val_accuracy: 0.9620
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0873 - accuracy: 0.9756 - val_loss: 0.1397 - val_accuracy: 0.9604
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1616 - accuracy: 0.9569
[CV] ..................................... n_neurons=38, total=  16.2s
[CV] n_neurons=39 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.4033 - accuracy: 0.8868 - val_loss: 0.2535 - val_accuracy: 0.9294
Epoch 2/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.2146 - accuracy: 0.9383 - val_loss: 0.1877 - val_accuracy: 0.9452
Epoch 3/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1697 - accuracy: 0.9510 - val_loss: 0.1541 - val_accuracy: 0.9582
Epoch 4/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1445 - accuracy: 0.9586 - val_loss: 0.1504 - val_accuracy: 0.9548
Epoch 5/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1274 - accuracy: 0.9637 - val_loss: 0.1398 - val_accuracy: 0.9620
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1153 - accuracy: 0.9666 - val_loss: 0.1355 - val_accuracy: 0.9636
Epoch 7/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1047 - accuracy: 0.9696 - val_loss: 0.1384 - val_accuracy: 0.9624
Epoch 8/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0982 - accuracy: 0.9728 - val_loss: 0.1347 - val_accuracy: 0.9634
Epoch 9/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0897 - accuracy: 0.9747 - val_loss: 0.1343 - val_accuracy: 0.9652
Epoch 10/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0850 - accuracy: 0.9760 - val_loss: 0.1275 - val_accuracy: 0.9672
Epoch 11/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0788 - accuracy: 0.9780 - val_loss: 0.1382 - val_accuracy: 0.9654
Epoch 12/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0749 - accuracy: 0.9791 - val_loss: 0.1359 - val_accuracy: 0.9660
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1492 - accuracy: 0.9616
[CV] ..................................... n_neurons=39, total=  18.0s
[CV] n_neurons=39 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.4019 - accuracy: 0.8886 - val_loss: 0.2391 - val_accuracy: 0.9318
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2258 - accuracy: 0.9366 - val_loss: 0.1908 - val_accuracy: 0.9462
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1801 - accuracy: 0.9485 - val_loss: 0.1658 - val_accuracy: 0.9520
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1524 - accuracy: 0.9563 - val_loss: 0.1496 - val_accuracy: 0.9564
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1336 - accuracy: 0.9625 - val_loss: 0.1501 - val_accuracy: 0.9576
Epoch 6/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1187 - accuracy: 0.9660 - val_loss: 0.1330 - val_accuracy: 0.9624
Epoch 7/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1074 - accuracy: 0.9684 - val_loss: 0.1293 - val_accuracy: 0.9638
Epoch 8/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0982 - accuracy: 0.9713 - val_loss: 0.1291 - val_accuracy: 0.9638
Epoch 9/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0911 - accuracy: 0.9734 - val_loss: 0.1273 - val_accuracy: 0.9628
Epoch 10/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0845 - accuracy: 0.9761 - val_loss: 0.1296 - val_accuracy: 0.9632
Epoch 11/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0796 - accuracy: 0.9774 - val_loss: 0.1241 - val_accuracy: 0.9648
Epoch 12/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0731 - accuracy: 0.9789 - val_loss: 0.1351 - val_accuracy: 0.9640
Epoch 13/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0708 - accuracy: 0.9797 - val_loss: 0.1320 - val_accuracy: 0.9648
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1515 - accuracy: 0.9615
[CV] ..................................... n_neurons=39, total=  19.0s
[CV] n_neurons=39 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.4105 - accuracy: 0.8865 - val_loss: 0.2464 - val_accuracy: 0.9294
Epoch 2/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.2304 - accuracy: 0.9334 - val_loss: 0.1929 - val_accuracy: 0.9458
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1842 - accuracy: 0.9464 - val_loss: 0.1673 - val_accuracy: 0.9530
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1566 - accuracy: 0.9543 - val_loss: 0.1558 - val_accuracy: 0.9552
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1369 - accuracy: 0.9604 - val_loss: 0.1460 - val_accuracy: 0.9574
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1230 - accuracy: 0.9647 - val_loss: 0.1493 - val_accuracy: 0.9578
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1120 - accuracy: 0.9682 - val_loss: 0.1394 - val_accuracy: 0.9596
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1031 - accuracy: 0.9707 - val_loss: 0.1365 - val_accuracy: 0.9624
Epoch 9/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0952 - accuracy: 0.9725 - val_loss: 0.1336 - val_accuracy: 0.9634
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0893 - accuracy: 0.9750 - val_loss: 0.1376 - val_accuracy: 0.9634
Epoch 11/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0831 - accuracy: 0.9761 - val_loss: 0.1337 - val_accuracy: 0.9642
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1519 - accuracy: 0.9603
[CV] ..................................... n_neurons=39, total=  16.1s
[CV] n_neurons=40 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.3974 - accuracy: 0.8876 - val_loss: 0.2508 - val_accuracy: 0.9324
Epoch 2/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.2207 - accuracy: 0.9368 - val_loss: 0.1994 - val_accuracy: 0.9446
Epoch 3/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1741 - accuracy: 0.9497 - val_loss: 0.1669 - val_accuracy: 0.9548
Epoch 4/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1467 - accuracy: 0.9579 - val_loss: 0.1563 - val_accuracy: 0.9538
Epoch 5/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1290 - accuracy: 0.9627 - val_loss: 0.1393 - val_accuracy: 0.9610
Epoch 6/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1159 - accuracy: 0.9670 - val_loss: 0.1421 - val_accuracy: 0.9574
Epoch 7/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.1056 - accuracy: 0.9698 - val_loss: 0.1287 - val_accuracy: 0.9632
Epoch 8/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0970 - accuracy: 0.9724 - val_loss: 0.1337 - val_accuracy: 0.9624
Epoch 9/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.0894 - accuracy: 0.9744 - val_loss: 0.1267 - val_accuracy: 0.9630
Epoch 10/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0833 - accuracy: 0.9765 - val_loss: 0.1304 - val_accuracy: 0.9642
Epoch 11/30
36666/36666 [==============================] - 1s 37us/sample - loss: 0.0794 - accuracy: 0.9775 - val_loss: 0.1368 - val_accuracy: 0.9612
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1510 - accuracy: 0.9600
[CV] ..................................... n_neurons=40, total=  16.6s
[CV] n_neurons=40 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.3945 - accuracy: 0.8923 - val_loss: 0.2252 - val_accuracy: 0.9350
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2137 - accuracy: 0.9400 - val_loss: 0.1763 - val_accuracy: 0.9494
Epoch 3/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1717 - accuracy: 0.9510 - val_loss: 0.1564 - val_accuracy: 0.9546
Epoch 4/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1461 - accuracy: 0.9587 - val_loss: 0.1327 - val_accuracy: 0.9608
Epoch 5/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1280 - accuracy: 0.9644 - val_loss: 0.1404 - val_accuracy: 0.9576
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1140 - accuracy: 0.9681 - val_loss: 0.1253 - val_accuracy: 0.9640
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1042 - accuracy: 0.9703 - val_loss: 0.1228 - val_accuracy: 0.9632
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0952 - accuracy: 0.9734 - val_loss: 0.1236 - val_accuracy: 0.9644
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0887 - accuracy: 0.9753 - val_loss: 0.1224 - val_accuracy: 0.9658
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0831 - accuracy: 0.9773 - val_loss: 0.1240 - val_accuracy: 0.9642
Epoch 11/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0788 - accuracy: 0.9780 - val_loss: 0.1210 - val_accuracy: 0.9656
Epoch 12/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0731 - accuracy: 0.9792 - val_loss: 0.1326 - val_accuracy: 0.9640
Epoch 13/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0701 - accuracy: 0.9813 - val_loss: 0.1321 - val_accuracy: 0.9654
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1521 - accuracy: 0.9605
[CV] ..................................... n_neurons=40, total=  19.0s
[CV] n_neurons=40 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.4092 - accuracy: 0.8885 - val_loss: 0.2322 - val_accuracy: 0.9338
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2179 - accuracy: 0.9369 - val_loss: 0.1812 - val_accuracy: 0.9506
Epoch 3/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1708 - accuracy: 0.9499 - val_loss: 0.1526 - val_accuracy: 0.9572
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1432 - accuracy: 0.9584 - val_loss: 0.1454 - val_accuracy: 0.9590
Epoch 5/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1245 - accuracy: 0.9646 - val_loss: 0.1403 - val_accuracy: 0.9626
Epoch 6/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1114 - accuracy: 0.9683 - val_loss: 0.1414 - val_accuracy: 0.9614
Epoch 7/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.1010 - accuracy: 0.9710 - val_loss: 0.1350 - val_accuracy: 0.9606
Epoch 8/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0921 - accuracy: 0.9748 - val_loss: 0.1270 - val_accuracy: 0.9672
Epoch 9/30
36667/36667 [==============================] - 1s 37us/sample - loss: 0.0855 - accuracy: 0.9755 - val_loss: 0.1309 - val_accuracy: 0.9652
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0812 - accuracy: 0.9773 - val_loss: 0.1293 - val_accuracy: 0.9660
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1498 - accuracy: 0.9596
[CV] ..................................... n_neurons=40, total=  14.6s
[CV] n_neurons=41 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.3924 - accuracy: 0.8919 - val_loss: 0.2353 - val_accuracy: 0.9360
Epoch 2/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.2117 - accuracy: 0.9395 - val_loss: 0.1826 - val_accuracy: 0.9484
Epoch 3/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1677 - accuracy: 0.9509 - val_loss: 0.1574 - val_accuracy: 0.9564
Epoch 4/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1411 - accuracy: 0.9590 - val_loss: 0.1520 - val_accuracy: 0.9564
Epoch 5/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1236 - accuracy: 0.9645 - val_loss: 0.1332 - val_accuracy: 0.9620
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1107 - accuracy: 0.9683 - val_loss: 0.1348 - val_accuracy: 0.9618
Epoch 7/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1007 - accuracy: 0.9709 - val_loss: 0.1315 - val_accuracy: 0.9632
Epoch 8/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0914 - accuracy: 0.9736 - val_loss: 0.1332 - val_accuracy: 0.9626
Epoch 9/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0839 - accuracy: 0.9759 - val_loss: 0.1307 - val_accuracy: 0.9638
Epoch 10/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0775 - accuracy: 0.9776 - val_loss: 0.1285 - val_accuracy: 0.9634
Epoch 11/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0723 - accuracy: 0.9795 - val_loss: 0.1342 - val_accuracy: 0.9636
Epoch 12/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0675 - accuracy: 0.9809 - val_loss: 0.1270 - val_accuracy: 0.9670
Epoch 13/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0643 - accuracy: 0.9817 - val_loss: 0.1301 - val_accuracy: 0.9674
Epoch 14/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0603 - accuracy: 0.9830 - val_loss: 0.1358 - val_accuracy: 0.9658
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1411 - accuracy: 0.9641
[CV] ..................................... n_neurons=41, total=  20.8s
[CV] n_neurons=41 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.3803 - accuracy: 0.8958 - val_loss: 0.2146 - val_accuracy: 0.9362
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1974 - accuracy: 0.9444 - val_loss: 0.1689 - val_accuracy: 0.9534
Epoch 3/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1571 - accuracy: 0.9554 - val_loss: 0.1491 - val_accuracy: 0.9578
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1329 - accuracy: 0.9623 - val_loss: 0.1400 - val_accuracy: 0.9598
Epoch 5/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1171 - accuracy: 0.9660 - val_loss: 0.1441 - val_accuracy: 0.9592
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1045 - accuracy: 0.9697 - val_loss: 0.1308 - val_accuracy: 0.9626
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0950 - accuracy: 0.9723 - val_loss: 0.1254 - val_accuracy: 0.9634
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0871 - accuracy: 0.9752 - val_loss: 0.1206 - val_accuracy: 0.9668
Epoch 9/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0818 - accuracy: 0.9770 - val_loss: 0.1256 - val_accuracy: 0.9670
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0769 - accuracy: 0.9780 - val_loss: 0.1252 - val_accuracy: 0.9670
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1429 - accuracy: 0.96010s - loss: 0.1352 - accura
[CV] ..................................... n_neurons=41, total=  15.0s
[CV] n_neurons=41 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.3926 - accuracy: 0.8930 - val_loss: 0.2207 - val_accuracy: 0.9356
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.2108 - accuracy: 0.9387 - val_loss: 0.1794 - val_accuracy: 0.9506
Epoch 3/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1662 - accuracy: 0.9505 - val_loss: 0.1485 - val_accuracy: 0.9602
Epoch 4/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1408 - accuracy: 0.9594 - val_loss: 0.1402 - val_accuracy: 0.9610
Epoch 5/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1225 - accuracy: 0.9641 - val_loss: 0.1350 - val_accuracy: 0.9624
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1097 - accuracy: 0.9687 - val_loss: 0.1359 - val_accuracy: 0.9634
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0985 - accuracy: 0.9708 - val_loss: 0.1230 - val_accuracy: 0.9658
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0898 - accuracy: 0.9749 - val_loss: 0.1213 - val_accuracy: 0.9670
Epoch 9/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0831 - accuracy: 0.9768 - val_loss: 0.1197 - val_accuracy: 0.9678
Epoch 10/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0782 - accuracy: 0.9785 - val_loss: 0.1276 - val_accuracy: 0.9660
Epoch 11/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0723 - accuracy: 0.9800 - val_loss: 0.1252 - val_accuracy: 0.9652
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1344 - accuracy: 0.9645
[CV] ..................................... n_neurons=41, total=  16.4s
[CV] n_neurons=42 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.3814 - accuracy: 0.8936 - val_loss: 0.2389 - val_accuracy: 0.9352
Epoch 2/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.2125 - accuracy: 0.9390 - val_loss: 0.1809 - val_accuracy: 0.9488
Epoch 3/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1674 - accuracy: 0.9515 - val_loss: 0.1524 - val_accuracy: 0.9570
Epoch 4/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1416 - accuracy: 0.9585 - val_loss: 0.1446 - val_accuracy: 0.9574
Epoch 5/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1246 - accuracy: 0.9648 - val_loss: 0.1324 - val_accuracy: 0.9626
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1113 - accuracy: 0.9683 - val_loss: 0.1351 - val_accuracy: 0.9600
Epoch 7/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1006 - accuracy: 0.9714 - val_loss: 0.1248 - val_accuracy: 0.9660
Epoch 8/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.0931 - accuracy: 0.9740 - val_loss: 0.1297 - val_accuracy: 0.9634
Epoch 9/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0859 - accuracy: 0.9760 - val_loss: 0.1204 - val_accuracy: 0.9636
Epoch 10/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0802 - accuracy: 0.9771 - val_loss: 0.1211 - val_accuracy: 0.9674
Epoch 11/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0749 - accuracy: 0.9794 - val_loss: 0.1285 - val_accuracy: 0.9634
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1478 - accuracy: 0.9606
[CV] ..................................... n_neurons=42, total=  16.4s
[CV] n_neurons=42 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3797 - accuracy: 0.8960 - val_loss: 0.2177 - val_accuracy: 0.9386
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.2056 - accuracy: 0.9426 - val_loss: 0.1713 - val_accuracy: 0.9532
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1636 - accuracy: 0.9537 - val_loss: 0.1488 - val_accuracy: 0.9574
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1391 - accuracy: 0.9610 - val_loss: 0.1344 - val_accuracy: 0.9614
Epoch 5/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1217 - accuracy: 0.9647 - val_loss: 0.1483 - val_accuracy: 0.9590
Epoch 6/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1078 - accuracy: 0.9681 - val_loss: 0.1244 - val_accuracy: 0.9670
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0964 - accuracy: 0.9721 - val_loss: 0.1254 - val_accuracy: 0.9620
Epoch 8/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0881 - accuracy: 0.9749 - val_loss: 0.1243 - val_accuracy: 0.9634
Epoch 9/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0804 - accuracy: 0.9763 - val_loss: 0.1197 - val_accuracy: 0.9670
Epoch 10/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0749 - accuracy: 0.9780 - val_loss: 0.1227 - val_accuracy: 0.9674
Epoch 11/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0697 - accuracy: 0.9803 - val_loss: 0.1192 - val_accuracy: 0.9660
Epoch 12/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0641 - accuracy: 0.9815 - val_loss: 0.1217 - val_accuracy: 0.9684
Epoch 13/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0615 - accuracy: 0.9825 - val_loss: 0.1240 - val_accuracy: 0.9646
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1449 - accuracy: 0.9608
[CV] ..................................... n_neurons=42, total=  19.8s
[CV] n_neurons=42 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.3924 - accuracy: 0.8927 - val_loss: 0.2309 - val_accuracy: 0.9338
Epoch 2/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.2158 - accuracy: 0.9372 - val_loss: 0.1859 - val_accuracy: 0.9486
Epoch 3/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1725 - accuracy: 0.9491 - val_loss: 0.1583 - val_accuracy: 0.9558
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1459 - accuracy: 0.9569 - val_loss: 0.1476 - val_accuracy: 0.9580
Epoch 5/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1256 - accuracy: 0.9638 - val_loss: 0.1334 - val_accuracy: 0.9644
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1126 - accuracy: 0.9680 - val_loss: 0.1396 - val_accuracy: 0.9596
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1016 - accuracy: 0.9713 - val_loss: 0.1248 - val_accuracy: 0.9654
Epoch 8/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0923 - accuracy: 0.9741 - val_loss: 0.1219 - val_accuracy: 0.9648
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0856 - accuracy: 0.9766 - val_loss: 0.1202 - val_accuracy: 0.9662
Epoch 10/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0792 - accuracy: 0.9776 - val_loss: 0.1251 - val_accuracy: 0.9664
Epoch 11/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0747 - accuracy: 0.9791 - val_loss: 0.1188 - val_accuracy: 0.9706
Epoch 12/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0696 - accuracy: 0.9803 - val_loss: 0.1167 - val_accuracy: 0.9684
Epoch 13/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0651 - accuracy: 0.9821 - val_loss: 0.1215 - val_accuracy: 0.9678
Epoch 14/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0608 - accuracy: 0.9834 - val_loss: 0.1207 - val_accuracy: 0.9698
18333/18333 [==============================] - 0s 21us/sample - loss: 0.1496 - accuracy: 0.9628
[CV] ..................................... n_neurons=42, total=  20.8s
[CV] n_neurons=43 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.3929 - accuracy: 0.8907 - val_loss: 0.2302 - val_accuracy: 0.9382
Epoch 2/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.2165 - accuracy: 0.9371 - val_loss: 0.1895 - val_accuracy: 0.9462
Epoch 3/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1706 - accuracy: 0.9500 - val_loss: 0.1620 - val_accuracy: 0.9528
Epoch 4/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1427 - accuracy: 0.9586 - val_loss: 0.1474 - val_accuracy: 0.9574
Epoch 5/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1233 - accuracy: 0.9648 - val_loss: 0.1375 - val_accuracy: 0.9626
Epoch 6/30
36666/36666 [==============================] - 1s 38us/sample - loss: 0.1095 - accuracy: 0.9683 - val_loss: 0.1389 - val_accuracy: 0.9614
Epoch 7/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0991 - accuracy: 0.9710 - val_loss: 0.1272 - val_accuracy: 0.9650
Epoch 8/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.0906 - accuracy: 0.9745 - val_loss: 0.1299 - val_accuracy: 0.9614
Epoch 9/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0833 - accuracy: 0.9767 - val_loss: 0.1244 - val_accuracy: 0.9636
Epoch 10/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0777 - accuracy: 0.9775 - val_loss: 0.1267 - val_accuracy: 0.9664
Epoch 11/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0722 - accuracy: 0.9798 - val_loss: 0.1390 - val_accuracy: 0.9636
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1411 - accuracy: 0.9616
[CV] ..................................... n_neurons=43, total=  18.1s
[CV] n_neurons=43 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.3923 - accuracy: 0.8922 - val_loss: 0.2377 - val_accuracy: 0.9332
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.2156 - accuracy: 0.9401 - val_loss: 0.1851 - val_accuracy: 0.9492
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1664 - accuracy: 0.9531 - val_loss: 0.1574 - val_accuracy: 0.9574
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1374 - accuracy: 0.9620 - val_loss: 0.1414 - val_accuracy: 0.9606
Epoch 5/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1179 - accuracy: 0.9665 - val_loss: 0.1424 - val_accuracy: 0.9614
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1043 - accuracy: 0.9704 - val_loss: 0.1304 - val_accuracy: 0.9646
Epoch 7/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0942 - accuracy: 0.9731 - val_loss: 0.1296 - val_accuracy: 0.9634
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0860 - accuracy: 0.9756 - val_loss: 0.1267 - val_accuracy: 0.9650
Epoch 9/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0797 - accuracy: 0.9773 - val_loss: 0.1244 - val_accuracy: 0.9668
Epoch 10/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0741 - accuracy: 0.9793 - val_loss: 0.1342 - val_accuracy: 0.9646
Epoch 11/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0691 - accuracy: 0.9806 - val_loss: 0.1314 - val_accuracy: 0.9672
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1407 - accuracy: 0.9638
[CV] ..................................... n_neurons=43, total=  17.2s
[CV] n_neurons=43 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.3881 - accuracy: 0.8920 - val_loss: 0.2232 - val_accuracy: 0.9360
Epoch 2/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.2103 - accuracy: 0.9392 - val_loss: 0.1796 - val_accuracy: 0.9516
Epoch 3/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1646 - accuracy: 0.9509 - val_loss: 0.1476 - val_accuracy: 0.9588
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1369 - accuracy: 0.9601 - val_loss: 0.1391 - val_accuracy: 0.9614
Epoch 5/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1181 - accuracy: 0.9656 - val_loss: 0.1347 - val_accuracy: 0.9636
Epoch 6/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.1034 - accuracy: 0.9698 - val_loss: 0.1305 - val_accuracy: 0.9644
Epoch 7/30
36667/36667 [==============================] - 1s 38us/sample - loss: 0.0925 - accuracy: 0.9725 - val_loss: 0.1210 - val_accuracy: 0.9654
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0849 - accuracy: 0.9750 - val_loss: 0.1219 - val_accuracy: 0.9662
Epoch 9/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0787 - accuracy: 0.9773 - val_loss: 0.1253 - val_accuracy: 0.9686
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1406 - accuracy: 0.9599
[CV] ..................................... n_neurons=43, total=  13.6s
[CV] n_neurons=44 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.3904 - accuracy: 0.8911 - val_loss: 0.2513 - val_accuracy: 0.9324
Epoch 2/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.2219 - accuracy: 0.9373 - val_loss: 0.1981 - val_accuracy: 0.9442
Epoch 3/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1748 - accuracy: 0.9498 - val_loss: 0.1613 - val_accuracy: 0.9528
Epoch 4/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1462 - accuracy: 0.9577 - val_loss: 0.1483 - val_accuracy: 0.9582
Epoch 5/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1271 - accuracy: 0.9634 - val_loss: 0.1356 - val_accuracy: 0.9628
Epoch 6/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1144 - accuracy: 0.9669 - val_loss: 0.1376 - val_accuracy: 0.9602
Epoch 7/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1025 - accuracy: 0.9701 - val_loss: 0.1324 - val_accuracy: 0.9630
Epoch 8/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0938 - accuracy: 0.9734 - val_loss: 0.1282 - val_accuracy: 0.9658
Epoch 9/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0853 - accuracy: 0.9758 - val_loss: 0.1249 - val_accuracy: 0.9648
Epoch 10/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0794 - accuracy: 0.9774 - val_loss: 0.1253 - val_accuracy: 0.9638
Epoch 11/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0733 - accuracy: 0.9797 - val_loss: 0.1289 - val_accuracy: 0.9632
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1519 - accuracy: 0.9602
[CV] ..................................... n_neurons=44, total=  16.9s
[CV] n_neurons=44 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.3846 - accuracy: 0.8928 - val_loss: 0.2341 - val_accuracy: 0.9314
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.2157 - accuracy: 0.9396 - val_loss: 0.1777 - val_accuracy: 0.9486
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1687 - accuracy: 0.9516 - val_loss: 0.1593 - val_accuracy: 0.9548
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1409 - accuracy: 0.9605 - val_loss: 0.1389 - val_accuracy: 0.9614
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1216 - accuracy: 0.9654 - val_loss: 0.1417 - val_accuracy: 0.9610
Epoch 6/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1064 - accuracy: 0.9689 - val_loss: 0.1300 - val_accuracy: 0.9638
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0955 - accuracy: 0.9724 - val_loss: 0.1209 - val_accuracy: 0.9662
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0863 - accuracy: 0.9758 - val_loss: 0.1207 - val_accuracy: 0.9664
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0796 - accuracy: 0.9773 - val_loss: 0.1146 - val_accuracy: 0.9688
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0732 - accuracy: 0.9798 - val_loss: 0.1180 - val_accuracy: 0.9666
Epoch 11/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0684 - accuracy: 0.9806 - val_loss: 0.1193 - val_accuracy: 0.9664
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1481 - accuracy: 0.9613
[CV] ..................................... n_neurons=44, total=  17.3s
[CV] n_neurons=44 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3898 - accuracy: 0.8928 - val_loss: 0.2288 - val_accuracy: 0.9318
Epoch 2/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.2139 - accuracy: 0.9389 - val_loss: 0.1878 - val_accuracy: 0.9474
Epoch 3/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1663 - accuracy: 0.9516 - val_loss: 0.1555 - val_accuracy: 0.9564
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1388 - accuracy: 0.9599 - val_loss: 0.1427 - val_accuracy: 0.9584
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1195 - accuracy: 0.9653 - val_loss: 0.1358 - val_accuracy: 0.9596
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1056 - accuracy: 0.9696 - val_loss: 0.1387 - val_accuracy: 0.9644
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0956 - accuracy: 0.9723 - val_loss: 0.1260 - val_accuracy: 0.9636
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0863 - accuracy: 0.9757 - val_loss: 0.1210 - val_accuracy: 0.9672
Epoch 9/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0802 - accuracy: 0.9778 - val_loss: 0.1174 - val_accuracy: 0.9676
Epoch 10/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0741 - accuracy: 0.9792 - val_loss: 0.1218 - val_accuracy: 0.9686
Epoch 11/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0686 - accuracy: 0.9809 - val_loss: 0.1261 - val_accuracy: 0.9664
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1397 - accuracy: 0.9645
[CV] ..................................... n_neurons=44, total=  17.7s
[CV] n_neurons=45 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.3849 - accuracy: 0.8939 - val_loss: 0.2405 - val_accuracy: 0.9362
Epoch 2/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.2180 - accuracy: 0.9391 - val_loss: 0.1919 - val_accuracy: 0.9460
Epoch 3/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1742 - accuracy: 0.9508 - val_loss: 0.1660 - val_accuracy: 0.9534
Epoch 4/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1471 - accuracy: 0.9576 - val_loss: 0.1575 - val_accuracy: 0.9552
Epoch 5/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1315 - accuracy: 0.9626 - val_loss: 0.1505 - val_accuracy: 0.9576
Epoch 6/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1188 - accuracy: 0.9660 - val_loss: 0.1461 - val_accuracy: 0.9594
Epoch 7/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1081 - accuracy: 0.9690 - val_loss: 0.1477 - val_accuracy: 0.9606
Epoch 8/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1002 - accuracy: 0.9716 - val_loss: 0.1529 - val_accuracy: 0.9592
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1635 - accuracy: 0.9547
[CV] ..................................... n_neurons=45, total=  13.0s
[CV] n_neurons=45 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.3859 - accuracy: 0.8934 - val_loss: 0.2286 - val_accuracy: 0.9342
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.2133 - accuracy: 0.9388 - val_loss: 0.1818 - val_accuracy: 0.9502
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1704 - accuracy: 0.9505 - val_loss: 0.1584 - val_accuracy: 0.9526
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1436 - accuracy: 0.9590 - val_loss: 0.1430 - val_accuracy: 0.9568
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1246 - accuracy: 0.9644 - val_loss: 0.1484 - val_accuracy: 0.9572
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1103 - accuracy: 0.9686 - val_loss: 0.1368 - val_accuracy: 0.9614
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0983 - accuracy: 0.9710 - val_loss: 0.1295 - val_accuracy: 0.9602
Epoch 8/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0881 - accuracy: 0.9750 - val_loss: 0.1272 - val_accuracy: 0.9638
Epoch 9/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0817 - accuracy: 0.9767 - val_loss: 0.1274 - val_accuracy: 0.9634
Epoch 10/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0752 - accuracy: 0.9784 - val_loss: 0.1309 - val_accuracy: 0.9666
18333/18333 [==============================] - 1s 27us/sample - loss: 0.1480 - accuracy: 0.9603
[CV] ..................................... n_neurons=45, total=  15.9s
[CV] n_neurons=45 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.3833 - accuracy: 0.8946 - val_loss: 0.2167 - val_accuracy: 0.9388
Epoch 2/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1998 - accuracy: 0.9417 - val_loss: 0.1728 - val_accuracy: 0.9506
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1558 - accuracy: 0.9538 - val_loss: 0.1400 - val_accuracy: 0.9612
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1304 - accuracy: 0.9617 - val_loss: 0.1402 - val_accuracy: 0.9602
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1130 - accuracy: 0.9673 - val_loss: 0.1286 - val_accuracy: 0.9640
Epoch 6/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1010 - accuracy: 0.9707 - val_loss: 0.1291 - val_accuracy: 0.9632
Epoch 7/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0897 - accuracy: 0.9747 - val_loss: 0.1225 - val_accuracy: 0.9646
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0808 - accuracy: 0.9768 - val_loss: 0.1168 - val_accuracy: 0.9652
Epoch 9/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0743 - accuracy: 0.9788 - val_loss: 0.1133 - val_accuracy: 0.9678
Epoch 10/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0689 - accuracy: 0.9813 - val_loss: 0.1280 - val_accuracy: 0.9646
Epoch 11/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0636 - accuracy: 0.9829 - val_loss: 0.1202 - val_accuracy: 0.9686
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1365 - accuracy: 0.9645
[CV] ..................................... n_neurons=45, total=  18.1s
[CV] n_neurons=46 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.3812 - accuracy: 0.8937 - val_loss: 0.2345 - val_accuracy: 0.9350
Epoch 2/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.2106 - accuracy: 0.9400 - val_loss: 0.1874 - val_accuracy: 0.9486
Epoch 3/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1663 - accuracy: 0.9517 - val_loss: 0.1572 - val_accuracy: 0.9552
Epoch 4/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1395 - accuracy: 0.9595 - val_loss: 0.1409 - val_accuracy: 0.9606
Epoch 5/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.1216 - accuracy: 0.9661 - val_loss: 0.1361 - val_accuracy: 0.9612
Epoch 6/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1080 - accuracy: 0.9684 - val_loss: 0.1297 - val_accuracy: 0.9618
Epoch 7/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0960 - accuracy: 0.9725 - val_loss: 0.1319 - val_accuracy: 0.9642
Epoch 8/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0875 - accuracy: 0.9749 - val_loss: 0.1309 - val_accuracy: 0.9624
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1546 - accuracy: 0.9563
[CV] ..................................... n_neurons=46, total=  12.9s
[CV] n_neurons=46 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.3768 - accuracy: 0.8947 - val_loss: 0.2299 - val_accuracy: 0.9316
Epoch 2/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.2121 - accuracy: 0.9393 - val_loss: 0.1748 - val_accuracy: 0.9492
Epoch 3/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1668 - accuracy: 0.9519 - val_loss: 0.1495 - val_accuracy: 0.9558
Epoch 4/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1402 - accuracy: 0.9600 - val_loss: 0.1350 - val_accuracy: 0.9600
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1219 - accuracy: 0.9647 - val_loss: 0.1393 - val_accuracy: 0.9578
Epoch 6/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1076 - accuracy: 0.9686 - val_loss: 0.1207 - val_accuracy: 0.9652
Epoch 7/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0970 - accuracy: 0.9716 - val_loss: 0.1211 - val_accuracy: 0.9634
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0886 - accuracy: 0.9744 - val_loss: 0.1220 - val_accuracy: 0.9630
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1511 - accuracy: 0.9587
[CV] ..................................... n_neurons=46, total=  13.0s
[CV] n_neurons=46 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.3745 - accuracy: 0.8980 - val_loss: 0.2135 - val_accuracy: 0.9378
Epoch 2/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1979 - accuracy: 0.9434 - val_loss: 0.1704 - val_accuracy: 0.9514
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1546 - accuracy: 0.9553 - val_loss: 0.1433 - val_accuracy: 0.9614
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1296 - accuracy: 0.9624 - val_loss: 0.1382 - val_accuracy: 0.9596
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1125 - accuracy: 0.9678 - val_loss: 0.1287 - val_accuracy: 0.9632
Epoch 6/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1005 - accuracy: 0.9713 - val_loss: 0.1382 - val_accuracy: 0.9612
Epoch 7/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0904 - accuracy: 0.9744 - val_loss: 0.1197 - val_accuracy: 0.9646
Epoch 8/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0821 - accuracy: 0.9767 - val_loss: 0.1185 - val_accuracy: 0.9664
Epoch 9/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0766 - accuracy: 0.9789 - val_loss: 0.1241 - val_accuracy: 0.9642
Epoch 10/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0708 - accuracy: 0.9809 - val_loss: 0.1357 - val_accuracy: 0.9618
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1422 - accuracy: 0.9626
[CV] ..................................... n_neurons=46, total=  17.5s
[CV] n_neurons=47 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.3773 - accuracy: 0.8952 - val_loss: 0.2430 - val_accuracy: 0.9354
Epoch 2/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.2051 - accuracy: 0.9407 - val_loss: 0.1783 - val_accuracy: 0.9500
Epoch 3/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.1579 - accuracy: 0.9546 - val_loss: 0.1507 - val_accuracy: 0.9554
Epoch 4/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1304 - accuracy: 0.9623 - val_loss: 0.1397 - val_accuracy: 0.9606
Epoch 5/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1139 - accuracy: 0.9685 - val_loss: 0.1295 - val_accuracy: 0.9644
Epoch 6/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1013 - accuracy: 0.9712 - val_loss: 0.1258 - val_accuracy: 0.9642
Epoch 7/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0918 - accuracy: 0.9735 - val_loss: 0.1260 - val_accuracy: 0.9634
Epoch 8/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0840 - accuracy: 0.9768 - val_loss: 0.1294 - val_accuracy: 0.9642
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1453 - accuracy: 0.9593
[CV] ..................................... n_neurons=47, total=  13.9s
[CV] n_neurons=47 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.3661 - accuracy: 0.8986 - val_loss: 0.2247 - val_accuracy: 0.9342
Epoch 2/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.2065 - accuracy: 0.9411 - val_loss: 0.1703 - val_accuracy: 0.9524
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1615 - accuracy: 0.9538 - val_loss: 0.1479 - val_accuracy: 0.9572
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1352 - accuracy: 0.9615 - val_loss: 0.1311 - val_accuracy: 0.9628
Epoch 5/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1173 - accuracy: 0.9662 - val_loss: 0.1398 - val_accuracy: 0.9610
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1028 - accuracy: 0.9703 - val_loss: 0.1252 - val_accuracy: 0.9662
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0919 - accuracy: 0.9735 - val_loss: 0.1257 - val_accuracy: 0.9636
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0841 - accuracy: 0.9756 - val_loss: 0.1185 - val_accuracy: 0.9676
Epoch 9/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0767 - accuracy: 0.9782 - val_loss: 0.1208 - val_accuracy: 0.9664
Epoch 10/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0712 - accuracy: 0.9800 - val_loss: 0.1198 - val_accuracy: 0.9700
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1376 - accuracy: 0.9623
[CV] ..................................... n_neurons=47, total=  16.9s
[CV] n_neurons=47 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.3915 - accuracy: 0.8901 - val_loss: 0.2366 - val_accuracy: 0.9318
Epoch 2/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.2230 - accuracy: 0.9344 - val_loss: 0.1931 - val_accuracy: 0.9476
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1742 - accuracy: 0.9480 - val_loss: 0.1619 - val_accuracy: 0.9562
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1447 - accuracy: 0.9576 - val_loss: 0.1544 - val_accuracy: 0.9538
Epoch 5/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1247 - accuracy: 0.9636 - val_loss: 0.1456 - val_accuracy: 0.9600
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1104 - accuracy: 0.9682 - val_loss: 0.1480 - val_accuracy: 0.9588
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0990 - accuracy: 0.9717 - val_loss: 0.1385 - val_accuracy: 0.9612
Epoch 8/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0890 - accuracy: 0.9753 - val_loss: 0.1352 - val_accuracy: 0.9626
Epoch 9/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0829 - accuracy: 0.9765 - val_loss: 0.1400 - val_accuracy: 0.9614
Epoch 10/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0761 - accuracy: 0.9784 - val_loss: 0.1411 - val_accuracy: 0.9626
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1438 - accuracy: 0.9594
[CV] ..................................... n_neurons=47, total=  17.1s
[CV] n_neurons=48 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 57us/sample - loss: 0.3790 - accuracy: 0.8948 - val_loss: 0.2213 - val_accuracy: 0.9418
Epoch 2/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.2017 - accuracy: 0.9420 - val_loss: 0.1711 - val_accuracy: 0.9528
Epoch 3/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.1589 - accuracy: 0.9535 - val_loss: 0.1499 - val_accuracy: 0.9606
Epoch 4/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1334 - accuracy: 0.9610 - val_loss: 0.1406 - val_accuracy: 0.9610
Epoch 5/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.1158 - accuracy: 0.9671 - val_loss: 0.1376 - val_accuracy: 0.9646
Epoch 6/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1029 - accuracy: 0.9701 - val_loss: 0.1291 - val_accuracy: 0.9626
Epoch 7/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0916 - accuracy: 0.9734 - val_loss: 0.1258 - val_accuracy: 0.9648
Epoch 8/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0827 - accuracy: 0.9773 - val_loss: 0.1241 - val_accuracy: 0.9650
Epoch 9/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0759 - accuracy: 0.9785 - val_loss: 0.1227 - val_accuracy: 0.9670
Epoch 10/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0688 - accuracy: 0.9809 - val_loss: 0.1205 - val_accuracy: 0.9674
Epoch 11/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0636 - accuracy: 0.9819 - val_loss: 0.1290 - val_accuracy: 0.9644
Epoch 12/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0589 - accuracy: 0.9837 - val_loss: 0.1304 - val_accuracy: 0.9658
18334/18334 [==============================] - 0s 25us/sample - loss: 0.1465 - accuracy: 0.9607s - loss: 0.1484 - accuracy: 0.
[CV] ..................................... n_neurons=48, total=  21.3s
[CV] n_neurons=48 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.3796 - accuracy: 0.8932 - val_loss: 0.2255 - val_accuracy: 0.9346
Epoch 2/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.2035 - accuracy: 0.9422 - val_loss: 0.1691 - val_accuracy: 0.9520
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1544 - accuracy: 0.9556 - val_loss: 0.1493 - val_accuracy: 0.9560
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1283 - accuracy: 0.9635 - val_loss: 0.1339 - val_accuracy: 0.9598
Epoch 5/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1106 - accuracy: 0.9686 - val_loss: 0.1402 - val_accuracy: 0.9600
Epoch 6/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0972 - accuracy: 0.9724 - val_loss: 0.1284 - val_accuracy: 0.9612
Epoch 7/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0868 - accuracy: 0.9741 - val_loss: 0.1248 - val_accuracy: 0.9624
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0788 - accuracy: 0.9779 - val_loss: 0.1221 - val_accuracy: 0.9674
Epoch 9/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0716 - accuracy: 0.9794 - val_loss: 0.1195 - val_accuracy: 0.9656
Epoch 10/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0660 - accuracy: 0.9812 - val_loss: 0.1194 - val_accuracy: 0.9672
Epoch 11/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0604 - accuracy: 0.9829 - val_loss: 0.1185 - val_accuracy: 0.9676
Epoch 12/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0559 - accuracy: 0.9844 - val_loss: 0.1236 - val_accuracy: 0.9660
Epoch 13/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0523 - accuracy: 0.9851 - val_loss: 0.1190 - val_accuracy: 0.9678
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1377 - accuracy: 0.9639
[CV] ..................................... n_neurons=48, total=  21.7s
[CV] n_neurons=48 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.3873 - accuracy: 0.8929 - val_loss: 0.2268 - val_accuracy: 0.9354
Epoch 2/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.2068 - accuracy: 0.9393 - val_loss: 0.1705 - val_accuracy: 0.9510
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1605 - accuracy: 0.9526 - val_loss: 0.1426 - val_accuracy: 0.9604
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1325 - accuracy: 0.9610 - val_loss: 0.1335 - val_accuracy: 0.9614
Epoch 5/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1131 - accuracy: 0.9670 - val_loss: 0.1218 - val_accuracy: 0.9654
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0989 - accuracy: 0.9710 - val_loss: 0.1252 - val_accuracy: 0.9628
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0884 - accuracy: 0.9747 - val_loss: 0.1103 - val_accuracy: 0.9692
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0805 - accuracy: 0.9772 - val_loss: 0.1080 - val_accuracy: 0.9692
Epoch 9/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0738 - accuracy: 0.9799 - val_loss: 0.1103 - val_accuracy: 0.9676
Epoch 10/30
36667/36667 [==============================] - 2s 63us/sample - loss: 0.0689 - accuracy: 0.9809 - val_loss: 0.1138 - val_accuracy: 0.9676
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1371 - accuracy: 0.9618
[CV] ..................................... n_neurons=48, total=  17.4s
[CV] n_neurons=49 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.3828 - accuracy: 0.8934 - val_loss: 0.2295 - val_accuracy: 0.9362
Epoch 2/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.2075 - accuracy: 0.9396 - val_loss: 0.1780 - val_accuracy: 0.9502
Epoch 3/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1597 - accuracy: 0.9531 - val_loss: 0.1477 - val_accuracy: 0.9564
Epoch 4/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1309 - accuracy: 0.9614 - val_loss: 0.1356 - val_accuracy: 0.9582
Epoch 5/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1139 - accuracy: 0.9672 - val_loss: 0.1236 - val_accuracy: 0.9634
Epoch 6/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.1001 - accuracy: 0.9704 - val_loss: 0.1247 - val_accuracy: 0.9624
Epoch 7/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0904 - accuracy: 0.9740 - val_loss: 0.1195 - val_accuracy: 0.9662
Epoch 8/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0834 - accuracy: 0.9760 - val_loss: 0.1167 - val_accuracy: 0.9642
Epoch 9/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0751 - accuracy: 0.9790 - val_loss: 0.1125 - val_accuracy: 0.9696
Epoch 10/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0700 - accuracy: 0.9795 - val_loss: 0.1153 - val_accuracy: 0.9668
Epoch 11/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0654 - accuracy: 0.9813 - val_loss: 0.1196 - val_accuracy: 0.9650
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1408 - accuracy: 0.9627
[CV] ..................................... n_neurons=49, total=  19.2s
[CV] n_neurons=49 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.3846 - accuracy: 0.8924 - val_loss: 0.2380 - val_accuracy: 0.9300
Epoch 2/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.2137 - accuracy: 0.9396 - val_loss: 0.1804 - val_accuracy: 0.9492
Epoch 3/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1684 - accuracy: 0.9529 - val_loss: 0.1601 - val_accuracy: 0.9552
Epoch 4/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1405 - accuracy: 0.9600 - val_loss: 0.1398 - val_accuracy: 0.9604
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1217 - accuracy: 0.9647 - val_loss: 0.1410 - val_accuracy: 0.9614
Epoch 6/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1069 - accuracy: 0.9689 - val_loss: 0.1285 - val_accuracy: 0.9646
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0949 - accuracy: 0.9721 - val_loss: 0.1283 - val_accuracy: 0.9638
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0860 - accuracy: 0.9747 - val_loss: 0.1255 - val_accuracy: 0.9672
Epoch 9/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0791 - accuracy: 0.9770 - val_loss: 0.1223 - val_accuracy: 0.9682
Epoch 10/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0730 - accuracy: 0.9789 - val_loss: 0.1246 - val_accuracy: 0.9672
Epoch 11/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0671 - accuracy: 0.9805 - val_loss: 0.1244 - val_accuracy: 0.9672
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1391 - accuracy: 0.9632
[CV] ..................................... n_neurons=49, total=  17.8s
[CV] n_neurons=49 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3795 - accuracy: 0.8946 - val_loss: 0.2210 - val_accuracy: 0.9346
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.2027 - accuracy: 0.9404 - val_loss: 0.1744 - val_accuracy: 0.9498
Epoch 3/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1580 - accuracy: 0.9536 - val_loss: 0.1461 - val_accuracy: 0.9588
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1307 - accuracy: 0.9623 - val_loss: 0.1342 - val_accuracy: 0.9624
Epoch 5/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1123 - accuracy: 0.9680 - val_loss: 0.1279 - val_accuracy: 0.9636
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0987 - accuracy: 0.9720 - val_loss: 0.1278 - val_accuracy: 0.9652
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0873 - accuracy: 0.9751 - val_loss: 0.1192 - val_accuracy: 0.9632
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0790 - accuracy: 0.9766 - val_loss: 0.1132 - val_accuracy: 0.9686
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0722 - accuracy: 0.9795 - val_loss: 0.1104 - val_accuracy: 0.9684
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0660 - accuracy: 0.9821 - val_loss: 0.1183 - val_accuracy: 0.9652
Epoch 11/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0608 - accuracy: 0.9831 - val_loss: 0.1145 - val_accuracy: 0.9672
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1359 - accuracy: 0.9642
[CV] ..................................... n_neurons=49, total=  17.2s
[CV] n_neurons=50 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 63us/sample - loss: 0.3768 - accuracy: 0.8934 - val_loss: 0.2219 - val_accuracy: 0.9390
Epoch 2/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1939 - accuracy: 0.9431 - val_loss: 0.1679 - val_accuracy: 0.9502
Epoch 3/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1489 - accuracy: 0.9555 - val_loss: 0.1441 - val_accuracy: 0.9592
Epoch 4/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1228 - accuracy: 0.9632 - val_loss: 0.1350 - val_accuracy: 0.9602
Epoch 5/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1063 - accuracy: 0.9690 - val_loss: 0.1272 - val_accuracy: 0.9662
Epoch 6/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0944 - accuracy: 0.9722 - val_loss: 0.1229 - val_accuracy: 0.9668
Epoch 7/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0844 - accuracy: 0.9751 - val_loss: 0.1288 - val_accuracy: 0.9670
Epoch 8/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0767 - accuracy: 0.9781 - val_loss: 0.1240 - val_accuracy: 0.9656
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1431 - accuracy: 0.9608
[CV] ..................................... n_neurons=50, total=  13.4s
[CV] n_neurons=50 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3656 - accuracy: 0.8992 - val_loss: 0.2215 - val_accuracy: 0.9356
Epoch 2/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1988 - accuracy: 0.9439 - val_loss: 0.1718 - val_accuracy: 0.9510
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1587 - accuracy: 0.9546 - val_loss: 0.1485 - val_accuracy: 0.9592
Epoch 4/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1332 - accuracy: 0.9628 - val_loss: 0.1314 - val_accuracy: 0.9612
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1159 - accuracy: 0.9674 - val_loss: 0.1372 - val_accuracy: 0.9624
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1016 - accuracy: 0.9715 - val_loss: 0.1256 - val_accuracy: 0.9672
Epoch 7/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0919 - accuracy: 0.9735 - val_loss: 0.1234 - val_accuracy: 0.9644
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0835 - accuracy: 0.9770 - val_loss: 0.1206 - val_accuracy: 0.9686
Epoch 9/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0771 - accuracy: 0.9791 - val_loss: 0.1229 - val_accuracy: 0.9678
Epoch 10/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0712 - accuracy: 0.9803 - val_loss: 0.1244 - val_accuracy: 0.9682
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1438 - accuracy: 0.9612
[CV] ..................................... n_neurons=50, total=  16.3s
[CV] n_neurons=50 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.3648 - accuracy: 0.8980 - val_loss: 0.2130 - val_accuracy: 0.9398
Epoch 2/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1959 - accuracy: 0.9419 - val_loss: 0.1586 - val_accuracy: 0.9544
Epoch 3/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1520 - accuracy: 0.9547 - val_loss: 0.1395 - val_accuracy: 0.9588
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1269 - accuracy: 0.9625 - val_loss: 0.1337 - val_accuracy: 0.9614
Epoch 5/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1092 - accuracy: 0.9680 - val_loss: 0.1224 - val_accuracy: 0.9640
Epoch 6/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0959 - accuracy: 0.9717 - val_loss: 0.1209 - val_accuracy: 0.9650
Epoch 7/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0846 - accuracy: 0.9760 - val_loss: 0.1084 - val_accuracy: 0.9684
Epoch 8/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0774 - accuracy: 0.9780 - val_loss: 0.1044 - val_accuracy: 0.9706
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0702 - accuracy: 0.9798 - val_loss: 0.1138 - val_accuracy: 0.9700
Epoch 10/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0656 - accuracy: 0.9818 - val_loss: 0.1173 - val_accuracy: 0.9690
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1314 - accuracy: 0.9633
[CV] ..................................... n_neurons=50, total=  16.0s
[CV] n_neurons=51 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.3721 - accuracy: 0.8955 - val_loss: 0.2337 - val_accuracy: 0.9372
Epoch 2/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.2086 - accuracy: 0.9398 - val_loss: 0.1844 - val_accuracy: 0.9474
Epoch 3/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1612 - accuracy: 0.9543 - val_loss: 0.1563 - val_accuracy: 0.9534
Epoch 4/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1340 - accuracy: 0.9606 - val_loss: 0.1449 - val_accuracy: 0.9584
Epoch 5/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1163 - accuracy: 0.9672 - val_loss: 0.1332 - val_accuracy: 0.9622
Epoch 6/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1023 - accuracy: 0.9709 - val_loss: 0.1418 - val_accuracy: 0.9614
Epoch 7/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0915 - accuracy: 0.9738 - val_loss: 0.1308 - val_accuracy: 0.9634
Epoch 8/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0833 - accuracy: 0.9760 - val_loss: 0.1289 - val_accuracy: 0.9632
Epoch 9/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0747 - accuracy: 0.9791 - val_loss: 0.1268 - val_accuracy: 0.9632
Epoch 10/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0694 - accuracy: 0.9799 - val_loss: 0.1209 - val_accuracy: 0.9646
Epoch 11/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0636 - accuracy: 0.9819 - val_loss: 0.1217 - val_accuracy: 0.9652
Epoch 12/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0581 - accuracy: 0.9830 - val_loss: 0.1227 - val_accuracy: 0.9644
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1470 - accuracy: 0.9631
[CV] ..................................... n_neurons=51, total=  19.0s
[CV] n_neurons=51 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3712 - accuracy: 0.8948 - val_loss: 0.2267 - val_accuracy: 0.9342
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.2077 - accuracy: 0.9409 - val_loss: 0.1718 - val_accuracy: 0.9520
Epoch 3/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1606 - accuracy: 0.9542 - val_loss: 0.1537 - val_accuracy: 0.9558
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1333 - accuracy: 0.9613 - val_loss: 0.1402 - val_accuracy: 0.9602
Epoch 5/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1138 - accuracy: 0.9672 - val_loss: 0.1386 - val_accuracy: 0.9610
Epoch 6/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0993 - accuracy: 0.9710 - val_loss: 0.1302 - val_accuracy: 0.9630
Epoch 7/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0886 - accuracy: 0.9736 - val_loss: 0.1243 - val_accuracy: 0.9626
Epoch 8/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0800 - accuracy: 0.9773 - val_loss: 0.1243 - val_accuracy: 0.9646
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0736 - accuracy: 0.9780 - val_loss: 0.1222 - val_accuracy: 0.9658
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0682 - accuracy: 0.9809 - val_loss: 0.1285 - val_accuracy: 0.9666
Epoch 11/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0629 - accuracy: 0.9826 - val_loss: 0.1275 - val_accuracy: 0.9686
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1356 - accuracy: 0.9645
[CV] ..................................... n_neurons=51, total=  17.3s
[CV] n_neurons=51 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.3755 - accuracy: 0.8944 - val_loss: 0.2113 - val_accuracy: 0.9384
Epoch 2/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1940 - accuracy: 0.9431 - val_loss: 0.1661 - val_accuracy: 0.9526
Epoch 3/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1495 - accuracy: 0.9554 - val_loss: 0.1363 - val_accuracy: 0.9620
Epoch 4/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1247 - accuracy: 0.9638 - val_loss: 0.1294 - val_accuracy: 0.9614
Epoch 5/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1070 - accuracy: 0.9689 - val_loss: 0.1183 - val_accuracy: 0.9672
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0950 - accuracy: 0.9732 - val_loss: 0.1187 - val_accuracy: 0.9642
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0850 - accuracy: 0.9754 - val_loss: 0.1128 - val_accuracy: 0.9656
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0767 - accuracy: 0.9781 - val_loss: 0.1117 - val_accuracy: 0.9682
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0705 - accuracy: 0.9800 - val_loss: 0.1139 - val_accuracy: 0.9682
Epoch 10/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0660 - accuracy: 0.9812 - val_loss: 0.1140 - val_accuracy: 0.9692
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1374 - accuracy: 0.9633
[CV] ..................................... n_neurons=51, total=  16.2s
[CV] n_neurons=52 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.3689 - accuracy: 0.8961 - val_loss: 0.2345 - val_accuracy: 0.9358
Epoch 2/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1993 - accuracy: 0.9424 - val_loss: 0.1743 - val_accuracy: 0.9502
Epoch 3/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1523 - accuracy: 0.9552 - val_loss: 0.1484 - val_accuracy: 0.9554
Epoch 4/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1252 - accuracy: 0.9629 - val_loss: 0.1353 - val_accuracy: 0.9614
Epoch 5/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1081 - accuracy: 0.9686 - val_loss: 0.1279 - val_accuracy: 0.9652
Epoch 6/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0950 - accuracy: 0.9720 - val_loss: 0.1274 - val_accuracy: 0.9662
Epoch 7/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0847 - accuracy: 0.9755 - val_loss: 0.1345 - val_accuracy: 0.9622
Epoch 8/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0772 - accuracy: 0.9774 - val_loss: 0.1354 - val_accuracy: 0.9618
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1430 - accuracy: 0.9623
[CV] ..................................... n_neurons=52, total=  12.9s
[CV] n_neurons=52 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.3617 - accuracy: 0.9012 - val_loss: 0.2157 - val_accuracy: 0.9346
Epoch 2/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1933 - accuracy: 0.9450 - val_loss: 0.1641 - val_accuracy: 0.9536
Epoch 3/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1487 - accuracy: 0.9571 - val_loss: 0.1415 - val_accuracy: 0.9578
Epoch 4/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1241 - accuracy: 0.9637 - val_loss: 0.1273 - val_accuracy: 0.9624
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1066 - accuracy: 0.9691 - val_loss: 0.1330 - val_accuracy: 0.9622
Epoch 6/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0925 - accuracy: 0.9735 - val_loss: 0.1171 - val_accuracy: 0.9666
Epoch 7/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0830 - accuracy: 0.9760 - val_loss: 0.1141 - val_accuracy: 0.9662
Epoch 8/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0744 - accuracy: 0.9792 - val_loss: 0.1105 - val_accuracy: 0.9684
Epoch 9/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0679 - accuracy: 0.9808 - val_loss: 0.1122 - val_accuracy: 0.9696
Epoch 10/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0629 - accuracy: 0.9824 - val_loss: 0.1185 - val_accuracy: 0.9692
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1357 - accuracy: 0.9648
[CV] ..................................... n_neurons=52, total=  16.7s
[CV] n_neurons=52 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.3700 - accuracy: 0.8972 - val_loss: 0.2230 - val_accuracy: 0.9370
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1993 - accuracy: 0.9416 - val_loss: 0.1768 - val_accuracy: 0.9478
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1512 - accuracy: 0.9559 - val_loss: 0.1415 - val_accuracy: 0.9626
Epoch 4/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1246 - accuracy: 0.9637 - val_loss: 0.1376 - val_accuracy: 0.9610
Epoch 5/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1056 - accuracy: 0.9697 - val_loss: 0.1233 - val_accuracy: 0.9658
Epoch 6/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0922 - accuracy: 0.9739 - val_loss: 0.1230 - val_accuracy: 0.9652
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0812 - accuracy: 0.9770 - val_loss: 0.1137 - val_accuracy: 0.9664
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0729 - accuracy: 0.9799 - val_loss: 0.1081 - val_accuracy: 0.9696
Epoch 9/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0670 - accuracy: 0.9814 - val_loss: 0.1141 - val_accuracy: 0.9704
Epoch 10/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0606 - accuracy: 0.9826 - val_loss: 0.1176 - val_accuracy: 0.9688
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1389 - accuracy: 0.9638
[CV] ..................................... n_neurons=52, total=  16.4s
[CV] n_neurons=53 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.3736 - accuracy: 0.8951 - val_loss: 0.2314 - val_accuracy: 0.9356
Epoch 2/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.2071 - accuracy: 0.9409 - val_loss: 0.1823 - val_accuracy: 0.9488
Epoch 3/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1617 - accuracy: 0.9535 - val_loss: 0.1507 - val_accuracy: 0.9562
Epoch 4/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.1352 - accuracy: 0.9604 - val_loss: 0.1435 - val_accuracy: 0.9586
Epoch 5/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1184 - accuracy: 0.9658 - val_loss: 0.1334 - val_accuracy: 0.9650
Epoch 6/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.1057 - accuracy: 0.9696 - val_loss: 0.1326 - val_accuracy: 0.9600
Epoch 7/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0954 - accuracy: 0.9729 - val_loss: 0.1246 - val_accuracy: 0.9664
Epoch 8/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0875 - accuracy: 0.9754 - val_loss: 0.1297 - val_accuracy: 0.9654
Epoch 9/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0792 - accuracy: 0.9781 - val_loss: 0.1266 - val_accuracy: 0.9652
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1472 - accuracy: 0.9613
[CV] ..................................... n_neurons=53, total=  16.0s
[CV] n_neurons=53 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3703 - accuracy: 0.8976 - val_loss: 0.2237 - val_accuracy: 0.9342
Epoch 2/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.2051 - accuracy: 0.9416 - val_loss: 0.1702 - val_accuracy: 0.9498
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1582 - accuracy: 0.9550 - val_loss: 0.1482 - val_accuracy: 0.9556
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1301 - accuracy: 0.9623 - val_loss: 0.1364 - val_accuracy: 0.9600
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1119 - accuracy: 0.9673 - val_loss: 0.1364 - val_accuracy: 0.9600
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0972 - accuracy: 0.9717 - val_loss: 0.1235 - val_accuracy: 0.9652
Epoch 7/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0868 - accuracy: 0.9740 - val_loss: 0.1196 - val_accuracy: 0.9672
Epoch 8/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0774 - accuracy: 0.9773 - val_loss: 0.1173 - val_accuracy: 0.9662
Epoch 9/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0709 - accuracy: 0.9795 - val_loss: 0.1159 - val_accuracy: 0.9666
Epoch 10/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0651 - accuracy: 0.9812 - val_loss: 0.1199 - val_accuracy: 0.9670
Epoch 11/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0594 - accuracy: 0.9832 - val_loss: 0.1221 - val_accuracy: 0.9662
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1333 - accuracy: 0.9655
[CV] ..................................... n_neurons=53, total=  19.0s
[CV] n_neurons=53 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 68us/sample - loss: 0.3766 - accuracy: 0.8964 - val_loss: 0.2109 - val_accuracy: 0.9392
Epoch 2/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1961 - accuracy: 0.9431 - val_loss: 0.1607 - val_accuracy: 0.9520
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1497 - accuracy: 0.9558 - val_loss: 0.1341 - val_accuracy: 0.9610
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1243 - accuracy: 0.9638 - val_loss: 0.1247 - val_accuracy: 0.9630
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1050 - accuracy: 0.9700 - val_loss: 0.1149 - val_accuracy: 0.9666
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0918 - accuracy: 0.9746 - val_loss: 0.1167 - val_accuracy: 0.9646
Epoch 7/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0807 - accuracy: 0.9774 - val_loss: 0.1034 - val_accuracy: 0.9688
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0730 - accuracy: 0.9798 - val_loss: 0.1028 - val_accuracy: 0.9702
Epoch 9/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0656 - accuracy: 0.9823 - val_loss: 0.1058 - val_accuracy: 0.9698
Epoch 10/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.0612 - accuracy: 0.9833 - val_loss: 0.1084 - val_accuracy: 0.9712
18333/18333 [==============================] - 1s 28us/sample - loss: 0.1370 - accuracy: 0.9638
[CV] ..................................... n_neurons=53, total=  18.3s
[CV] n_neurons=54 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 64us/sample - loss: 0.3760 - accuracy: 0.8939 - val_loss: 0.2272 - val_accuracy: 0.9368
Epoch 2/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.2031 - accuracy: 0.9414 - val_loss: 0.1713 - val_accuracy: 0.9518
Epoch 3/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1544 - accuracy: 0.9556 - val_loss: 0.1460 - val_accuracy: 0.9592
Epoch 4/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1266 - accuracy: 0.9629 - val_loss: 0.1374 - val_accuracy: 0.9592
Epoch 5/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1088 - accuracy: 0.9687 - val_loss: 0.1238 - val_accuracy: 0.9638
Epoch 6/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0962 - accuracy: 0.9723 - val_loss: 0.1276 - val_accuracy: 0.9642
Epoch 7/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0848 - accuracy: 0.9761 - val_loss: 0.1247 - val_accuracy: 0.9642
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1451 - accuracy: 0.9588
[CV] ..................................... n_neurons=54, total=  12.3s
[CV] n_neurons=54 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.3664 - accuracy: 0.8990 - val_loss: 0.2113 - val_accuracy: 0.9378
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1895 - accuracy: 0.9454 - val_loss: 0.1581 - val_accuracy: 0.9546
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1457 - accuracy: 0.9579 - val_loss: 0.1404 - val_accuracy: 0.9590
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1193 - accuracy: 0.9657 - val_loss: 0.1221 - val_accuracy: 0.9638
Epoch 5/30
36667/36667 [==============================] - 291s 8ms/sample - loss: 0.1024 - accuracy: 0.9698 - val_loss: 0.1328 - val_accuracy: 0.9610
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0901 - accuracy: 0.9733 - val_loss: 0.1149 - val_accuracy: 0.9668
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0798 - accuracy: 0.9757 - val_loss: 0.1128 - val_accuracy: 0.9658
Epoch 8/30
36667/36667 [==============================] - 3s 94us/sample - loss: 0.0710 - accuracy: 0.9795 - val_loss: 0.1115 - val_accuracy: 0.9690
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0657 - accuracy: 0.9809 - val_loss: 0.1106 - val_accuracy: 0.9692
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0598 - accuracy: 0.9819 - val_loss: 0.1118 - val_accuracy: 0.9690
Epoch 11/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0553 - accuracy: 0.9840 - val_loss: 0.1161 - val_accuracy: 0.9670
18333/18333 [==============================] - 1s 36us/sample - loss: 0.1355 - accuracy: 0.9650
[CV] ..................................... n_neurons=54, total= 5.2min
[CV] n_neurons=54 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3718 - accuracy: 0.8962 - val_loss: 0.2061 - val_accuracy: 0.9406
Epoch 2/30
36667/36667 [==============================] - 3s 73us/sample - loss: 0.1903 - accuracy: 0.9437 - val_loss: 0.1528 - val_accuracy: 0.9564
Epoch 3/30
36667/36667 [==============================] - 3s 69us/sample - loss: 0.1416 - accuracy: 0.9574 - val_loss: 0.1311 - val_accuracy: 0.9644
Epoch 4/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.1158 - accuracy: 0.9658 - val_loss: 0.1236 - val_accuracy: 0.9640
Epoch 5/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.0976 - accuracy: 0.9720 - val_loss: 0.1176 - val_accuracy: 0.9684
Epoch 6/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.0857 - accuracy: 0.9759 - val_loss: 0.1112 - val_accuracy: 0.9694
Epoch 7/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0753 - accuracy: 0.9789 - val_loss: 0.1057 - val_accuracy: 0.9696
Epoch 8/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0675 - accuracy: 0.9809 - val_loss: 0.1087 - val_accuracy: 0.9718
Epoch 9/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0616 - accuracy: 0.9831 - val_loss: 0.1066 - val_accuracy: 0.9708
18333/18333 [==============================] - 1s 29us/sample - loss: 0.1304 - accuracy: 0.9645
[CV] ..................................... n_neurons=54, total=  20.6s
[CV] n_neurons=55 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 58us/sample - loss: 0.3693 - accuracy: 0.8959 - val_loss: 0.2227 - val_accuracy: 0.9370
Epoch 2/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.2006 - accuracy: 0.9419 - val_loss: 0.1688 - val_accuracy: 0.9500
Epoch 3/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1553 - accuracy: 0.9546 - val_loss: 0.1424 - val_accuracy: 0.9574
Epoch 4/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.1276 - accuracy: 0.9618 - val_loss: 0.1452 - val_accuracy: 0.9556
Epoch 5/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1106 - accuracy: 0.9681 - val_loss: 0.1273 - val_accuracy: 0.9624
Epoch 6/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0969 - accuracy: 0.9726 - val_loss: 0.1233 - val_accuracy: 0.9642
Epoch 7/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0860 - accuracy: 0.9757 - val_loss: 0.1223 - val_accuracy: 0.9654
Epoch 8/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0776 - accuracy: 0.9782 - val_loss: 0.1201 - val_accuracy: 0.9650
Epoch 9/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0708 - accuracy: 0.9797 - val_loss: 0.1165 - val_accuracy: 0.9654
Epoch 10/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0641 - accuracy: 0.9823 - val_loss: 0.1183 - val_accuracy: 0.9670
Epoch 11/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0590 - accuracy: 0.9836 - val_loss: 0.1280 - val_accuracy: 0.9670
18334/18334 [==============================] - 1s 28us/sample - loss: 0.1385 - accuracy: 0.9625
[CV] ..................................... n_neurons=55, total=  21.5s
[CV] n_neurons=55 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.3698 - accuracy: 0.8971 - val_loss: 0.2189 - val_accuracy: 0.9364
Epoch 2/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.2038 - accuracy: 0.9423 - val_loss: 0.1767 - val_accuracy: 0.9516
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1578 - accuracy: 0.9553 - val_loss: 0.1522 - val_accuracy: 0.9566
Epoch 4/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1301 - accuracy: 0.9632 - val_loss: 0.1380 - val_accuracy: 0.9624
Epoch 5/30
36667/36667 [==============================] - 141s 4ms/sample - loss: 0.1121 - accuracy: 0.9684 - val_loss: 0.1397 - val_accuracy: 0.9598
Epoch 6/30
36667/36667 [==============================] - 2s 64us/sample - loss: 0.0968 - accuracy: 0.9724 - val_loss: 0.1341 - val_accuracy: 0.9638
Epoch 7/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.0863 - accuracy: 0.9751 - val_loss: 0.1313 - val_accuracy: 0.9632
Epoch 8/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0779 - accuracy: 0.9782 - val_loss: 0.1228 - val_accuracy: 0.9668
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0714 - accuracy: 0.9797 - val_loss: 0.1224 - val_accuracy: 0.9662
Epoch 10/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0644 - accuracy: 0.9815 - val_loss: 0.1271 - val_accuracy: 0.9672
Epoch 11/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0593 - accuracy: 0.9823 - val_loss: 0.1241 - val_accuracy: 0.9694
18333/18333 [==============================] - 2s 118us/sample - loss: 0.1356 - accuracy: 0.9629
[CV] ..................................... n_neurons=55, total= 2.7min
[CV] n_neurons=55 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 4s 107us/sample - loss: 0.3733 - accuracy: 0.8966 - val_loss: 0.2157 - val_accuracy: 0.9366
Epoch 2/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.2013 - accuracy: 0.9416 - val_loss: 0.1682 - val_accuracy: 0.9508
Epoch 3/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1564 - accuracy: 0.9541 - val_loss: 0.1357 - val_accuracy: 0.9616
Epoch 4/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1307 - accuracy: 0.9621 - val_loss: 0.1297 - val_accuracy: 0.9638
Epoch 5/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.1122 - accuracy: 0.9675 - val_loss: 0.1270 - val_accuracy: 0.9646
Epoch 6/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0995 - accuracy: 0.9711 - val_loss: 0.1231 - val_accuracy: 0.9652
Epoch 7/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0891 - accuracy: 0.9745 - val_loss: 0.1158 - val_accuracy: 0.9656
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0806 - accuracy: 0.9771 - val_loss: 0.1154 - val_accuracy: 0.9688
Epoch 9/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.0739 - accuracy: 0.9790 - val_loss: 0.1143 - val_accuracy: 0.9706
Epoch 10/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0688 - accuracy: 0.9805 - val_loss: 0.1291 - val_accuracy: 0.9656
Epoch 11/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0619 - accuracy: 0.9828 - val_loss: 0.1226 - val_accuracy: 0.9674
18333/18333 [==============================] - 1s 28us/sample - loss: 0.1383 - accuracy: 0.9645
[CV] ..................................... n_neurons=55, total=  24.7s
[CV] n_neurons=56 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.3668 - accuracy: 0.8966 - val_loss: 0.2230 - val_accuracy: 0.9386
Epoch 2/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.1993 - accuracy: 0.9425 - val_loss: 0.1756 - val_accuracy: 0.9496
Epoch 3/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1571 - accuracy: 0.9548 - val_loss: 0.1537 - val_accuracy: 0.9558
Epoch 4/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.1304 - accuracy: 0.9627 - val_loss: 0.1448 - val_accuracy: 0.9576
Epoch 5/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1134 - accuracy: 0.9675 - val_loss: 0.1318 - val_accuracy: 0.9626
Epoch 6/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0991 - accuracy: 0.9718 - val_loss: 0.1325 - val_accuracy: 0.9606
Epoch 7/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0874 - accuracy: 0.9748 - val_loss: 0.1262 - val_accuracy: 0.9648
Epoch 8/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0793 - accuracy: 0.9779 - val_loss: 0.1305 - val_accuracy: 0.9624
Epoch 9/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.0714 - accuracy: 0.9799 - val_loss: 0.1222 - val_accuracy: 0.9658
Epoch 10/30
36666/36666 [==============================] - 2s 63us/sample - loss: 0.0654 - accuracy: 0.9816 - val_loss: 0.1147 - val_accuracy: 0.9660
Epoch 11/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0596 - accuracy: 0.9836 - val_loss: 0.1276 - val_accuracy: 0.9666
Epoch 12/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0539 - accuracy: 0.9843 - val_loss: 0.1224 - val_accuracy: 0.9674
18334/18334 [==============================] - 0s 27us/sample - loss: 0.1363 - accuracy: 0.9643
[CV] ..................................... n_neurons=56, total=  22.3s
[CV] n_neurons=56 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 64us/sample - loss: 0.3616 - accuracy: 0.8993 - val_loss: 0.2193 - val_accuracy: 0.9378
Epoch 2/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.1933 - accuracy: 0.9452 - val_loss: 0.1623 - val_accuracy: 0.9536
Epoch 3/30
36667/36667 [==============================] - 2s 68us/sample - loss: 0.1476 - accuracy: 0.9579 - val_loss: 0.1473 - val_accuracy: 0.9574
Epoch 4/30
36667/36667 [==============================] - 3s 95us/sample - loss: 0.1226 - accuracy: 0.9650 - val_loss: 0.1269 - val_accuracy: 0.9632
Epoch 5/30
36667/36667 [==============================] - 3s 89us/sample - loss: 0.1054 - accuracy: 0.9695 - val_loss: 0.1319 - val_accuracy: 0.9606
Epoch 6/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0910 - accuracy: 0.9738 - val_loss: 0.1197 - val_accuracy: 0.9652
Epoch 7/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0811 - accuracy: 0.9757 - val_loss: 0.1183 - val_accuracy: 0.9660
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0730 - accuracy: 0.9793 - val_loss: 0.1169 - val_accuracy: 0.9672
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0664 - accuracy: 0.9811 - val_loss: 0.1168 - val_accuracy: 0.9674
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0605 - accuracy: 0.9822 - val_loss: 0.1169 - val_accuracy: 0.9686
Epoch 11/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0545 - accuracy: 0.9849 - val_loss: 0.1221 - val_accuracy: 0.9662
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1399 - accuracy: 0.9633
[CV] ..................................... n_neurons=56, total=  24.9s
[CV] n_neurons=56 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.3642 - accuracy: 0.8993 - val_loss: 0.2114 - val_accuracy: 0.9370
Epoch 2/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.1941 - accuracy: 0.9438 - val_loss: 0.1633 - val_accuracy: 0.9548
Epoch 3/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1498 - accuracy: 0.9557 - val_loss: 0.1353 - val_accuracy: 0.9646
Epoch 4/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1243 - accuracy: 0.9630 - val_loss: 0.1293 - val_accuracy: 0.9638
Epoch 5/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.1067 - accuracy: 0.9701 - val_loss: 0.1261 - val_accuracy: 0.9646
Epoch 6/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0940 - accuracy: 0.9729 - val_loss: 0.1256 - val_accuracy: 0.9654
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0828 - accuracy: 0.9765 - val_loss: 0.1162 - val_accuracy: 0.9654
Epoch 8/30
36667/36667 [==============================] - 2s 65us/sample - loss: 0.0753 - accuracy: 0.9780 - val_loss: 0.1131 - val_accuracy: 0.9678
Epoch 9/30
36667/36667 [==============================] - 3s 85us/sample - loss: 0.0695 - accuracy: 0.9802 - val_loss: 0.1155 - val_accuracy: 0.9688
Epoch 10/30
36667/36667 [==============================] - 3s 77us/sample - loss: 0.0634 - accuracy: 0.9819 - val_loss: 0.1213 - val_accuracy: 0.9680
18333/18333 [==============================] - 1s 30us/sample - loss: 0.1404 - accuracy: 0.9633
[CV] ..................................... n_neurons=56, total=  22.2s
[CV] n_neurons=57 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 3s 69us/sample - loss: 0.3717 - accuracy: 0.8959 - val_loss: 0.2228 - val_accuracy: 0.9414
Epoch 2/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1903 - accuracy: 0.9449 - val_loss: 0.1627 - val_accuracy: 0.9538
Epoch 3/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.1414 - accuracy: 0.9590 - val_loss: 0.1356 - val_accuracy: 0.9600
Epoch 4/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.1138 - accuracy: 0.9666 - val_loss: 0.1311 - val_accuracy: 0.9608
Epoch 5/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.0964 - accuracy: 0.9719 - val_loss: 0.1180 - val_accuracy: 0.9666
Epoch 6/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0845 - accuracy: 0.9756 - val_loss: 0.1227 - val_accuracy: 0.9662
Epoch 7/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0750 - accuracy: 0.9782 - val_loss: 0.1178 - val_accuracy: 0.9650
Epoch 8/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0682 - accuracy: 0.9806 - val_loss: 0.1177 - val_accuracy: 0.9690
Epoch 9/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0617 - accuracy: 0.9824 - val_loss: 0.1115 - val_accuracy: 0.9710
Epoch 10/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0559 - accuracy: 0.9838 - val_loss: 0.1151 - val_accuracy: 0.9702
Epoch 11/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0512 - accuracy: 0.9852 - val_loss: 0.1199 - val_accuracy: 0.9690
18334/18334 [==============================] - 0s 25us/sample - loss: 0.1235 - accuracy: 0.9677
[CV] ..................................... n_neurons=57, total=  20.7s
[CV] n_neurons=57 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.3581 - accuracy: 0.9013 - val_loss: 0.2063 - val_accuracy: 0.9408
Epoch 2/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1865 - accuracy: 0.9472 - val_loss: 0.1554 - val_accuracy: 0.9560
Epoch 3/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1415 - accuracy: 0.9591 - val_loss: 0.1301 - val_accuracy: 0.9632
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1163 - accuracy: 0.9662 - val_loss: 0.1204 - val_accuracy: 0.9656
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0983 - accuracy: 0.9713 - val_loss: 0.1286 - val_accuracy: 0.9636
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0857 - accuracy: 0.9756 - val_loss: 0.1128 - val_accuracy: 0.9682
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0752 - accuracy: 0.9781 - val_loss: 0.1063 - val_accuracy: 0.9696
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0674 - accuracy: 0.9800 - val_loss: 0.1082 - val_accuracy: 0.9682
Epoch 9/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0609 - accuracy: 0.9822 - val_loss: 0.1091 - val_accuracy: 0.9694
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1322 - accuracy: 0.9635
[CV] ..................................... n_neurons=57, total=  16.5s
[CV] n_neurons=57 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.3643 - accuracy: 0.8994 - val_loss: 0.2026 - val_accuracy: 0.9404
Epoch 2/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1859 - accuracy: 0.9447 - val_loss: 0.1599 - val_accuracy: 0.9534
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1410 - accuracy: 0.9584 - val_loss: 0.1298 - val_accuracy: 0.9624
Epoch 4/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1167 - accuracy: 0.9662 - val_loss: 0.1289 - val_accuracy: 0.9628
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0989 - accuracy: 0.9712 - val_loss: 0.1169 - val_accuracy: 0.9652
Epoch 6/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0858 - accuracy: 0.9755 - val_loss: 0.1201 - val_accuracy: 0.9664
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0757 - accuracy: 0.9779 - val_loss: 0.1113 - val_accuracy: 0.9684
Epoch 8/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0676 - accuracy: 0.9808 - val_loss: 0.1063 - val_accuracy: 0.9680
Epoch 9/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0613 - accuracy: 0.9824 - val_loss: 0.1164 - val_accuracy: 0.9682
Epoch 10/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0555 - accuracy: 0.9844 - val_loss: 0.1233 - val_accuracy: 0.9688
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1329 - accuracy: 0.9643
[CV] ..................................... n_neurons=57, total=  17.0s
[CV] n_neurons=58 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.3661 - accuracy: 0.8962 - val_loss: 0.2246 - val_accuracy: 0.9414
Epoch 2/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1914 - accuracy: 0.9452 - val_loss: 0.1676 - val_accuracy: 0.9522
Epoch 3/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1461 - accuracy: 0.9577 - val_loss: 0.1407 - val_accuracy: 0.9616
Epoch 4/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1206 - accuracy: 0.9661 - val_loss: 0.1354 - val_accuracy: 0.9626
Epoch 5/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1041 - accuracy: 0.9700 - val_loss: 0.1248 - val_accuracy: 0.9670
Epoch 6/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0919 - accuracy: 0.9733 - val_loss: 0.1254 - val_accuracy: 0.9648
Epoch 7/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.0825 - accuracy: 0.9765 - val_loss: 0.1203 - val_accuracy: 0.9674
Epoch 8/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.0753 - accuracy: 0.9785 - val_loss: 0.1189 - val_accuracy: 0.9676
Epoch 9/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0681 - accuracy: 0.9808 - val_loss: 0.1154 - val_accuracy: 0.9662
Epoch 10/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0627 - accuracy: 0.9829 - val_loss: 0.1120 - val_accuracy: 0.9704
Epoch 11/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0574 - accuracy: 0.9837 - val_loss: 0.1187 - val_accuracy: 0.9682
Epoch 12/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0527 - accuracy: 0.9852 - val_loss: 0.1162 - val_accuracy: 0.9680
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1544 - accuracy: 0.9623
[CV] ..................................... n_neurons=58, total=  21.1s
[CV] n_neurons=58 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.3564 - accuracy: 0.9004 - val_loss: 0.2095 - val_accuracy: 0.9350
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1886 - accuracy: 0.9460 - val_loss: 0.1569 - val_accuracy: 0.9528
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1459 - accuracy: 0.9581 - val_loss: 0.1470 - val_accuracy: 0.9548
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1206 - accuracy: 0.9657 - val_loss: 0.1294 - val_accuracy: 0.9622
Epoch 5/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.1052 - accuracy: 0.9700 - val_loss: 0.1304 - val_accuracy: 0.9612
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0901 - accuracy: 0.9736 - val_loss: 0.1153 - val_accuracy: 0.9658
Epoch 7/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0799 - accuracy: 0.9766 - val_loss: 0.1166 - val_accuracy: 0.9656
Epoch 8/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0705 - accuracy: 0.9800 - val_loss: 0.1175 - val_accuracy: 0.9694
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1378 - accuracy: 0.9617
[CV] ..................................... n_neurons=58, total=  14.5s
[CV] n_neurons=58 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.3687 - accuracy: 0.8965 - val_loss: 0.2189 - val_accuracy: 0.9386
Epoch 2/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1986 - accuracy: 0.9423 - val_loss: 0.1631 - val_accuracy: 0.9526
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1520 - accuracy: 0.9552 - val_loss: 0.1370 - val_accuracy: 0.9624
Epoch 4/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1263 - accuracy: 0.9637 - val_loss: 0.1273 - val_accuracy: 0.9628
Epoch 5/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1082 - accuracy: 0.9696 - val_loss: 0.1285 - val_accuracy: 0.9654
Epoch 6/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0962 - accuracy: 0.9726 - val_loss: 0.1271 - val_accuracy: 0.9630
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0862 - accuracy: 0.9755 - val_loss: 0.1122 - val_accuracy: 0.9682
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0773 - accuracy: 0.9786 - val_loss: 0.1148 - val_accuracy: 0.9678
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0703 - accuracy: 0.9809 - val_loss: 0.1145 - val_accuracy: 0.9682
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1334 - accuracy: 0.9626
[CV] ..................................... n_neurons=58, total=  14.9s
[CV] n_neurons=59 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.3662 - accuracy: 0.8969 - val_loss: 0.2245 - val_accuracy: 0.9392
Epoch 2/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1938 - accuracy: 0.9435 - val_loss: 0.1626 - val_accuracy: 0.9506
Epoch 3/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1474 - accuracy: 0.9573 - val_loss: 0.1391 - val_accuracy: 0.9582
Epoch 4/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1201 - accuracy: 0.9642 - val_loss: 0.1312 - val_accuracy: 0.9608
Epoch 5/30
36666/36666 [==============================] - 4s 105us/sample - loss: 0.1046 - accuracy: 0.9703 - val_loss: 0.1194 - val_accuracy: 0.9654
Epoch 6/30
36666/36666 [==============================] - 3s 84us/sample - loss: 0.0908 - accuracy: 0.9740 - val_loss: 0.1308 - val_accuracy: 0.9620
Epoch 7/30
36666/36666 [==============================] - 2s 62us/sample - loss: 0.0809 - accuracy: 0.9774 - val_loss: 0.1171 - val_accuracy: 0.9666
Epoch 8/30
36666/36666 [==============================] - 3s 78us/sample - loss: 0.0728 - accuracy: 0.9787 - val_loss: 0.1225 - val_accuracy: 0.9654
Epoch 9/30
36666/36666 [==============================] - 3s 85us/sample - loss: 0.0661 - accuracy: 0.9823 - val_loss: 0.1112 - val_accuracy: 0.9692
Epoch 10/30
36666/36666 [==============================] - 3s 84us/sample - loss: 0.0602 - accuracy: 0.9827 - val_loss: 0.1139 - val_accuracy: 0.9694
Epoch 11/30
36666/36666 [==============================] - 3s 74us/sample - loss: 0.0552 - accuracy: 0.9842 - val_loss: 0.1227 - val_accuracy: 0.9678
18334/18334 [==============================] - 1s 30us/sample - loss: 0.1334 - accuracy: 0.9638
[CV] ..................................... n_neurons=59, total=  28.5s
[CV] n_neurons=59 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 66us/sample - loss: 0.3640 - accuracy: 0.9004 - val_loss: 0.2069 - val_accuracy: 0.9386
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1927 - accuracy: 0.9452 - val_loss: 0.1606 - val_accuracy: 0.9532
Epoch 3/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1484 - accuracy: 0.9578 - val_loss: 0.1381 - val_accuracy: 0.9626
Epoch 4/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.1239 - accuracy: 0.9646 - val_loss: 0.1264 - val_accuracy: 0.9648
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1058 - accuracy: 0.9693 - val_loss: 0.1361 - val_accuracy: 0.9606
Epoch 6/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.0937 - accuracy: 0.9731 - val_loss: 0.1180 - val_accuracy: 0.9684
Epoch 7/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0826 - accuracy: 0.9755 - val_loss: 0.1165 - val_accuracy: 0.9664
Epoch 8/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0745 - accuracy: 0.9788 - val_loss: 0.1097 - val_accuracy: 0.9700
Epoch 9/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0667 - accuracy: 0.9809 - val_loss: 0.1133 - val_accuracy: 0.9696
Epoch 10/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0615 - accuracy: 0.9831 - val_loss: 0.1190 - val_accuracy: 0.9686
18333/18333 [==============================] - 1s 33us/sample - loss: 0.1307 - accuracy: 0.9662
[CV] ..................................... n_neurons=59, total=  19.4s
[CV] n_neurons=59 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.3628 - accuracy: 0.8998 - val_loss: 0.2053 - val_accuracy: 0.9412
Epoch 2/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1938 - accuracy: 0.9434 - val_loss: 0.1635 - val_accuracy: 0.9540
Epoch 3/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.1503 - accuracy: 0.9555 - val_loss: 0.1415 - val_accuracy: 0.9612
Epoch 4/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.1241 - accuracy: 0.9649 - val_loss: 0.1321 - val_accuracy: 0.9608
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1061 - accuracy: 0.9701 - val_loss: 0.1269 - val_accuracy: 0.9648
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0928 - accuracy: 0.9738 - val_loss: 0.1211 - val_accuracy: 0.9670
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0822 - accuracy: 0.9776 - val_loss: 0.1110 - val_accuracy: 0.9686
Epoch 8/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0727 - accuracy: 0.9800 - val_loss: 0.1157 - val_accuracy: 0.9688
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0667 - accuracy: 0.9814 - val_loss: 0.1231 - val_accuracy: 0.9664
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1321 - accuracy: 0.9633
[CV] ..................................... n_neurons=59, total=  16.5s
[CV] n_neurons=60 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.3578 - accuracy: 0.9006 - val_loss: 0.2255 - val_accuracy: 0.9388
Epoch 2/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.1925 - accuracy: 0.9444 - val_loss: 0.1662 - val_accuracy: 0.9524
Epoch 3/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1479 - accuracy: 0.9572 - val_loss: 0.1439 - val_accuracy: 0.9578
Epoch 4/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1219 - accuracy: 0.9637 - val_loss: 0.1338 - val_accuracy: 0.9592
Epoch 5/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1041 - accuracy: 0.9700 - val_loss: 0.1240 - val_accuracy: 0.9632
Epoch 6/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0910 - accuracy: 0.9738 - val_loss: 0.1226 - val_accuracy: 0.9624
Epoch 7/30
36666/36666 [==============================] - 2s 57us/sample - loss: 0.0802 - accuracy: 0.9768 - val_loss: 0.1199 - val_accuracy: 0.9650
Epoch 8/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0721 - accuracy: 0.9795 - val_loss: 0.1155 - val_accuracy: 0.9646
Epoch 9/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0648 - accuracy: 0.9815 - val_loss: 0.1153 - val_accuracy: 0.9684
Epoch 10/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0590 - accuracy: 0.9827 - val_loss: 0.1122 - val_accuracy: 0.9678
Epoch 11/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0533 - accuracy: 0.9845 - val_loss: 0.1210 - val_accuracy: 0.9658
Epoch 12/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0484 - accuracy: 0.9862 - val_loss: 0.1135 - val_accuracy: 0.9698
18334/18334 [==============================] - 1s 34us/sample - loss: 0.1390 - accuracy: 0.9634
[CV] ..................................... n_neurons=60, total=  20.2s
[CV] n_neurons=60 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.3565 - accuracy: 0.9019 - val_loss: 0.2087 - val_accuracy: 0.9374
Epoch 2/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.1907 - accuracy: 0.9454 - val_loss: 0.1649 - val_accuracy: 0.9550
Epoch 3/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1466 - accuracy: 0.9581 - val_loss: 0.1452 - val_accuracy: 0.9576
Epoch 4/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1218 - accuracy: 0.9653 - val_loss: 0.1278 - val_accuracy: 0.9640
Epoch 5/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1055 - accuracy: 0.9697 - val_loss: 0.1318 - val_accuracy: 0.9630
Epoch 6/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.0921 - accuracy: 0.9742 - val_loss: 0.1214 - val_accuracy: 0.9640
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0811 - accuracy: 0.9767 - val_loss: 0.1143 - val_accuracy: 0.9668
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0722 - accuracy: 0.9798 - val_loss: 0.1187 - val_accuracy: 0.9668
Epoch 9/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0668 - accuracy: 0.9810 - val_loss: 0.1116 - val_accuracy: 0.9688
Epoch 10/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0607 - accuracy: 0.9829 - val_loss: 0.1195 - val_accuracy: 0.9680
Epoch 11/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0548 - accuracy: 0.9846 - val_loss: 0.1113 - val_accuracy: 0.9690
Epoch 12/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0499 - accuracy: 0.9863 - val_loss: 0.1165 - val_accuracy: 0.9684
Epoch 13/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0464 - accuracy: 0.9872 - val_loss: 0.1201 - val_accuracy: 0.9660
18333/18333 [==============================] - 1s 34us/sample - loss: 0.1356 - accuracy: 0.9657
[CV] ..................................... n_neurons=60, total=  23.8s
[CV] n_neurons=60 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.3628 - accuracy: 0.8975 - val_loss: 0.2086 - val_accuracy: 0.9388
Epoch 2/30
36667/36667 [==============================] - 2s 63us/sample - loss: 0.1906 - accuracy: 0.9448 - val_loss: 0.1605 - val_accuracy: 0.9538
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1458 - accuracy: 0.9557 - val_loss: 0.1347 - val_accuracy: 0.9602
Epoch 4/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.1202 - accuracy: 0.9639 - val_loss: 0.1270 - val_accuracy: 0.9632
Epoch 5/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1015 - accuracy: 0.9704 - val_loss: 0.1181 - val_accuracy: 0.9658
Epoch 6/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0877 - accuracy: 0.9744 - val_loss: 0.1196 - val_accuracy: 0.9654
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0770 - accuracy: 0.9776 - val_loss: 0.1052 - val_accuracy: 0.9678
Epoch 8/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0687 - accuracy: 0.9800 - val_loss: 0.1064 - val_accuracy: 0.9720
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0617 - accuracy: 0.9829 - val_loss: 0.1103 - val_accuracy: 0.9676
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1302 - accuracy: 0.9641
[CV] ..................................... n_neurons=60, total=  17.2s
[CV] n_neurons=61 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 3s 73us/sample - loss: 0.3614 - accuracy: 0.8987 - val_loss: 0.2181 - val_accuracy: 0.9412
Epoch 2/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1905 - accuracy: 0.9450 - val_loss: 0.1683 - val_accuracy: 0.9518
Epoch 3/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.1461 - accuracy: 0.9581 - val_loss: 0.1395 - val_accuracy: 0.9602
Epoch 4/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1190 - accuracy: 0.9657 - val_loss: 0.1343 - val_accuracy: 0.9604
Epoch 5/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.1032 - accuracy: 0.9705 - val_loss: 0.1259 - val_accuracy: 0.9646
Epoch 6/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0901 - accuracy: 0.9741 - val_loss: 0.1278 - val_accuracy: 0.9622
Epoch 7/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0800 - accuracy: 0.9773 - val_loss: 0.1231 - val_accuracy: 0.9658
Epoch 8/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0718 - accuracy: 0.9797 - val_loss: 0.1264 - val_accuracy: 0.9644
Epoch 9/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0645 - accuracy: 0.9820 - val_loss: 0.1205 - val_accuracy: 0.9670
Epoch 10/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0589 - accuracy: 0.9840 - val_loss: 0.1144 - val_accuracy: 0.9698
Epoch 11/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0537 - accuracy: 0.9846 - val_loss: 0.1139 - val_accuracy: 0.9700
Epoch 12/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0494 - accuracy: 0.9857 - val_loss: 0.1181 - val_accuracy: 0.9690
Epoch 13/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0455 - accuracy: 0.9870 - val_loss: 0.1209 - val_accuracy: 0.9686
18334/18334 [==============================] - 0s 25us/sample - loss: 0.1420 - accuracy: 0.9645
[CV] ..................................... n_neurons=61, total=  23.2s
[CV] n_neurons=61 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.3580 - accuracy: 0.8998 - val_loss: 0.2123 - val_accuracy: 0.9394
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1972 - accuracy: 0.9430 - val_loss: 0.1638 - val_accuracy: 0.9536
Epoch 3/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1521 - accuracy: 0.9568 - val_loss: 0.1423 - val_accuracy: 0.9570
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1249 - accuracy: 0.9640 - val_loss: 0.1243 - val_accuracy: 0.9624
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1067 - accuracy: 0.9694 - val_loss: 0.1241 - val_accuracy: 0.9628
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0919 - accuracy: 0.9730 - val_loss: 0.1138 - val_accuracy: 0.9688
Epoch 7/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0810 - accuracy: 0.9764 - val_loss: 0.1161 - val_accuracy: 0.9672
Epoch 8/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0719 - accuracy: 0.9797 - val_loss: 0.1114 - val_accuracy: 0.9660
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0652 - accuracy: 0.9819 - val_loss: 0.1151 - val_accuracy: 0.9684
Epoch 10/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0602 - accuracy: 0.9832 - val_loss: 0.1219 - val_accuracy: 0.9680
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1360 - accuracy: 0.9635
[CV] ..................................... n_neurons=61, total=  16.5s
[CV] n_neurons=61 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3601 - accuracy: 0.9008 - val_loss: 0.2025 - val_accuracy: 0.9436
Epoch 2/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1861 - accuracy: 0.9450 - val_loss: 0.1562 - val_accuracy: 0.9550
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1422 - accuracy: 0.9577 - val_loss: 0.1282 - val_accuracy: 0.9654
Epoch 4/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1171 - accuracy: 0.9658 - val_loss: 0.1196 - val_accuracy: 0.9636
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0994 - accuracy: 0.9716 - val_loss: 0.1117 - val_accuracy: 0.9678
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0869 - accuracy: 0.9753 - val_loss: 0.1221 - val_accuracy: 0.9670
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0762 - accuracy: 0.9786 - val_loss: 0.1079 - val_accuracy: 0.9680
Epoch 8/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0689 - accuracy: 0.9805 - val_loss: 0.1030 - val_accuracy: 0.9726
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0625 - accuracy: 0.9827 - val_loss: 0.1024 - val_accuracy: 0.9712
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0570 - accuracy: 0.9844 - val_loss: 0.1160 - val_accuracy: 0.9698
Epoch 11/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0514 - accuracy: 0.9860 - val_loss: 0.1120 - val_accuracy: 0.9716
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1252 - accuracy: 0.9676
[CV] ..................................... n_neurons=61, total=  17.5s
[CV] n_neurons=62 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.3576 - accuracy: 0.8994 - val_loss: 0.2196 - val_accuracy: 0.9390
Epoch 2/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1923 - accuracy: 0.9435 - val_loss: 0.1674 - val_accuracy: 0.9534
Epoch 3/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1445 - accuracy: 0.9587 - val_loss: 0.1361 - val_accuracy: 0.9626
Epoch 4/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.1158 - accuracy: 0.9659 - val_loss: 0.1292 - val_accuracy: 0.9636
Epoch 5/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0984 - accuracy: 0.9721 - val_loss: 0.1212 - val_accuracy: 0.9666
Epoch 6/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0862 - accuracy: 0.9755 - val_loss: 0.1174 - val_accuracy: 0.9664
Epoch 7/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0763 - accuracy: 0.9776 - val_loss: 0.1163 - val_accuracy: 0.9690
Epoch 8/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0680 - accuracy: 0.9805 - val_loss: 0.1121 - val_accuracy: 0.9686
Epoch 9/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0603 - accuracy: 0.9828 - val_loss: 0.1126 - val_accuracy: 0.9700
Epoch 10/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0550 - accuracy: 0.9841 - val_loss: 0.1054 - val_accuracy: 0.9708
Epoch 11/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0505 - accuracy: 0.9859 - val_loss: 0.1101 - val_accuracy: 0.9720
Epoch 12/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0457 - accuracy: 0.9872 - val_loss: 0.1100 - val_accuracy: 0.9712
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1315 - accuracy: 0.9670
[CV] ..................................... n_neurons=62, total=  19.2s
[CV] n_neurons=62 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.3563 - accuracy: 0.9013 - val_loss: 0.2070 - val_accuracy: 0.9408
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1892 - accuracy: 0.9457 - val_loss: 0.1541 - val_accuracy: 0.9564
Epoch 3/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1446 - accuracy: 0.9586 - val_loss: 0.1376 - val_accuracy: 0.9590
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1189 - accuracy: 0.9659 - val_loss: 0.1160 - val_accuracy: 0.9674
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1001 - accuracy: 0.9712 - val_loss: 0.1231 - val_accuracy: 0.9642
Epoch 6/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0859 - accuracy: 0.9751 - val_loss: 0.1131 - val_accuracy: 0.9690
Epoch 7/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0755 - accuracy: 0.9779 - val_loss: 0.1110 - val_accuracy: 0.9662
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0669 - accuracy: 0.9809 - val_loss: 0.1062 - val_accuracy: 0.9680
Epoch 9/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0601 - accuracy: 0.9828 - val_loss: 0.1077 - val_accuracy: 0.9674
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0535 - accuracy: 0.9845 - val_loss: 0.1145 - val_accuracy: 0.9688
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1297 - accuracy: 0.9667
[CV] ..................................... n_neurons=62, total=  16.7s
[CV] n_neurons=62 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3650 - accuracy: 0.8989 - val_loss: 0.2119 - val_accuracy: 0.9390
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1935 - accuracy: 0.9445 - val_loss: 0.1621 - val_accuracy: 0.9522
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1472 - accuracy: 0.9562 - val_loss: 0.1334 - val_accuracy: 0.9626
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1209 - accuracy: 0.9654 - val_loss: 0.1348 - val_accuracy: 0.9606
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1021 - accuracy: 0.9710 - val_loss: 0.1206 - val_accuracy: 0.9650
Epoch 6/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0894 - accuracy: 0.9740 - val_loss: 0.1275 - val_accuracy: 0.9652
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0787 - accuracy: 0.9775 - val_loss: 0.1151 - val_accuracy: 0.9658
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0706 - accuracy: 0.9797 - val_loss: 0.1150 - val_accuracy: 0.9662
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0653 - accuracy: 0.9817 - val_loss: 0.1152 - val_accuracy: 0.9672
Epoch 10/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0592 - accuracy: 0.9842 - val_loss: 0.1201 - val_accuracy: 0.9686
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1336 - accuracy: 0.9641
[CV] ..................................... n_neurons=62, total=  15.9s
[CV] n_neurons=63 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.3492 - accuracy: 0.9023 - val_loss: 0.2163 - val_accuracy: 0.9434
Epoch 2/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1859 - accuracy: 0.9461 - val_loss: 0.1534 - val_accuracy: 0.9554
Epoch 3/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1421 - accuracy: 0.9587 - val_loss: 0.1344 - val_accuracy: 0.9608
Epoch 4/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1161 - accuracy: 0.9669 - val_loss: 0.1312 - val_accuracy: 0.9628
Epoch 5/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1008 - accuracy: 0.9711 - val_loss: 0.1184 - val_accuracy: 0.9684
Epoch 6/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0871 - accuracy: 0.9744 - val_loss: 0.1209 - val_accuracy: 0.9636
Epoch 7/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0778 - accuracy: 0.9772 - val_loss: 0.1151 - val_accuracy: 0.9686
Epoch 8/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0697 - accuracy: 0.9805 - val_loss: 0.1147 - val_accuracy: 0.9678
Epoch 9/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0626 - accuracy: 0.9820 - val_loss: 0.1092 - val_accuracy: 0.9720
Epoch 10/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.0563 - accuracy: 0.9840 - val_loss: 0.1107 - val_accuracy: 0.9694
Epoch 11/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0516 - accuracy: 0.9852 - val_loss: 0.1160 - val_accuracy: 0.9702
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1365 - accuracy: 0.9642
[CV] ..................................... n_neurons=63, total=  18.5s
[CV] n_neurons=63 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3483 - accuracy: 0.9035 - val_loss: 0.2039 - val_accuracy: 0.9406
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1870 - accuracy: 0.9461 - val_loss: 0.1556 - val_accuracy: 0.9576
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1442 - accuracy: 0.9586 - val_loss: 0.1350 - val_accuracy: 0.9630
Epoch 4/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1184 - accuracy: 0.9654 - val_loss: 0.1236 - val_accuracy: 0.9648
Epoch 5/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1002 - accuracy: 0.9714 - val_loss: 0.1302 - val_accuracy: 0.9612
Epoch 6/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0867 - accuracy: 0.9746 - val_loss: 0.1168 - val_accuracy: 0.9664
Epoch 7/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0765 - accuracy: 0.9780 - val_loss: 0.1117 - val_accuracy: 0.9676
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0677 - accuracy: 0.9812 - val_loss: 0.1123 - val_accuracy: 0.9696
Epoch 9/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0614 - accuracy: 0.9829 - val_loss: 0.1095 - val_accuracy: 0.9708
Epoch 10/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0562 - accuracy: 0.9839 - val_loss: 0.1222 - val_accuracy: 0.9690
Epoch 11/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0505 - accuracy: 0.9866 - val_loss: 0.1146 - val_accuracy: 0.9694
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1319 - accuracy: 0.9664
[CV] ..................................... n_neurons=63, total=  17.3s
[CV] n_neurons=63 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.3523 - accuracy: 0.9005 - val_loss: 0.1979 - val_accuracy: 0.9442
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1855 - accuracy: 0.9456 - val_loss: 0.1573 - val_accuracy: 0.9538
Epoch 3/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1415 - accuracy: 0.9584 - val_loss: 0.1301 - val_accuracy: 0.9618
Epoch 4/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1162 - accuracy: 0.9663 - val_loss: 0.1238 - val_accuracy: 0.9638
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0987 - accuracy: 0.9720 - val_loss: 0.1161 - val_accuracy: 0.9674
Epoch 6/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0861 - accuracy: 0.9761 - val_loss: 0.1173 - val_accuracy: 0.9674
Epoch 7/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0762 - accuracy: 0.9787 - val_loss: 0.1118 - val_accuracy: 0.9664
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0673 - accuracy: 0.9807 - val_loss: 0.1132 - val_accuracy: 0.9688
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0612 - accuracy: 0.9829 - val_loss: 0.1177 - val_accuracy: 0.9674
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1386 - accuracy: 0.9626
[CV] ..................................... n_neurons=63, total=  14.5s
[CV] n_neurons=64 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.3568 - accuracy: 0.8995 - val_loss: 0.2188 - val_accuracy: 0.9420
Epoch 2/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1930 - accuracy: 0.9449 - val_loss: 0.1651 - val_accuracy: 0.9536
Epoch 3/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1451 - accuracy: 0.9577 - val_loss: 0.1336 - val_accuracy: 0.9628
Epoch 4/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1158 - accuracy: 0.9665 - val_loss: 0.1340 - val_accuracy: 0.9616
Epoch 5/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0990 - accuracy: 0.9725 - val_loss: 0.1203 - val_accuracy: 0.9662
Epoch 6/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0855 - accuracy: 0.9758 - val_loss: 0.1213 - val_accuracy: 0.9660
Epoch 7/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0755 - accuracy: 0.9788 - val_loss: 0.1180 - val_accuracy: 0.9690
Epoch 8/30
36666/36666 [==============================] - 1s 39us/sample - loss: 0.0671 - accuracy: 0.9811 - val_loss: 0.1195 - val_accuracy: 0.9672
Epoch 9/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.1152 - val_accuracy: 0.9692
Epoch 10/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0536 - accuracy: 0.9845 - val_loss: 0.1118 - val_accuracy: 0.9698
Epoch 11/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0496 - accuracy: 0.9859 - val_loss: 0.1248 - val_accuracy: 0.9688
Epoch 12/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0442 - accuracy: 0.9879 - val_loss: 0.1215 - val_accuracy: 0.9658
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1375 - accuracy: 0.9663
[CV] ..................................... n_neurons=64, total=  18.9s
[CV] n_neurons=64 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.3520 - accuracy: 0.9002 - val_loss: 0.2148 - val_accuracy: 0.9382
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1906 - accuracy: 0.9450 - val_loss: 0.1554 - val_accuracy: 0.9568
Epoch 3/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1432 - accuracy: 0.9583 - val_loss: 0.1366 - val_accuracy: 0.9584
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1168 - accuracy: 0.9666 - val_loss: 0.1200 - val_accuracy: 0.9646
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0997 - accuracy: 0.9714 - val_loss: 0.1374 - val_accuracy: 0.9590
Epoch 6/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0859 - accuracy: 0.9756 - val_loss: 0.1198 - val_accuracy: 0.9642
Epoch 7/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0766 - accuracy: 0.9776 - val_loss: 0.1104 - val_accuracy: 0.9688
Epoch 8/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0669 - accuracy: 0.9812 - val_loss: 0.1084 - val_accuracy: 0.9688
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0613 - accuracy: 0.9827 - val_loss: 0.1144 - val_accuracy: 0.9690
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0557 - accuracy: 0.9839 - val_loss: 0.1188 - val_accuracy: 0.9688
18333/18333 [==============================] - 1s 29us/sample - loss: 0.1300 - accuracy: 0.9655
[CV] ..................................... n_neurons=64, total=  15.7s
[CV] n_neurons=64 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.3637 - accuracy: 0.8984 - val_loss: 0.2041 - val_accuracy: 0.9416
Epoch 2/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1899 - accuracy: 0.9437 - val_loss: 0.1601 - val_accuracy: 0.9534
Epoch 3/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1454 - accuracy: 0.9569 - val_loss: 0.1347 - val_accuracy: 0.9610
Epoch 4/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1195 - accuracy: 0.9647 - val_loss: 0.1252 - val_accuracy: 0.9638
Epoch 5/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1022 - accuracy: 0.9702 - val_loss: 0.1234 - val_accuracy: 0.9650
Epoch 6/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0892 - accuracy: 0.9746 - val_loss: 0.1273 - val_accuracy: 0.9650
Epoch 7/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0794 - accuracy: 0.9768 - val_loss: 0.1185 - val_accuracy: 0.9648
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0719 - accuracy: 0.9794 - val_loss: 0.1135 - val_accuracy: 0.9692
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0652 - accuracy: 0.9815 - val_loss: 0.1187 - val_accuracy: 0.9650
Epoch 10/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0596 - accuracy: 0.9831 - val_loss: 0.1256 - val_accuracy: 0.9652
18333/18333 [==============================] - 1s 56us/sample - loss: 0.1363 - accuracy: 0.9627
[CV] ..................................... n_neurons=64, total=  16.0s
[CV] n_neurons=65 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.3517 - accuracy: 0.9014 - val_loss: 0.2148 - val_accuracy: 0.9436
Epoch 2/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1875 - accuracy: 0.9460 - val_loss: 0.1583 - val_accuracy: 0.9550
Epoch 3/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1404 - accuracy: 0.9580 - val_loss: 0.1336 - val_accuracy: 0.9628
Epoch 4/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1139 - accuracy: 0.9660 - val_loss: 0.1289 - val_accuracy: 0.9628
Epoch 5/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0962 - accuracy: 0.9722 - val_loss: 0.1222 - val_accuracy: 0.9688
Epoch 6/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0833 - accuracy: 0.9763 - val_loss: 0.1166 - val_accuracy: 0.9686
Epoch 7/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0727 - accuracy: 0.9780 - val_loss: 0.1143 - val_accuracy: 0.9698
Epoch 8/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0659 - accuracy: 0.9809 - val_loss: 0.1205 - val_accuracy: 0.9684
Epoch 9/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0588 - accuracy: 0.9837 - val_loss: 0.1104 - val_accuracy: 0.9712
Epoch 10/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0528 - accuracy: 0.9848 - val_loss: 0.1082 - val_accuracy: 0.9710
Epoch 11/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0477 - accuracy: 0.9866 - val_loss: 0.1119 - val_accuracy: 0.9702
Epoch 12/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0429 - accuracy: 0.9884 - val_loss: 0.1144 - val_accuracy: 0.9712
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1276 - accuracy: 0.9675
[CV] ..................................... n_neurons=65, total=  18.8s
[CV] n_neurons=65 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3482 - accuracy: 0.9026 - val_loss: 0.2112 - val_accuracy: 0.9374
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1826 - accuracy: 0.9481 - val_loss: 0.1540 - val_accuracy: 0.9574
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1368 - accuracy: 0.9610 - val_loss: 0.1346 - val_accuracy: 0.9620
Epoch 4/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1102 - accuracy: 0.9686 - val_loss: 0.1169 - val_accuracy: 0.9684
Epoch 5/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0932 - accuracy: 0.9738 - val_loss: 0.1270 - val_accuracy: 0.9650
Epoch 6/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0802 - accuracy: 0.9767 - val_loss: 0.1165 - val_accuracy: 0.9670
Epoch 7/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0710 - accuracy: 0.9795 - val_loss: 0.1100 - val_accuracy: 0.9696
Epoch 8/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0622 - accuracy: 0.9819 - val_loss: 0.1077 - val_accuracy: 0.9724
Epoch 9/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.0562 - accuracy: 0.9841 - val_loss: 0.1098 - val_accuracy: 0.9738
Epoch 10/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0518 - accuracy: 0.9853 - val_loss: 0.1171 - val_accuracy: 0.9712
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1267 - accuracy: 0.9669
[CV] ..................................... n_neurons=65, total=  15.6s
[CV] n_neurons=65 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.3670 - accuracy: 0.8963 - val_loss: 0.2077 - val_accuracy: 0.9404
Epoch 2/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1921 - accuracy: 0.9439 - val_loss: 0.1622 - val_accuracy: 0.9532
Epoch 3/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1449 - accuracy: 0.9585 - val_loss: 0.1369 - val_accuracy: 0.9622
Epoch 4/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1180 - accuracy: 0.9654 - val_loss: 0.1277 - val_accuracy: 0.9632
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0993 - accuracy: 0.9719 - val_loss: 0.1177 - val_accuracy: 0.9676
Epoch 6/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0859 - accuracy: 0.9757 - val_loss: 0.1199 - val_accuracy: 0.9640
Epoch 7/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0748 - accuracy: 0.9786 - val_loss: 0.1066 - val_accuracy: 0.9700
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0675 - accuracy: 0.9807 - val_loss: 0.1042 - val_accuracy: 0.9698
Epoch 9/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0599 - accuracy: 0.9835 - val_loss: 0.1123 - val_accuracy: 0.9708
Epoch 10/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0554 - accuracy: 0.9848 - val_loss: 0.1226 - val_accuracy: 0.9700
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1270 - accuracy: 0.9651
[CV] ..................................... n_neurons=65, total=  16.1s
[CV] n_neurons=66 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.3547 - accuracy: 0.9013 - val_loss: 0.2127 - val_accuracy: 0.9430
Epoch 2/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1846 - accuracy: 0.9459 - val_loss: 0.1594 - val_accuracy: 0.9524
Epoch 3/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.1397 - accuracy: 0.9597 - val_loss: 0.1364 - val_accuracy: 0.9576
Epoch 4/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1131 - accuracy: 0.9673 - val_loss: 0.1311 - val_accuracy: 0.9610
Epoch 5/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0976 - accuracy: 0.9726 - val_loss: 0.1192 - val_accuracy: 0.9654
Epoch 6/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0844 - accuracy: 0.9760 - val_loss: 0.1230 - val_accuracy: 0.9628
Epoch 7/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0731 - accuracy: 0.9787 - val_loss: 0.1208 - val_accuracy: 0.9662
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1360 - accuracy: 0.9627
[CV] ..................................... n_neurons=66, total=  11.5s
[CV] n_neurons=66 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3477 - accuracy: 0.9035 - val_loss: 0.2060 - val_accuracy: 0.9406
Epoch 2/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1836 - accuracy: 0.9461 - val_loss: 0.1523 - val_accuracy: 0.9592
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1383 - accuracy: 0.9604 - val_loss: 0.1342 - val_accuracy: 0.9612
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1132 - accuracy: 0.9679 - val_loss: 0.1195 - val_accuracy: 0.9674
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0960 - accuracy: 0.9722 - val_loss: 0.1298 - val_accuracy: 0.9652
Epoch 6/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0828 - accuracy: 0.9765 - val_loss: 0.1248 - val_accuracy: 0.9668
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1437 - accuracy: 0.9603
[CV] ..................................... n_neurons=66, total=   9.7s
[CV] n_neurons=66 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.3547 - accuracy: 0.8996 - val_loss: 0.2032 - val_accuracy: 0.9392
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1869 - accuracy: 0.9454 - val_loss: 0.1534 - val_accuracy: 0.9554
Epoch 3/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1419 - accuracy: 0.9585 - val_loss: 0.1287 - val_accuracy: 0.9658
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1163 - accuracy: 0.9658 - val_loss: 0.1216 - val_accuracy: 0.9642
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0976 - accuracy: 0.9722 - val_loss: 0.1196 - val_accuracy: 0.9648
Epoch 6/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0844 - accuracy: 0.9760 - val_loss: 0.1161 - val_accuracy: 0.9682
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0739 - accuracy: 0.9790 - val_loss: 0.1120 - val_accuracy: 0.9650
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0656 - accuracy: 0.9815 - val_loss: 0.1126 - val_accuracy: 0.9688
Epoch 9/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0592 - accuracy: 0.9837 - val_loss: 0.1108 - val_accuracy: 0.9700
Epoch 10/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0535 - accuracy: 0.9846 - val_loss: 0.1197 - val_accuracy: 0.9676
Epoch 11/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0478 - accuracy: 0.9869 - val_loss: 0.1098 - val_accuracy: 0.9714
Epoch 12/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0441 - accuracy: 0.9876 - val_loss: 0.1100 - val_accuracy: 0.9694
Epoch 13/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0399 - accuracy: 0.9895 - val_loss: 0.1176 - val_accuracy: 0.9704
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1399 - accuracy: 0.9671
[CV] ..................................... n_neurons=66, total=  21.8s
[CV] n_neurons=67 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.3440 - accuracy: 0.9038 - val_loss: 0.2077 - val_accuracy: 0.9454
Epoch 2/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1784 - accuracy: 0.9480 - val_loss: 0.1518 - val_accuracy: 0.9586
Epoch 3/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1371 - accuracy: 0.9602 - val_loss: 0.1282 - val_accuracy: 0.9632
Epoch 4/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1119 - accuracy: 0.9671 - val_loss: 0.1261 - val_accuracy: 0.9634
Epoch 5/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0962 - accuracy: 0.9726 - val_loss: 0.1167 - val_accuracy: 0.9672
Epoch 6/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0838 - accuracy: 0.9756 - val_loss: 0.1130 - val_accuracy: 0.9684
Epoch 7/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0734 - accuracy: 0.9785 - val_loss: 0.1171 - val_accuracy: 0.9684
Epoch 8/30
36666/36666 [==============================] - 2s 41us/sample - loss: 0.0653 - accuracy: 0.9817 - val_loss: 0.1225 - val_accuracy: 0.9680
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1339 - accuracy: 0.9631
[CV] ..................................... n_neurons=67, total=  13.2s
[CV] n_neurons=67 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.3553 - accuracy: 0.9012 - val_loss: 0.1969 - val_accuracy: 0.9460
Epoch 2/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1852 - accuracy: 0.9468 - val_loss: 0.1497 - val_accuracy: 0.9582
Epoch 3/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1406 - accuracy: 0.9595 - val_loss: 0.1286 - val_accuracy: 0.9618
Epoch 4/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1148 - accuracy: 0.9671 - val_loss: 0.1154 - val_accuracy: 0.9658
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0958 - accuracy: 0.9722 - val_loss: 0.1190 - val_accuracy: 0.9676
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0820 - accuracy: 0.9756 - val_loss: 0.1072 - val_accuracy: 0.9690
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0713 - accuracy: 0.9794 - val_loss: 0.1041 - val_accuracy: 0.9716
Epoch 8/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0623 - accuracy: 0.9824 - val_loss: 0.1057 - val_accuracy: 0.9718
Epoch 9/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0566 - accuracy: 0.9843 - val_loss: 0.1143 - val_accuracy: 0.9700
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1233 - accuracy: 0.9668
[CV] ..................................... n_neurons=67, total=  14.9s
[CV] n_neurons=67 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.3664 - accuracy: 0.8985 - val_loss: 0.2070 - val_accuracy: 0.9406
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1855 - accuracy: 0.9461 - val_loss: 0.1544 - val_accuracy: 0.9544
Epoch 3/30
36667/36667 [==============================] - 1s 39us/sample - loss: 0.1369 - accuracy: 0.9596 - val_loss: 0.1265 - val_accuracy: 0.9662
Epoch 4/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.1112 - accuracy: 0.9674 - val_loss: 0.1230 - val_accuracy: 0.9658
Epoch 5/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0926 - accuracy: 0.9730 - val_loss: 0.1108 - val_accuracy: 0.9686
Epoch 6/30
36667/36667 [==============================] - 1s 41us/sample - loss: 0.0802 - accuracy: 0.9765 - val_loss: 0.1104 - val_accuracy: 0.9686
Epoch 7/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0691 - accuracy: 0.9804 - val_loss: 0.1024 - val_accuracy: 0.9726
Epoch 8/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0614 - accuracy: 0.9821 - val_loss: 0.1050 - val_accuracy: 0.9732
Epoch 9/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0548 - accuracy: 0.9843 - val_loss: 0.1156 - val_accuracy: 0.9708
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1259 - accuracy: 0.9650
[CV] ..................................... n_neurons=67, total=  14.2s
[CV] n_neurons=68 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.3509 - accuracy: 0.8984 - val_loss: 0.2167 - val_accuracy: 0.9422
Epoch 2/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1836 - accuracy: 0.9463 - val_loss: 0.1678 - val_accuracy: 0.9518
Epoch 3/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1385 - accuracy: 0.9596 - val_loss: 0.1313 - val_accuracy: 0.9594
Epoch 4/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1129 - accuracy: 0.9665 - val_loss: 0.1217 - val_accuracy: 0.9642
Epoch 5/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0972 - accuracy: 0.9721 - val_loss: 0.1164 - val_accuracy: 0.9654
Epoch 6/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0845 - accuracy: 0.9765 - val_loss: 0.1182 - val_accuracy: 0.9648
Epoch 7/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0742 - accuracy: 0.9789 - val_loss: 0.1136 - val_accuracy: 0.9672
Epoch 8/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0670 - accuracy: 0.9810 - val_loss: 0.1141 - val_accuracy: 0.9674
Epoch 9/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.0593 - accuracy: 0.9836 - val_loss: 0.1084 - val_accuracy: 0.9712
Epoch 10/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0532 - accuracy: 0.9852 - val_loss: 0.1124 - val_accuracy: 0.9688
Epoch 11/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0486 - accuracy: 0.9863 - val_loss: 0.1085 - val_accuracy: 0.9702
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1337 - accuracy: 0.9657
[CV] ..................................... n_neurons=68, total=  17.4s
[CV] n_neurons=68 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.3479 - accuracy: 0.9021 - val_loss: 0.2066 - val_accuracy: 0.9388
Epoch 2/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1866 - accuracy: 0.9464 - val_loss: 0.1557 - val_accuracy: 0.9550
Epoch 3/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1414 - accuracy: 0.9593 - val_loss: 0.1399 - val_accuracy: 0.9590
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1166 - accuracy: 0.9664 - val_loss: 0.1184 - val_accuracy: 0.9656
Epoch 5/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0988 - accuracy: 0.9712 - val_loss: 0.1304 - val_accuracy: 0.9646
Epoch 6/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.0848 - accuracy: 0.9751 - val_loss: 0.1110 - val_accuracy: 0.9702
Epoch 7/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0741 - accuracy: 0.9784 - val_loss: 0.1097 - val_accuracy: 0.9684
Epoch 8/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.0653 - accuracy: 0.9811 - val_loss: 0.1152 - val_accuracy: 0.9706
Epoch 9/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.0592 - accuracy: 0.9828 - val_loss: 0.1122 - val_accuracy: 0.9708
18333/18333 [==============================] - 1s 27us/sample - loss: 0.1399 - accuracy: 0.9615
[CV] ..................................... n_neurons=68, total=  17.7s
[CV] n_neurons=68 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 64us/sample - loss: 0.3529 - accuracy: 0.9008 - val_loss: 0.2007 - val_accuracy: 0.9422
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1797 - accuracy: 0.9473 - val_loss: 0.1527 - val_accuracy: 0.9538
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1340 - accuracy: 0.9603 - val_loss: 0.1253 - val_accuracy: 0.9656
Epoch 4/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.1092 - accuracy: 0.9681 - val_loss: 0.1222 - val_accuracy: 0.9638
Epoch 5/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.0925 - accuracy: 0.9735 - val_loss: 0.1162 - val_accuracy: 0.9686
Epoch 6/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0810 - accuracy: 0.9770 - val_loss: 0.1145 - val_accuracy: 0.9672
Epoch 7/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0707 - accuracy: 0.9794 - val_loss: 0.1056 - val_accuracy: 0.9706
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0633 - accuracy: 0.9817 - val_loss: 0.1055 - val_accuracy: 0.9744
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0573 - accuracy: 0.9834 - val_loss: 0.1159 - val_accuracy: 0.9694
Epoch 10/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0527 - accuracy: 0.9856 - val_loss: 0.1170 - val_accuracy: 0.9664
18333/18333 [==============================] - 1s 76us/sample - loss: 0.1340 - accuracy: 0.9647
[CV] ..................................... n_neurons=68, total=  20.6s
[CV] n_neurons=69 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.3569 - accuracy: 0.8999 - val_loss: 0.2178 - val_accuracy: 0.9418
Epoch 2/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1888 - accuracy: 0.9455 - val_loss: 0.1622 - val_accuracy: 0.9526
Epoch 3/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1438 - accuracy: 0.9586 - val_loss: 0.1324 - val_accuracy: 0.9626
Epoch 4/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1168 - accuracy: 0.9654 - val_loss: 0.1310 - val_accuracy: 0.9612
Epoch 5/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0998 - accuracy: 0.9709 - val_loss: 0.1122 - val_accuracy: 0.9700
Epoch 6/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0871 - accuracy: 0.9750 - val_loss: 0.1212 - val_accuracy: 0.9656
Epoch 7/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0767 - accuracy: 0.9776 - val_loss: 0.1111 - val_accuracy: 0.9710
Epoch 8/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0678 - accuracy: 0.9809 - val_loss: 0.1201 - val_accuracy: 0.9662
Epoch 9/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0612 - accuracy: 0.9833 - val_loss: 0.1124 - val_accuracy: 0.9706
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1277 - accuracy: 0.9655
[CV] ..................................... n_neurons=69, total=  15.9s
[CV] n_neurons=69 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.3544 - accuracy: 0.9020 - val_loss: 0.2021 - val_accuracy: 0.9422
Epoch 2/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1799 - accuracy: 0.9478 - val_loss: 0.1521 - val_accuracy: 0.9588
Epoch 3/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1343 - accuracy: 0.9610 - val_loss: 0.1345 - val_accuracy: 0.9606
Epoch 4/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.1102 - accuracy: 0.9691 - val_loss: 0.1203 - val_accuracy: 0.9652
Epoch 5/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0937 - accuracy: 0.9726 - val_loss: 0.1272 - val_accuracy: 0.9646
Epoch 6/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.0805 - accuracy: 0.9764 - val_loss: 0.1116 - val_accuracy: 0.9690
Epoch 7/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.0714 - accuracy: 0.9790 - val_loss: 0.1136 - val_accuracy: 0.9686
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0628 - accuracy: 0.9816 - val_loss: 0.1148 - val_accuracy: 0.9696
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1281 - accuracy: 0.9650
[CV] ..................................... n_neurons=69, total=  15.9s
[CV] n_neurons=69 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.3494 - accuracy: 0.9013 - val_loss: 0.1944 - val_accuracy: 0.9450
Epoch 2/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1778 - accuracy: 0.9487 - val_loss: 0.1453 - val_accuracy: 0.9598
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1330 - accuracy: 0.9615 - val_loss: 0.1264 - val_accuracy: 0.9656
Epoch 4/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.1087 - accuracy: 0.9689 - val_loss: 0.1166 - val_accuracy: 0.9654
Epoch 5/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0908 - accuracy: 0.9747 - val_loss: 0.1158 - val_accuracy: 0.9698
Epoch 6/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0788 - accuracy: 0.9771 - val_loss: 0.1168 - val_accuracy: 0.9676
Epoch 7/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0687 - accuracy: 0.9806 - val_loss: 0.1032 - val_accuracy: 0.9716
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0610 - accuracy: 0.9829 - val_loss: 0.1008 - val_accuracy: 0.9712
Epoch 9/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0544 - accuracy: 0.9844 - val_loss: 0.1093 - val_accuracy: 0.9708
Epoch 10/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0501 - accuracy: 0.9857 - val_loss: 0.1175 - val_accuracy: 0.9700
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1279 - accuracy: 0.9677
[CV] ..................................... n_neurons=69, total=  18.2s
[CV] n_neurons=70 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.3562 - accuracy: 0.9003 - val_loss: 0.2106 - val_accuracy: 0.9436
Epoch 2/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1892 - accuracy: 0.9455 - val_loss: 0.1653 - val_accuracy: 0.9540
Epoch 3/30
36666/36666 [==============================] - 1s 40us/sample - loss: 0.1423 - accuracy: 0.9587 - val_loss: 0.1353 - val_accuracy: 0.9614
Epoch 4/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1151 - accuracy: 0.9670 - val_loss: 0.1277 - val_accuracy: 0.9640
Epoch 5/30
36666/36666 [==============================] - 1s 41us/sample - loss: 0.0988 - accuracy: 0.9718 - val_loss: 0.1154 - val_accuracy: 0.9678
Epoch 6/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0846 - accuracy: 0.9761 - val_loss: 0.1175 - val_accuracy: 0.9666
Epoch 7/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0737 - accuracy: 0.9794 - val_loss: 0.1161 - val_accuracy: 0.9686
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1393 - accuracy: 0.9619
[CV] ..................................... n_neurons=70, total=  11.5s
[CV] n_neurons=70 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.3485 - accuracy: 0.9041 - val_loss: 0.2035 - val_accuracy: 0.9390
Epoch 2/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1863 - accuracy: 0.9467 - val_loss: 0.1553 - val_accuracy: 0.9560
Epoch 3/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1418 - accuracy: 0.9587 - val_loss: 0.1326 - val_accuracy: 0.9590
Epoch 4/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.1153 - accuracy: 0.9670 - val_loss: 0.1157 - val_accuracy: 0.9672
Epoch 5/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0975 - accuracy: 0.9719 - val_loss: 0.1246 - val_accuracy: 0.9642
Epoch 6/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0831 - accuracy: 0.9758 - val_loss: 0.1080 - val_accuracy: 0.9696
Epoch 7/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0733 - accuracy: 0.9784 - val_loss: 0.1080 - val_accuracy: 0.9670
Epoch 8/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.0641 - accuracy: 0.9814 - val_loss: 0.1081 - val_accuracy: 0.9710
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1317 - accuracy: 0.9651
[CV] ..................................... n_neurons=70, total=  12.8s
[CV] n_neurons=70 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.3561 - accuracy: 0.8993 - val_loss: 0.2107 - val_accuracy: 0.9404
Epoch 2/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1863 - accuracy: 0.9466 - val_loss: 0.1583 - val_accuracy: 0.9560
Epoch 3/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1375 - accuracy: 0.9606 - val_loss: 0.1297 - val_accuracy: 0.9638
Epoch 4/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1116 - accuracy: 0.9685 - val_loss: 0.1228 - val_accuracy: 0.9644
Epoch 5/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0942 - accuracy: 0.9735 - val_loss: 0.1169 - val_accuracy: 0.9666
Epoch 6/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0821 - accuracy: 0.9771 - val_loss: 0.1158 - val_accuracy: 0.9642
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0715 - accuracy: 0.9797 - val_loss: 0.1088 - val_accuracy: 0.9680
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0632 - accuracy: 0.9823 - val_loss: 0.1112 - val_accuracy: 0.9692
Epoch 9/30
36667/36667 [==============================] - 1s 40us/sample - loss: 0.0569 - accuracy: 0.9838 - val_loss: 0.1081 - val_accuracy: 0.9718
Epoch 10/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0526 - accuracy: 0.9850 - val_loss: 0.1204 - val_accuracy: 0.9688
Epoch 11/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0469 - accuracy: 0.9869 - val_loss: 0.1096 - val_accuracy: 0.9710
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1277 - accuracy: 0.9671
[CV] ..................................... n_neurons=70, total=  17.8s
[CV] n_neurons=71 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.3515 - accuracy: 0.9005 - val_loss: 0.2212 - val_accuracy: 0.9428
Epoch 2/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.1812 - accuracy: 0.9469 - val_loss: 0.1500 - val_accuracy: 0.9580
Epoch 3/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1344 - accuracy: 0.9604 - val_loss: 0.1277 - val_accuracy: 0.9636
Epoch 4/30
36666/36666 [==============================] - 2s 64us/sample - loss: 0.1069 - accuracy: 0.9681 - val_loss: 0.1297 - val_accuracy: 0.9602
Epoch 5/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.0924 - accuracy: 0.9735 - val_loss: 0.1145 - val_accuracy: 0.9678
Epoch 6/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.0808 - accuracy: 0.9761 - val_loss: 0.1128 - val_accuracy: 0.9686
Epoch 7/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0700 - accuracy: 0.9796 - val_loss: 0.1225 - val_accuracy: 0.9672
Epoch 8/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.0635 - accuracy: 0.9820 - val_loss: 0.1136 - val_accuracy: 0.9686
18334/18334 [==============================] - 1s 37us/sample - loss: 0.1365 - accuracy: 0.9639
[CV] ..................................... n_neurons=71, total=  16.1s
[CV] n_neurons=71 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 64us/sample - loss: 0.3517 - accuracy: 0.9016 - val_loss: 0.2037 - val_accuracy: 0.9406
Epoch 2/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1869 - accuracy: 0.9466 - val_loss: 0.1475 - val_accuracy: 0.9604
Epoch 3/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1397 - accuracy: 0.9600 - val_loss: 0.1273 - val_accuracy: 0.9628
Epoch 4/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1127 - accuracy: 0.9674 - val_loss: 0.1127 - val_accuracy: 0.9668
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0951 - accuracy: 0.9721 - val_loss: 0.1214 - val_accuracy: 0.9646
Epoch 6/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0816 - accuracy: 0.9767 - val_loss: 0.1066 - val_accuracy: 0.9696
Epoch 7/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0712 - accuracy: 0.9794 - val_loss: 0.1091 - val_accuracy: 0.9664
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0627 - accuracy: 0.9822 - val_loss: 0.1040 - val_accuracy: 0.9716
Epoch 9/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.0569 - accuracy: 0.9834 - val_loss: 0.1025 - val_accuracy: 0.9710
Epoch 10/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0511 - accuracy: 0.9851 - val_loss: 0.1113 - val_accuracy: 0.9710
Epoch 11/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0464 - accuracy: 0.9871 - val_loss: 0.1097 - val_accuracy: 0.9708
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1241 - accuracy: 0.9689
[CV] ..................................... n_neurons=71, total=  21.1s
[CV] n_neurons=71 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3506 - accuracy: 0.9020 - val_loss: 0.2036 - val_accuracy: 0.9396
Epoch 2/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1788 - accuracy: 0.9479 - val_loss: 0.1569 - val_accuracy: 0.9550
Epoch 3/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.1333 - accuracy: 0.9611 - val_loss: 0.1296 - val_accuracy: 0.9634
Epoch 4/30
36667/36667 [==============================] - 2s 68us/sample - loss: 0.1091 - accuracy: 0.9683 - val_loss: 0.1269 - val_accuracy: 0.9644
Epoch 5/30
36667/36667 [==============================] - 3s 71us/sample - loss: 0.0915 - accuracy: 0.9741 - val_loss: 0.1154 - val_accuracy: 0.9684
Epoch 6/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0802 - accuracy: 0.9771 - val_loss: 0.1202 - val_accuracy: 0.9672
Epoch 7/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0706 - accuracy: 0.9798 - val_loss: 0.1108 - val_accuracy: 0.9686
Epoch 8/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0627 - accuracy: 0.9828 - val_loss: 0.1097 - val_accuracy: 0.9708
Epoch 9/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0569 - accuracy: 0.9845 - val_loss: 0.1129 - val_accuracy: 0.9696
Epoch 10/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0519 - accuracy: 0.9858 - val_loss: 0.1152 - val_accuracy: 0.9700
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1291 - accuracy: 0.9661
[CV] ..................................... n_neurons=71, total=  21.7s
[CV] n_neurons=72 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 58us/sample - loss: 0.3407 - accuracy: 0.9051 - val_loss: 0.2028 - val_accuracy: 0.9460
Epoch 2/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1735 - accuracy: 0.9494 - val_loss: 0.1502 - val_accuracy: 0.9570
Epoch 3/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1314 - accuracy: 0.9614 - val_loss: 0.1284 - val_accuracy: 0.9610
Epoch 4/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1073 - accuracy: 0.9680 - val_loss: 0.1177 - val_accuracy: 0.9638
Epoch 5/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0914 - accuracy: 0.9734 - val_loss: 0.1158 - val_accuracy: 0.9694
Epoch 6/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0788 - accuracy: 0.9771 - val_loss: 0.1166 - val_accuracy: 0.9664
Epoch 7/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0690 - accuracy: 0.9797 - val_loss: 0.1099 - val_accuracy: 0.9682
Epoch 8/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0607 - accuracy: 0.9824 - val_loss: 0.1113 - val_accuracy: 0.9714
Epoch 9/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0551 - accuracy: 0.9843 - val_loss: 0.1086 - val_accuracy: 0.9694
Epoch 10/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0489 - accuracy: 0.9862 - val_loss: 0.1069 - val_accuracy: 0.9704
Epoch 11/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0441 - accuracy: 0.9876 - val_loss: 0.1156 - val_accuracy: 0.9710
Epoch 12/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0390 - accuracy: 0.9891 - val_loss: 0.1129 - val_accuracy: 0.9702
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1356 - accuracy: 0.9664
[CV] ..................................... n_neurons=72, total=  21.7s
[CV] n_neurons=72 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.3441 - accuracy: 0.9050 - val_loss: 0.1983 - val_accuracy: 0.9436
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1749 - accuracy: 0.9494 - val_loss: 0.1456 - val_accuracy: 0.9590
Epoch 3/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1303 - accuracy: 0.9616 - val_loss: 0.1262 - val_accuracy: 0.9624
Epoch 4/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1056 - accuracy: 0.9690 - val_loss: 0.1118 - val_accuracy: 0.9666
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0885 - accuracy: 0.9735 - val_loss: 0.1259 - val_accuracy: 0.9660
Epoch 6/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0759 - accuracy: 0.9779 - val_loss: 0.1101 - val_accuracy: 0.9702
Epoch 7/30
36667/36667 [==============================] - 3s 73us/sample - loss: 0.0660 - accuracy: 0.9806 - val_loss: 0.1124 - val_accuracy: 0.9680
Epoch 8/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0580 - accuracy: 0.9832 - val_loss: 0.1108 - val_accuracy: 0.9686
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1308 - accuracy: 0.9638
[CV] ..................................... n_neurons=72, total=  16.0s
[CV] n_neurons=72 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.3493 - accuracy: 0.9033 - val_loss: 0.1957 - val_accuracy: 0.9448
Epoch 2/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1774 - accuracy: 0.9473 - val_loss: 0.1464 - val_accuracy: 0.9586
Epoch 3/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1307 - accuracy: 0.9609 - val_loss: 0.1210 - val_accuracy: 0.9670
Epoch 4/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1049 - accuracy: 0.9691 - val_loss: 0.1206 - val_accuracy: 0.9656
Epoch 5/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0873 - accuracy: 0.9747 - val_loss: 0.1115 - val_accuracy: 0.9672
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0753 - accuracy: 0.9782 - val_loss: 0.1137 - val_accuracy: 0.9680
Epoch 7/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0655 - accuracy: 0.9818 - val_loss: 0.1068 - val_accuracy: 0.9698
Epoch 8/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0586 - accuracy: 0.9832 - val_loss: 0.1036 - val_accuracy: 0.9714
Epoch 9/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0524 - accuracy: 0.9855 - val_loss: 0.1107 - val_accuracy: 0.9708
Epoch 10/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0471 - accuracy: 0.9865 - val_loss: 0.1143 - val_accuracy: 0.9706
18333/18333 [==============================] - 1s 28us/sample - loss: 0.1222 - accuracy: 0.9683
[CV] ..................................... n_neurons=72, total=  18.8s
[CV] n_neurons=73 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 63us/sample - loss: 0.3445 - accuracy: 0.9026 - val_loss: 0.2001 - val_accuracy: 0.9436
Epoch 2/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.1783 - accuracy: 0.9478 - val_loss: 0.1516 - val_accuracy: 0.9532
Epoch 3/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.1345 - accuracy: 0.9608 - val_loss: 0.1257 - val_accuracy: 0.9608
Epoch 4/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.1089 - accuracy: 0.9680 - val_loss: 0.1207 - val_accuracy: 0.9620
Epoch 5/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0939 - accuracy: 0.9739 - val_loss: 0.1117 - val_accuracy: 0.9666
Epoch 6/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.0810 - accuracy: 0.9765 - val_loss: 0.1162 - val_accuracy: 0.9644
Epoch 7/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.0705 - accuracy: 0.9795 - val_loss: 0.1073 - val_accuracy: 0.9704
Epoch 8/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0640 - accuracy: 0.9819 - val_loss: 0.1166 - val_accuracy: 0.9678
Epoch 9/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.0564 - accuracy: 0.9845 - val_loss: 0.1034 - val_accuracy: 0.9712
Epoch 10/30
36666/36666 [==============================] - 2s 65us/sample - loss: 0.0510 - accuracy: 0.9855 - val_loss: 0.1058 - val_accuracy: 0.9710
Epoch 11/30
36666/36666 [==============================] - 2s 67us/sample - loss: 0.0469 - accuracy: 0.9870 - val_loss: 0.1108 - val_accuracy: 0.9700
18334/18334 [==============================] - 1s 30us/sample - loss: 0.1306 - accuracy: 0.9660
[CV] ..................................... n_neurons=73, total=  24.0s
[CV] n_neurons=73 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 3s 69us/sample - loss: 0.3454 - accuracy: 0.9038 - val_loss: 0.1995 - val_accuracy: 0.9426
Epoch 2/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1819 - accuracy: 0.9473 - val_loss: 0.1491 - val_accuracy: 0.9588
Epoch 3/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1387 - accuracy: 0.9599 - val_loss: 0.1292 - val_accuracy: 0.9630
Epoch 4/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1143 - accuracy: 0.9675 - val_loss: 0.1192 - val_accuracy: 0.9660
Epoch 5/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0966 - accuracy: 0.9717 - val_loss: 0.1341 - val_accuracy: 0.9642
Epoch 6/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0825 - accuracy: 0.9757 - val_loss: 0.1144 - val_accuracy: 0.9676
Epoch 7/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0714 - accuracy: 0.9785 - val_loss: 0.1187 - val_accuracy: 0.9656
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0627 - accuracy: 0.9816 - val_loss: 0.1180 - val_accuracy: 0.9690
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1385 - accuracy: 0.9618
[CV] ..................................... n_neurons=73, total=  15.9s
[CV] n_neurons=73 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.3481 - accuracy: 0.9022 - val_loss: 0.1892 - val_accuracy: 0.9480
Epoch 2/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1751 - accuracy: 0.9495 - val_loss: 0.1517 - val_accuracy: 0.9578
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1308 - accuracy: 0.9618 - val_loss: 0.1240 - val_accuracy: 0.9634
Epoch 4/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1049 - accuracy: 0.9690 - val_loss: 0.1205 - val_accuracy: 0.9630
Epoch 5/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0880 - accuracy: 0.9746 - val_loss: 0.1069 - val_accuracy: 0.9702
Epoch 6/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0758 - accuracy: 0.9785 - val_loss: 0.1224 - val_accuracy: 0.9656
Epoch 7/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.0656 - accuracy: 0.9815 - val_loss: 0.0974 - val_accuracy: 0.9708
Epoch 8/30
36667/36667 [==============================] - 3s 69us/sample - loss: 0.0586 - accuracy: 0.9836 - val_loss: 0.1001 - val_accuracy: 0.9736
Epoch 9/30
36667/36667 [==============================] - 2s 66us/sample - loss: 0.0521 - accuracy: 0.9859 - val_loss: 0.1040 - val_accuracy: 0.9736
18333/18333 [==============================] - 1s 35us/sample - loss: 0.1231 - accuracy: 0.9665
[CV] ..................................... n_neurons=73, total=  19.1s
[CV] n_neurons=74 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 66us/sample - loss: 0.3424 - accuracy: 0.9041 - val_loss: 0.2016 - val_accuracy: 0.9460
Epoch 2/30
36666/36666 [==============================] - 2s 67us/sample - loss: 0.1771 - accuracy: 0.9488 - val_loss: 0.1536 - val_accuracy: 0.9526
Epoch 3/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.1328 - accuracy: 0.9617 - val_loss: 0.1315 - val_accuracy: 0.9624
Epoch 4/30
36666/36666 [==============================] - 2s 56us/sample - loss: 0.1078 - accuracy: 0.9682 - val_loss: 0.1282 - val_accuracy: 0.9618
Epoch 5/30
36666/36666 [==============================] - 2s 56us/sample - loss: 0.0920 - accuracy: 0.9740 - val_loss: 0.1120 - val_accuracy: 0.9686
Epoch 6/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.0789 - accuracy: 0.9768 - val_loss: 0.1148 - val_accuracy: 0.9666
Epoch 7/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0696 - accuracy: 0.9798 - val_loss: 0.1189 - val_accuracy: 0.9672
18334/18334 [==============================] - 0s 26us/sample - loss: 0.1333 - accuracy: 0.9627
[CV] ..................................... n_neurons=74, total=  15.6s
[CV] n_neurons=74 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 3s 74us/sample - loss: 0.3388 - accuracy: 0.9044 - val_loss: 0.1960 - val_accuracy: 0.9416
Epoch 2/30
36667/36667 [==============================] - 3s 82us/sample - loss: 0.1737 - accuracy: 0.9507 - val_loss: 0.1456 - val_accuracy: 0.9592
Epoch 3/30
36667/36667 [==============================] - 2s 67us/sample - loss: 0.1294 - accuracy: 0.9627 - val_loss: 0.1277 - val_accuracy: 0.9618
Epoch 4/30
36667/36667 [==============================] - 2s 63us/sample - loss: 0.1051 - accuracy: 0.9702 - val_loss: 0.1087 - val_accuracy: 0.9680
Epoch 5/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0887 - accuracy: 0.9741 - val_loss: 0.1105 - val_accuracy: 0.9680
Epoch 6/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.0752 - accuracy: 0.9785 - val_loss: 0.1039 - val_accuracy: 0.9686
Epoch 7/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0661 - accuracy: 0.9804 - val_loss: 0.1012 - val_accuracy: 0.9712
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0581 - accuracy: 0.9833 - val_loss: 0.1023 - val_accuracy: 0.9710
Epoch 9/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0523 - accuracy: 0.9851 - val_loss: 0.1039 - val_accuracy: 0.9718
18333/18333 [==============================] - 1s 29us/sample - loss: 0.1346 - accuracy: 0.9641
[CV] ..................................... n_neurons=74, total=  21.3s
[CV] n_neurons=74 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.3493 - accuracy: 0.9014 - val_loss: 0.1967 - val_accuracy: 0.9446
Epoch 2/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.1740 - accuracy: 0.9485 - val_loss: 0.1505 - val_accuracy: 0.9564
Epoch 3/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.1305 - accuracy: 0.9620 - val_loss: 0.1258 - val_accuracy: 0.9660
Epoch 4/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1068 - accuracy: 0.9691 - val_loss: 0.1250 - val_accuracy: 0.9634
Epoch 5/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.0902 - accuracy: 0.9743 - val_loss: 0.1204 - val_accuracy: 0.9670
Epoch 6/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0777 - accuracy: 0.9783 - val_loss: 0.1144 - val_accuracy: 0.9696
Epoch 7/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.0687 - accuracy: 0.9811 - val_loss: 0.1031 - val_accuracy: 0.9718
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0605 - accuracy: 0.9831 - val_loss: 0.1041 - val_accuracy: 0.9724
Epoch 9/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0545 - accuracy: 0.9850 - val_loss: 0.1110 - val_accuracy: 0.9702
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1263 - accuracy: 0.9657
[CV] ..................................... n_neurons=74, total=  19.1s
[CV] n_neurons=75 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.3462 - accuracy: 0.9021 - val_loss: 0.2065 - val_accuracy: 0.9458
Epoch 2/30
36666/36666 [==============================] - 2s 58us/sample - loss: 0.1809 - accuracy: 0.9478 - val_loss: 0.1555 - val_accuracy: 0.9574
Epoch 3/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.1361 - accuracy: 0.9597 - val_loss: 0.1280 - val_accuracy: 0.9622
Epoch 4/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.1100 - accuracy: 0.9683 - val_loss: 0.1274 - val_accuracy: 0.9640
Epoch 5/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0935 - accuracy: 0.9731 - val_loss: 0.1109 - val_accuracy: 0.9694
Epoch 6/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0804 - accuracy: 0.9767 - val_loss: 0.1169 - val_accuracy: 0.9676
Epoch 7/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0691 - accuracy: 0.9795 - val_loss: 0.1144 - val_accuracy: 0.9696
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1335 - accuracy: 0.9624
[CV] ..................................... n_neurons=75, total=  13.5s
[CV] n_neurons=75 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.3399 - accuracy: 0.9047 - val_loss: 0.1948 - val_accuracy: 0.9444
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1749 - accuracy: 0.9503 - val_loss: 0.1459 - val_accuracy: 0.9600
Epoch 3/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1316 - accuracy: 0.9627 - val_loss: 0.1223 - val_accuracy: 0.9642
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1070 - accuracy: 0.9695 - val_loss: 0.1083 - val_accuracy: 0.9682
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0911 - accuracy: 0.9736 - val_loss: 0.1122 - val_accuracy: 0.9688
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0782 - accuracy: 0.9771 - val_loss: 0.1036 - val_accuracy: 0.9684
Epoch 7/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0684 - accuracy: 0.9799 - val_loss: 0.0995 - val_accuracy: 0.9700
Epoch 8/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0592 - accuracy: 0.9828 - val_loss: 0.0987 - val_accuracy: 0.9724
Epoch 9/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0538 - accuracy: 0.9840 - val_loss: 0.0977 - val_accuracy: 0.9732
Epoch 10/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0479 - accuracy: 0.9864 - val_loss: 0.1077 - val_accuracy: 0.9710
Epoch 11/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0426 - accuracy: 0.9876 - val_loss: 0.1004 - val_accuracy: 0.9736
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1249 - accuracy: 0.9685
[CV] ..................................... n_neurons=75, total=  19.3s
[CV] n_neurons=75 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.3448 - accuracy: 0.9044 - val_loss: 0.1959 - val_accuracy: 0.9426
Epoch 2/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1840 - accuracy: 0.9452 - val_loss: 0.1510 - val_accuracy: 0.9556
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1377 - accuracy: 0.9593 - val_loss: 0.1213 - val_accuracy: 0.9636
Epoch 4/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1132 - accuracy: 0.9673 - val_loss: 0.1232 - val_accuracy: 0.9604
Epoch 5/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0948 - accuracy: 0.9719 - val_loss: 0.1109 - val_accuracy: 0.9702
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0827 - accuracy: 0.9761 - val_loss: 0.1130 - val_accuracy: 0.9664
Epoch 7/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0722 - accuracy: 0.9796 - val_loss: 0.1030 - val_accuracy: 0.9702
Epoch 8/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0643 - accuracy: 0.9819 - val_loss: 0.1003 - val_accuracy: 0.9706
Epoch 9/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0575 - accuracy: 0.9839 - val_loss: 0.1020 - val_accuracy: 0.9714
Epoch 10/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0522 - accuracy: 0.9855 - val_loss: 0.1114 - val_accuracy: 0.9708
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1297 - accuracy: 0.9665
[CV] ..................................... n_neurons=75, total=  17.2s
[CV] n_neurons=76 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.3407 - accuracy: 0.9038 - val_loss: 0.1969 - val_accuracy: 0.9478
Epoch 2/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.1746 - accuracy: 0.9492 - val_loss: 0.1460 - val_accuracy: 0.9580
Epoch 3/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.1294 - accuracy: 0.9622 - val_loss: 0.1250 - val_accuracy: 0.9638
Epoch 4/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1038 - accuracy: 0.9702 - val_loss: 0.1188 - val_accuracy: 0.9666
Epoch 5/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0889 - accuracy: 0.9749 - val_loss: 0.1099 - val_accuracy: 0.9682
Epoch 6/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0756 - accuracy: 0.9784 - val_loss: 0.1090 - val_accuracy: 0.9682
Epoch 7/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0667 - accuracy: 0.9807 - val_loss: 0.1133 - val_accuracy: 0.9702
Epoch 8/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0593 - accuracy: 0.9837 - val_loss: 0.1062 - val_accuracy: 0.9694
Epoch 9/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0528 - accuracy: 0.9852 - val_loss: 0.1029 - val_accuracy: 0.9738
Epoch 10/30
36666/36666 [==============================] - 2s 42us/sample - loss: 0.0463 - accuracy: 0.9868 - val_loss: 0.1050 - val_accuracy: 0.9720
Epoch 11/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.0420 - accuracy: 0.9880 - val_loss: 0.1106 - val_accuracy: 0.9716
18334/18334 [==============================] - 1s 28us/sample - loss: 0.1178 - accuracy: 0.9696
[CV] ..................................... n_neurons=76, total=  19.1s
[CV] n_neurons=76 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.3422 - accuracy: 0.9038 - val_loss: 0.2018 - val_accuracy: 0.9428
Epoch 2/30
36667/36667 [==============================] - 2s 41us/sample - loss: 0.1783 - accuracy: 0.9484 - val_loss: 0.1462 - val_accuracy: 0.9598
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1330 - accuracy: 0.9613 - val_loss: 0.1302 - val_accuracy: 0.9628
Epoch 4/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.1074 - accuracy: 0.9682 - val_loss: 0.1077 - val_accuracy: 0.9692
Epoch 5/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0899 - accuracy: 0.9733 - val_loss: 0.1157 - val_accuracy: 0.9652
Epoch 6/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0756 - accuracy: 0.9779 - val_loss: 0.1049 - val_accuracy: 0.9710
Epoch 7/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0660 - accuracy: 0.9804 - val_loss: 0.1027 - val_accuracy: 0.9712
Epoch 8/30
36667/36667 [==============================] - 2s 42us/sample - loss: 0.0573 - accuracy: 0.9834 - val_loss: 0.1049 - val_accuracy: 0.9704
Epoch 9/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0512 - accuracy: 0.9850 - val_loss: 0.1023 - val_accuracy: 0.9720
Epoch 10/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0461 - accuracy: 0.9869 - val_loss: 0.1094 - val_accuracy: 0.9706
Epoch 11/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0400 - accuracy: 0.9884 - val_loss: 0.1040 - val_accuracy: 0.9736
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1294 - accuracy: 0.9669
[CV] ..................................... n_neurons=76, total=  18.3s
[CV] n_neurons=76 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3422 - accuracy: 0.9045 - val_loss: 0.1904 - val_accuracy: 0.9450
Epoch 2/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1748 - accuracy: 0.9485 - val_loss: 0.1445 - val_accuracy: 0.9598
Epoch 3/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.1305 - accuracy: 0.9608 - val_loss: 0.1248 - val_accuracy: 0.9662
Epoch 4/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1060 - accuracy: 0.9696 - val_loss: 0.1143 - val_accuracy: 0.9664
Epoch 5/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0895 - accuracy: 0.9738 - val_loss: 0.1154 - val_accuracy: 0.9686
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0764 - accuracy: 0.9780 - val_loss: 0.1186 - val_accuracy: 0.9664
18333/18333 [==============================] - 0s 27us/sample - loss: 0.1380 - accuracy: 0.9617
[CV] ..................................... n_neurons=76, total=  10.9s
[CV] n_neurons=77 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 3s 80us/sample - loss: 0.3394 - accuracy: 0.9044 - val_loss: 0.2044 - val_accuracy: 0.9450
Epoch 2/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1735 - accuracy: 0.9496 - val_loss: 0.1522 - val_accuracy: 0.9556
Epoch 3/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1289 - accuracy: 0.9620 - val_loss: 0.1186 - val_accuracy: 0.9668
Epoch 4/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1040 - accuracy: 0.9699 - val_loss: 0.1174 - val_accuracy: 0.9684
Epoch 5/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.0883 - accuracy: 0.9744 - val_loss: 0.1073 - val_accuracy: 0.9684
Epoch 6/30
36666/36666 [==============================] - 2s 56us/sample - loss: 0.0753 - accuracy: 0.9783 - val_loss: 0.1182 - val_accuracy: 0.9648
Epoch 7/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0650 - accuracy: 0.9808 - val_loss: 0.1169 - val_accuracy: 0.9664
18334/18334 [==============================] - 1s 29us/sample - loss: 0.1262 - accuracy: 0.9648
[CV] ..................................... n_neurons=77, total=  14.5s
[CV] n_neurons=77 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.3320 - accuracy: 0.9083 - val_loss: 0.1929 - val_accuracy: 0.9450
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1741 - accuracy: 0.9500 - val_loss: 0.1478 - val_accuracy: 0.9570
Epoch 3/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1311 - accuracy: 0.9614 - val_loss: 0.1295 - val_accuracy: 0.9618
Epoch 4/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1059 - accuracy: 0.9693 - val_loss: 0.1108 - val_accuracy: 0.9676
Epoch 5/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0881 - accuracy: 0.9746 - val_loss: 0.1211 - val_accuracy: 0.9664
Epoch 6/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0746 - accuracy: 0.9785 - val_loss: 0.1095 - val_accuracy: 0.9704
Epoch 7/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.0648 - accuracy: 0.9809 - val_loss: 0.1038 - val_accuracy: 0.9694
Epoch 8/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0567 - accuracy: 0.9835 - val_loss: 0.1025 - val_accuracy: 0.9710
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0504 - accuracy: 0.9848 - val_loss: 0.1090 - val_accuracy: 0.9696
Epoch 10/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0454 - accuracy: 0.9871 - val_loss: 0.1103 - val_accuracy: 0.9700
18333/18333 [==============================] - 1s 28us/sample - loss: 0.1218 - accuracy: 0.9683
[CV] ..................................... n_neurons=77, total=  19.3s
[CV] n_neurons=77 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.3487 - accuracy: 0.9009 - val_loss: 0.1990 - val_accuracy: 0.9416
Epoch 2/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1750 - accuracy: 0.9480 - val_loss: 0.1483 - val_accuracy: 0.9572
Epoch 3/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1291 - accuracy: 0.9621 - val_loss: 0.1226 - val_accuracy: 0.9678
Epoch 4/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1039 - accuracy: 0.9695 - val_loss: 0.1153 - val_accuracy: 0.9682
Epoch 5/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0879 - accuracy: 0.9749 - val_loss: 0.1160 - val_accuracy: 0.9704
Epoch 6/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0754 - accuracy: 0.9787 - val_loss: 0.1171 - val_accuracy: 0.9682
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1364 - accuracy: 0.9622
[CV] ..................................... n_neurons=77, total=  11.9s
[CV] n_neurons=78 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 63us/sample - loss: 0.3401 - accuracy: 0.9042 - val_loss: 0.2014 - val_accuracy: 0.9458
Epoch 2/30
36666/36666 [==============================] - 2s 60us/sample - loss: 0.1750 - accuracy: 0.9488 - val_loss: 0.1552 - val_accuracy: 0.9536
Epoch 3/30
36666/36666 [==============================] - 3s 76us/sample - loss: 0.1288 - accuracy: 0.9628 - val_loss: 0.1253 - val_accuracy: 0.9642
Epoch 4/30
36666/36666 [==============================] - 3s 68us/sample - loss: 0.1030 - accuracy: 0.9699 - val_loss: 0.1234 - val_accuracy: 0.9638
Epoch 5/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0871 - accuracy: 0.9756 - val_loss: 0.1136 - val_accuracy: 0.9692
Epoch 6/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0748 - accuracy: 0.9783 - val_loss: 0.1122 - val_accuracy: 0.9696
Epoch 7/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0645 - accuracy: 0.9804 - val_loss: 0.1099 - val_accuracy: 0.9690
Epoch 8/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0574 - accuracy: 0.9836 - val_loss: 0.1074 - val_accuracy: 0.9724
Epoch 9/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0503 - accuracy: 0.9852 - val_loss: 0.1126 - val_accuracy: 0.9688
Epoch 10/30
36666/36666 [==============================] - 3s 75us/sample - loss: 0.0447 - accuracy: 0.9876 - val_loss: 0.1119 - val_accuracy: 0.9718
18334/18334 [==============================] - 0s 25us/sample - loss: 0.1240 - accuracy: 0.9677
[CV] ..................................... n_neurons=78, total=  22.1s
[CV] n_neurons=78 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.3416 - accuracy: 0.9036 - val_loss: 0.1968 - val_accuracy: 0.9416
Epoch 2/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1790 - accuracy: 0.9479 - val_loss: 0.1459 - val_accuracy: 0.9554
Epoch 3/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1322 - accuracy: 0.9610 - val_loss: 0.1241 - val_accuracy: 0.9620
Epoch 4/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1061 - accuracy: 0.9693 - val_loss: 0.1056 - val_accuracy: 0.9688
Epoch 5/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0883 - accuracy: 0.9746 - val_loss: 0.1160 - val_accuracy: 0.9648
Epoch 6/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0747 - accuracy: 0.9777 - val_loss: 0.1021 - val_accuracy: 0.9710
Epoch 7/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0654 - accuracy: 0.9803 - val_loss: 0.0985 - val_accuracy: 0.9706
Epoch 8/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0574 - accuracy: 0.9835 - val_loss: 0.1013 - val_accuracy: 0.9716
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0505 - accuracy: 0.9861 - val_loss: 0.1003 - val_accuracy: 0.9728
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1296 - accuracy: 0.9648
[CV] ..................................... n_neurons=78, total=  17.5s
[CV] n_neurons=78 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.3426 - accuracy: 0.9043 - val_loss: 0.1880 - val_accuracy: 0.9452
Epoch 2/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1698 - accuracy: 0.9496 - val_loss: 0.1422 - val_accuracy: 0.9584
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1269 - accuracy: 0.9633 - val_loss: 0.1164 - val_accuracy: 0.9658
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1029 - accuracy: 0.9698 - val_loss: 0.1150 - val_accuracy: 0.9660
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0863 - accuracy: 0.9758 - val_loss: 0.1060 - val_accuracy: 0.9694
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0738 - accuracy: 0.9786 - val_loss: 0.1102 - val_accuracy: 0.9686
Epoch 7/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0652 - accuracy: 0.9812 - val_loss: 0.0984 - val_accuracy: 0.9702
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0568 - accuracy: 0.9838 - val_loss: 0.0981 - val_accuracy: 0.9692
Epoch 9/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0511 - accuracy: 0.9855 - val_loss: 0.1005 - val_accuracy: 0.9718
Epoch 10/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0459 - accuracy: 0.9864 - val_loss: 0.1122 - val_accuracy: 0.9700
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1335 - accuracy: 0.9657
[CV] ..................................... n_neurons=78, total=  18.2s
[CV] n_neurons=79 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.3420 - accuracy: 0.9037 - val_loss: 0.1995 - val_accuracy: 0.9456
Epoch 2/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1756 - accuracy: 0.9495 - val_loss: 0.1447 - val_accuracy: 0.9574
Epoch 3/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.1279 - accuracy: 0.9626 - val_loss: 0.1227 - val_accuracy: 0.9662
Epoch 4/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1022 - accuracy: 0.9702 - val_loss: 0.1226 - val_accuracy: 0.9646
Epoch 5/30
36666/36666 [==============================] - 2s 61us/sample - loss: 0.0871 - accuracy: 0.9749 - val_loss: 0.1045 - val_accuracy: 0.9682
Epoch 6/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.0749 - accuracy: 0.9786 - val_loss: 0.1071 - val_accuracy: 0.9690
Epoch 7/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.0644 - accuracy: 0.9815 - val_loss: 0.1111 - val_accuracy: 0.9704
18334/18334 [==============================] - 1s 32us/sample - loss: 0.1296 - accuracy: 0.9642
[CV] ..................................... n_neurons=79, total=  14.7s
[CV] n_neurons=79 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 3s 73us/sample - loss: 0.3439 - accuracy: 0.9047 - val_loss: 0.1955 - val_accuracy: 0.9420
Epoch 2/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1752 - accuracy: 0.9494 - val_loss: 0.1462 - val_accuracy: 0.9568
Epoch 3/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1310 - accuracy: 0.9622 - val_loss: 0.1308 - val_accuracy: 0.9610
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1049 - accuracy: 0.9690 - val_loss: 0.1126 - val_accuracy: 0.9664
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0876 - accuracy: 0.9751 - val_loss: 0.1216 - val_accuracy: 0.9660
Epoch 6/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0733 - accuracy: 0.9786 - val_loss: 0.1091 - val_accuracy: 0.9682
Epoch 7/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0635 - accuracy: 0.9808 - val_loss: 0.1081 - val_accuracy: 0.9694
Epoch 8/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0546 - accuracy: 0.9845 - val_loss: 0.1086 - val_accuracy: 0.9680
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0496 - accuracy: 0.9857 - val_loss: 0.1113 - val_accuracy: 0.9692
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1258 - accuracy: 0.9671
[CV] ..................................... n_neurons=79, total=  17.3s
[CV] n_neurons=79 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.3524 - accuracy: 0.9010 - val_loss: 0.2028 - val_accuracy: 0.9426
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1780 - accuracy: 0.9481 - val_loss: 0.1567 - val_accuracy: 0.9568
Epoch 3/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1312 - accuracy: 0.9614 - val_loss: 0.1248 - val_accuracy: 0.9632
Epoch 4/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1065 - accuracy: 0.9686 - val_loss: 0.1221 - val_accuracy: 0.9642
Epoch 5/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0877 - accuracy: 0.9744 - val_loss: 0.1125 - val_accuracy: 0.9682
Epoch 6/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0750 - accuracy: 0.9779 - val_loss: 0.1110 - val_accuracy: 0.9662
Epoch 7/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0645 - accuracy: 0.9816 - val_loss: 0.1020 - val_accuracy: 0.9698
Epoch 8/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0570 - accuracy: 0.9840 - val_loss: 0.1019 - val_accuracy: 0.9708
Epoch 9/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0513 - accuracy: 0.9862 - val_loss: 0.1087 - val_accuracy: 0.9726
Epoch 10/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0464 - accuracy: 0.9877 - val_loss: 0.1141 - val_accuracy: 0.9690
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1247 - accuracy: 0.9662
[CV] ..................................... n_neurons=79, total=  19.6s
[CV] n_neurons=80 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 64us/sample - loss: 0.3417 - accuracy: 0.9046 - val_loss: 0.1977 - val_accuracy: 0.9486
Epoch 2/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1750 - accuracy: 0.9491 - val_loss: 0.1479 - val_accuracy: 0.9560
Epoch 3/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.1296 - accuracy: 0.9620 - val_loss: 0.1225 - val_accuracy: 0.9642
Epoch 4/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.1027 - accuracy: 0.9698 - val_loss: 0.1172 - val_accuracy: 0.9656
Epoch 5/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0867 - accuracy: 0.9747 - val_loss: 0.1092 - val_accuracy: 0.9700
Epoch 6/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0735 - accuracy: 0.9789 - val_loss: 0.1089 - val_accuracy: 0.9678
Epoch 7/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0641 - accuracy: 0.9809 - val_loss: 0.1122 - val_accuracy: 0.9704
Epoch 8/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0570 - accuracy: 0.9835 - val_loss: 0.1134 - val_accuracy: 0.9694
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1285 - accuracy: 0.9638
[CV] ..................................... n_neurons=80, total=  15.1s
[CV] n_neurons=80 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.3360 - accuracy: 0.9056 - val_loss: 0.1930 - val_accuracy: 0.9462
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1697 - accuracy: 0.9517 - val_loss: 0.1448 - val_accuracy: 0.9610
Epoch 3/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1237 - accuracy: 0.9640 - val_loss: 0.1284 - val_accuracy: 0.9652
Epoch 4/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0992 - accuracy: 0.9717 - val_loss: 0.1083 - val_accuracy: 0.9702
Epoch 5/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0827 - accuracy: 0.9754 - val_loss: 0.1223 - val_accuracy: 0.9692
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0692 - accuracy: 0.9797 - val_loss: 0.1078 - val_accuracy: 0.9700
Epoch 7/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0611 - accuracy: 0.9828 - val_loss: 0.1023 - val_accuracy: 0.9718
Epoch 8/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0518 - accuracy: 0.9852 - val_loss: 0.1105 - val_accuracy: 0.9724
Epoch 9/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0472 - accuracy: 0.9862 - val_loss: 0.1108 - val_accuracy: 0.9726
18333/18333 [==============================] - 1s 28us/sample - loss: 0.1254 - accuracy: 0.9660
[CV] ..................................... n_neurons=80, total=  16.6s
[CV] n_neurons=80 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.3408 - accuracy: 0.9041 - val_loss: 0.1900 - val_accuracy: 0.9442
Epoch 2/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1745 - accuracy: 0.9487 - val_loss: 0.1468 - val_accuracy: 0.9600
Epoch 3/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1304 - accuracy: 0.9616 - val_loss: 0.1209 - val_accuracy: 0.9670
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1049 - accuracy: 0.9689 - val_loss: 0.1204 - val_accuracy: 0.9660
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0888 - accuracy: 0.9747 - val_loss: 0.1167 - val_accuracy: 0.9682
Epoch 6/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0758 - accuracy: 0.9784 - val_loss: 0.1303 - val_accuracy: 0.9654
Epoch 7/30
36667/36667 [==============================] - 2s 43us/sample - loss: 0.0660 - accuracy: 0.9812 - val_loss: 0.1111 - val_accuracy: 0.9686
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0584 - accuracy: 0.9840 - val_loss: 0.1058 - val_accuracy: 0.9714
Epoch 9/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0519 - accuracy: 0.9856 - val_loss: 0.1197 - val_accuracy: 0.9698
Epoch 10/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0465 - accuracy: 0.9870 - val_loss: 0.1228 - val_accuracy: 0.9686
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1244 - accuracy: 0.9673
[CV] ..................................... n_neurons=80, total=  17.7s
[CV] n_neurons=81 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 57us/sample - loss: 0.3417 - accuracy: 0.9020 - val_loss: 0.1980 - val_accuracy: 0.9478
Epoch 2/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.1751 - accuracy: 0.9488 - val_loss: 0.1573 - val_accuracy: 0.9528
Epoch 3/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1279 - accuracy: 0.9627 - val_loss: 0.1269 - val_accuracy: 0.9632
Epoch 4/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.1024 - accuracy: 0.9702 - val_loss: 0.1299 - val_accuracy: 0.9628
Epoch 5/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0871 - accuracy: 0.9745 - val_loss: 0.1146 - val_accuracy: 0.9668
Epoch 6/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0739 - accuracy: 0.9786 - val_loss: 0.1146 - val_accuracy: 0.9680
Epoch 7/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0633 - accuracy: 0.9815 - val_loss: 0.1177 - val_accuracy: 0.9680
Epoch 8/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0565 - accuracy: 0.9842 - val_loss: 0.1181 - val_accuracy: 0.9658
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1315 - accuracy: 0.9646
[CV] ..................................... n_neurons=81, total=  14.9s
[CV] n_neurons=81 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.3361 - accuracy: 0.9068 - val_loss: 0.1895 - val_accuracy: 0.9458
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1680 - accuracy: 0.9517 - val_loss: 0.1444 - val_accuracy: 0.9590
Epoch 3/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1235 - accuracy: 0.9642 - val_loss: 0.1260 - val_accuracy: 0.9614
Epoch 4/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0990 - accuracy: 0.9707 - val_loss: 0.1117 - val_accuracy: 0.9660
Epoch 5/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0825 - accuracy: 0.9760 - val_loss: 0.1199 - val_accuracy: 0.9664
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0698 - accuracy: 0.9797 - val_loss: 0.1059 - val_accuracy: 0.9706
Epoch 7/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0604 - accuracy: 0.9815 - val_loss: 0.1080 - val_accuracy: 0.9696
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0520 - accuracy: 0.9853 - val_loss: 0.1105 - val_accuracy: 0.9694
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1230 - accuracy: 0.9668
[CV] ..................................... n_neurons=81, total=  14.1s
[CV] n_neurons=81 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 3s 68us/sample - loss: 0.3459 - accuracy: 0.9027 - val_loss: 0.1984 - val_accuracy: 0.9420
Epoch 2/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1766 - accuracy: 0.9479 - val_loss: 0.1509 - val_accuracy: 0.9562
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1298 - accuracy: 0.9615 - val_loss: 0.1260 - val_accuracy: 0.9644
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1047 - accuracy: 0.9693 - val_loss: 0.1229 - val_accuracy: 0.9636
Epoch 5/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0869 - accuracy: 0.9747 - val_loss: 0.1137 - val_accuracy: 0.9690
Epoch 6/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.0747 - accuracy: 0.9783 - val_loss: 0.1143 - val_accuracy: 0.9674
Epoch 7/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.0648 - accuracy: 0.9810 - val_loss: 0.1138 - val_accuracy: 0.9666
18333/18333 [==============================] - 1s 29us/sample - loss: 0.1266 - accuracy: 0.9661
[CV] ..................................... n_neurons=81, total=  15.0s
[CV] n_neurons=82 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 3s 87us/sample - loss: 0.3362 - accuracy: 0.9046 - val_loss: 0.2071 - val_accuracy: 0.9436
Epoch 2/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1721 - accuracy: 0.9503 - val_loss: 0.1462 - val_accuracy: 0.9578
Epoch 3/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.1289 - accuracy: 0.9629 - val_loss: 0.1263 - val_accuracy: 0.9628
Epoch 4/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.1033 - accuracy: 0.9698 - val_loss: 0.1218 - val_accuracy: 0.9662
Epoch 5/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0879 - accuracy: 0.9740 - val_loss: 0.1111 - val_accuracy: 0.9676
Epoch 6/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0740 - accuracy: 0.9786 - val_loss: 0.1159 - val_accuracy: 0.9680
Epoch 7/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0648 - accuracy: 0.9809 - val_loss: 0.1145 - val_accuracy: 0.9698
18334/18334 [==============================] - 0s 25us/sample - loss: 0.1264 - accuracy: 0.9654
[CV] ..................................... n_neurons=82, total=  15.0s
[CV] n_neurons=82 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.3357 - accuracy: 0.9069 - val_loss: 0.1921 - val_accuracy: 0.9448
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1709 - accuracy: 0.9506 - val_loss: 0.1451 - val_accuracy: 0.9596
Epoch 3/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.1270 - accuracy: 0.9640 - val_loss: 0.1252 - val_accuracy: 0.9630
Epoch 4/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1033 - accuracy: 0.9707 - val_loss: 0.1079 - val_accuracy: 0.9680
Epoch 5/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0866 - accuracy: 0.9749 - val_loss: 0.1216 - val_accuracy: 0.9664
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0737 - accuracy: 0.9786 - val_loss: 0.1060 - val_accuracy: 0.9690
Epoch 7/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0631 - accuracy: 0.9807 - val_loss: 0.1047 - val_accuracy: 0.9688
Epoch 8/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0552 - accuracy: 0.9843 - val_loss: 0.1032 - val_accuracy: 0.9716
Epoch 9/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0502 - accuracy: 0.9854 - val_loss: 0.1067 - val_accuracy: 0.9714
Epoch 10/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0448 - accuracy: 0.9866 - val_loss: 0.1095 - val_accuracy: 0.9712
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1250 - accuracy: 0.9685
[CV] ..................................... n_neurons=82, total=  18.1s
[CV] n_neurons=82 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.3412 - accuracy: 0.9035 - val_loss: 0.1950 - val_accuracy: 0.9424
Epoch 2/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1771 - accuracy: 0.9483 - val_loss: 0.1454 - val_accuracy: 0.9576
Epoch 3/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1313 - accuracy: 0.9620 - val_loss: 0.1226 - val_accuracy: 0.9644
Epoch 4/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1053 - accuracy: 0.9686 - val_loss: 0.1193 - val_accuracy: 0.9634
Epoch 5/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0878 - accuracy: 0.9747 - val_loss: 0.1097 - val_accuracy: 0.9684
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0741 - accuracy: 0.9779 - val_loss: 0.1137 - val_accuracy: 0.9670
Epoch 7/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0652 - accuracy: 0.9810 - val_loss: 0.1007 - val_accuracy: 0.9706
Epoch 8/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0565 - accuracy: 0.9842 - val_loss: 0.1028 - val_accuracy: 0.9712
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0504 - accuracy: 0.9856 - val_loss: 0.1036 - val_accuracy: 0.9702
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1204 - accuracy: 0.9662
[CV] ..................................... n_neurons=82, total=  17.5s
[CV] n_neurons=83 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.3335 - accuracy: 0.9059 - val_loss: 0.2012 - val_accuracy: 0.9454
Epoch 2/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1677 - accuracy: 0.9514 - val_loss: 0.1423 - val_accuracy: 0.9568
Epoch 3/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.1240 - accuracy: 0.9646 - val_loss: 0.1237 - val_accuracy: 0.9612
Epoch 4/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0999 - accuracy: 0.9701 - val_loss: 0.1216 - val_accuracy: 0.9636
Epoch 5/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0854 - accuracy: 0.9756 - val_loss: 0.1079 - val_accuracy: 0.9686
Epoch 6/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0729 - accuracy: 0.9791 - val_loss: 0.1116 - val_accuracy: 0.9692
Epoch 7/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0632 - accuracy: 0.9813 - val_loss: 0.1062 - val_accuracy: 0.9698
Epoch 8/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0560 - accuracy: 0.9836 - val_loss: 0.1164 - val_accuracy: 0.9674
Epoch 9/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0500 - accuracy: 0.9861 - val_loss: 0.1082 - val_accuracy: 0.9710
18334/18334 [==============================] - 1s 31us/sample - loss: 0.1307 - accuracy: 0.9661
[CV] ..................................... n_neurons=83, total=  16.1s
[CV] n_neurons=83 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.3308 - accuracy: 0.9071 - val_loss: 0.1972 - val_accuracy: 0.9416
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1724 - accuracy: 0.9503 - val_loss: 0.1455 - val_accuracy: 0.9592
Epoch 3/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1274 - accuracy: 0.9635 - val_loss: 0.1226 - val_accuracy: 0.9644
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1017 - accuracy: 0.9700 - val_loss: 0.1083 - val_accuracy: 0.9684
Epoch 5/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0846 - accuracy: 0.9748 - val_loss: 0.1166 - val_accuracy: 0.9654
Epoch 6/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0719 - accuracy: 0.9788 - val_loss: 0.1080 - val_accuracy: 0.9684
Epoch 7/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0618 - accuracy: 0.9819 - val_loss: 0.1027 - val_accuracy: 0.9686
Epoch 8/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0543 - accuracy: 0.9839 - val_loss: 0.1118 - val_accuracy: 0.9686
Epoch 9/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0487 - accuracy: 0.9860 - val_loss: 0.1072 - val_accuracy: 0.9714
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1342 - accuracy: 0.9646
[CV] ..................................... n_neurons=83, total=  16.7s
[CV] n_neurons=83 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.3367 - accuracy: 0.9045 - val_loss: 0.1868 - val_accuracy: 0.9462
Epoch 2/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1694 - accuracy: 0.9497 - val_loss: 0.1425 - val_accuracy: 0.9588
Epoch 3/30
36667/36667 [==============================] - 2s 67us/sample - loss: 0.1258 - accuracy: 0.9623 - val_loss: 0.1236 - val_accuracy: 0.9642
Epoch 4/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.1024 - accuracy: 0.9696 - val_loss: 0.1199 - val_accuracy: 0.9640
Epoch 5/30
36667/36667 [==============================] - 3s 75us/sample - loss: 0.0843 - accuracy: 0.9757 - val_loss: 0.1141 - val_accuracy: 0.9678
Epoch 6/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0737 - accuracy: 0.9793 - val_loss: 0.1190 - val_accuracy: 0.9660
Epoch 7/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.0640 - accuracy: 0.9816 - val_loss: 0.1031 - val_accuracy: 0.9698
Epoch 8/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0563 - accuracy: 0.9838 - val_loss: 0.1087 - val_accuracy: 0.9694
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0501 - accuracy: 0.9858 - val_loss: 0.1073 - val_accuracy: 0.9700
18333/18333 [==============================] - 1s 55us/sample - loss: 0.1305 - accuracy: 0.9665
[CV] ..................................... n_neurons=83, total=  19.9s
[CV] n_neurons=84 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 65us/sample - loss: 0.3369 - accuracy: 0.9045 - val_loss: 0.1970 - val_accuracy: 0.9484
Epoch 2/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.1720 - accuracy: 0.9496 - val_loss: 0.1482 - val_accuracy: 0.9572
Epoch 3/30
36666/36666 [==============================] - 3s 73us/sample - loss: 0.1256 - accuracy: 0.9635 - val_loss: 0.1216 - val_accuracy: 0.9640
Epoch 4/30
36666/36666 [==============================] - 3s 70us/sample - loss: 0.0993 - accuracy: 0.9705 - val_loss: 0.1177 - val_accuracy: 0.9674
Epoch 5/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.0825 - accuracy: 0.9759 - val_loss: 0.1058 - val_accuracy: 0.9712
Epoch 6/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0699 - accuracy: 0.9798 - val_loss: 0.1056 - val_accuracy: 0.9700
Epoch 7/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0598 - accuracy: 0.9832 - val_loss: 0.1115 - val_accuracy: 0.9710
Epoch 8/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0533 - accuracy: 0.9850 - val_loss: 0.1056 - val_accuracy: 0.9724
Epoch 9/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0468 - accuracy: 0.9875 - val_loss: 0.1081 - val_accuracy: 0.9722
Epoch 10/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0416 - accuracy: 0.9888 - val_loss: 0.1085 - val_accuracy: 0.9730
18334/18334 [==============================] - 0s 26us/sample - loss: 0.1265 - accuracy: 0.9679
[CV] ..................................... n_neurons=84, total=  21.5s
[CV] n_neurons=84 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.3371 - accuracy: 0.9066 - val_loss: 0.1912 - val_accuracy: 0.9444
Epoch 2/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.1759 - accuracy: 0.9498 - val_loss: 0.1455 - val_accuracy: 0.9598
Epoch 3/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.1315 - accuracy: 0.9627 - val_loss: 0.1289 - val_accuracy: 0.9618
Epoch 4/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1063 - accuracy: 0.9692 - val_loss: 0.1091 - val_accuracy: 0.9692
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0893 - accuracy: 0.9742 - val_loss: 0.1186 - val_accuracy: 0.9678
Epoch 6/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0755 - accuracy: 0.9782 - val_loss: 0.1093 - val_accuracy: 0.9696
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1370 - accuracy: 0.9603
[CV] ..................................... n_neurons=84, total=  12.2s
[CV] n_neurons=84 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.3412 - accuracy: 0.9046 - val_loss: 0.1940 - val_accuracy: 0.9442
Epoch 2/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1693 - accuracy: 0.9506 - val_loss: 0.1470 - val_accuracy: 0.9588
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1240 - accuracy: 0.9632 - val_loss: 0.1248 - val_accuracy: 0.9640
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1001 - accuracy: 0.9709 - val_loss: 0.1188 - val_accuracy: 0.9658
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0825 - accuracy: 0.9761 - val_loss: 0.1132 - val_accuracy: 0.9684
Epoch 6/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0701 - accuracy: 0.9802 - val_loss: 0.1154 - val_accuracy: 0.9692
Epoch 7/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0612 - accuracy: 0.9830 - val_loss: 0.1072 - val_accuracy: 0.9698
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0540 - accuracy: 0.9851 - val_loss: 0.1062 - val_accuracy: 0.9714
Epoch 9/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0475 - accuracy: 0.9872 - val_loss: 0.1088 - val_accuracy: 0.9716
Epoch 10/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0428 - accuracy: 0.9879 - val_loss: 0.1110 - val_accuracy: 0.9724
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1284 - accuracy: 0.9660
[CV] ..................................... n_neurons=84, total=  18.1s
[CV] n_neurons=85 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 56us/sample - loss: 0.3400 - accuracy: 0.9033 - val_loss: 0.2187 - val_accuracy: 0.9422
Epoch 2/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.1802 - accuracy: 0.9481 - val_loss: 0.1540 - val_accuracy: 0.9560
Epoch 3/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1313 - accuracy: 0.9612 - val_loss: 0.1252 - val_accuracy: 0.9644
Epoch 4/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1043 - accuracy: 0.9694 - val_loss: 0.1201 - val_accuracy: 0.9666
Epoch 5/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0878 - accuracy: 0.9748 - val_loss: 0.1118 - val_accuracy: 0.9700
Epoch 6/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0751 - accuracy: 0.9785 - val_loss: 0.1135 - val_accuracy: 0.9674
Epoch 7/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0648 - accuracy: 0.9814 - val_loss: 0.1085 - val_accuracy: 0.9696
Epoch 8/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0563 - accuracy: 0.9843 - val_loss: 0.1123 - val_accuracy: 0.9678
Epoch 9/30
36666/36666 [==============================] - 2s 67us/sample - loss: 0.0502 - accuracy: 0.9863 - val_loss: 0.1083 - val_accuracy: 0.9710
Epoch 10/30
36666/36666 [==============================] - 2s 64us/sample - loss: 0.0442 - accuracy: 0.9873 - val_loss: 0.1084 - val_accuracy: 0.9706
Epoch 11/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0397 - accuracy: 0.9890 - val_loss: 0.1110 - val_accuracy: 0.9718
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1295 - accuracy: 0.9689
[CV] ..................................... n_neurons=85, total=  21.7s
[CV] n_neurons=85 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.3423 - accuracy: 0.9042 - val_loss: 0.1993 - val_accuracy: 0.9398
Epoch 2/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1725 - accuracy: 0.9508 - val_loss: 0.1478 - val_accuracy: 0.9596
Epoch 3/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1267 - accuracy: 0.9634 - val_loss: 0.1253 - val_accuracy: 0.9642
Epoch 4/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0999 - accuracy: 0.9710 - val_loss: 0.1088 - val_accuracy: 0.9684
Epoch 5/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0824 - accuracy: 0.9762 - val_loss: 0.1140 - val_accuracy: 0.9676
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0681 - accuracy: 0.9807 - val_loss: 0.1052 - val_accuracy: 0.9730
Epoch 7/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0589 - accuracy: 0.9824 - val_loss: 0.1040 - val_accuracy: 0.9704
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0506 - accuracy: 0.9853 - val_loss: 0.1044 - val_accuracy: 0.9716
Epoch 9/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0449 - accuracy: 0.9874 - val_loss: 0.1010 - val_accuracy: 0.9728
Epoch 10/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0395 - accuracy: 0.9884 - val_loss: 0.1095 - val_accuracy: 0.9722
Epoch 11/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0351 - accuracy: 0.9896 - val_loss: 0.1021 - val_accuracy: 0.9732
18333/18333 [==============================] - 1s 31us/sample - loss: 0.1221 - accuracy: 0.9702
[CV] ..................................... n_neurons=85, total=  20.7s
[CV] n_neurons=85 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 66us/sample - loss: 0.3452 - accuracy: 0.9016 - val_loss: 0.1941 - val_accuracy: 0.9434
Epoch 2/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1776 - accuracy: 0.9470 - val_loss: 0.1443 - val_accuracy: 0.9596
Epoch 3/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1322 - accuracy: 0.9606 - val_loss: 0.1223 - val_accuracy: 0.9638
Epoch 4/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1073 - accuracy: 0.9688 - val_loss: 0.1172 - val_accuracy: 0.9632
Epoch 5/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0889 - accuracy: 0.9743 - val_loss: 0.1095 - val_accuracy: 0.9674
Epoch 6/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0767 - accuracy: 0.9778 - val_loss: 0.1187 - val_accuracy: 0.9660
Epoch 7/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0661 - accuracy: 0.9804 - val_loss: 0.1022 - val_accuracy: 0.9698
Epoch 8/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0576 - accuracy: 0.9831 - val_loss: 0.1037 - val_accuracy: 0.9708
Epoch 9/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0515 - accuracy: 0.9854 - val_loss: 0.1031 - val_accuracy: 0.9726
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1255 - accuracy: 0.9655
[CV] ..................................... n_neurons=85, total=  17.4s
[CV] n_neurons=86 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 3s 90us/sample - loss: 0.3382 - accuracy: 0.9038 - val_loss: 0.2035 - val_accuracy: 0.9456
Epoch 2/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1713 - accuracy: 0.9510 - val_loss: 0.1493 - val_accuracy: 0.9562
Epoch 3/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1268 - accuracy: 0.9626 - val_loss: 0.1243 - val_accuracy: 0.9640
Epoch 4/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1015 - accuracy: 0.9707 - val_loss: 0.1258 - val_accuracy: 0.9642
Epoch 5/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0858 - accuracy: 0.9752 - val_loss: 0.1093 - val_accuracy: 0.9712
Epoch 6/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0741 - accuracy: 0.9788 - val_loss: 0.1127 - val_accuracy: 0.9704
Epoch 7/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0643 - accuracy: 0.9818 - val_loss: 0.1183 - val_accuracy: 0.9684
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1263 - accuracy: 0.9644
[CV] ..................................... n_neurons=86, total=  14.5s
[CV] n_neurons=86 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.3346 - accuracy: 0.9058 - val_loss: 0.1921 - val_accuracy: 0.9466
Epoch 2/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1709 - accuracy: 0.9510 - val_loss: 0.1459 - val_accuracy: 0.9604
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1270 - accuracy: 0.9638 - val_loss: 0.1257 - val_accuracy: 0.9632
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1027 - accuracy: 0.9705 - val_loss: 0.1125 - val_accuracy: 0.9678
Epoch 5/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0857 - accuracy: 0.9753 - val_loss: 0.1184 - val_accuracy: 0.9682
Epoch 6/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0724 - accuracy: 0.9794 - val_loss: 0.1078 - val_accuracy: 0.9694
Epoch 7/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0630 - accuracy: 0.9814 - val_loss: 0.1096 - val_accuracy: 0.9694
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0545 - accuracy: 0.9847 - val_loss: 0.1094 - val_accuracy: 0.9682
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1228 - accuracy: 0.9665
[CV] ..................................... n_neurons=86, total=  14.6s
[CV] n_neurons=86 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3331 - accuracy: 0.9060 - val_loss: 0.1849 - val_accuracy: 0.9456
Epoch 2/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1702 - accuracy: 0.9500 - val_loss: 0.1412 - val_accuracy: 0.9602
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1265 - accuracy: 0.9626 - val_loss: 0.1199 - val_accuracy: 0.9654
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1012 - accuracy: 0.9708 - val_loss: 0.1163 - val_accuracy: 0.9670
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0839 - accuracy: 0.9753 - val_loss: 0.1116 - val_accuracy: 0.9700
Epoch 6/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0712 - accuracy: 0.9796 - val_loss: 0.1165 - val_accuracy: 0.9660
Epoch 7/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0608 - accuracy: 0.9828 - val_loss: 0.1040 - val_accuracy: 0.9700
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0524 - accuracy: 0.9848 - val_loss: 0.0983 - val_accuracy: 0.9730
Epoch 9/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0464 - accuracy: 0.9871 - val_loss: 0.1062 - val_accuracy: 0.9718
Epoch 10/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0413 - accuracy: 0.9882 - val_loss: 0.1092 - val_accuracy: 0.9720
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1212 - accuracy: 0.9687
[CV] ..................................... n_neurons=86, total=  18.2s
[CV] n_neurons=87 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.3278 - accuracy: 0.9073 - val_loss: 0.1877 - val_accuracy: 0.9514
Epoch 2/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1620 - accuracy: 0.9530 - val_loss: 0.1383 - val_accuracy: 0.9596
Epoch 3/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1199 - accuracy: 0.9656 - val_loss: 0.1181 - val_accuracy: 0.9652
Epoch 4/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0958 - accuracy: 0.9719 - val_loss: 0.1206 - val_accuracy: 0.9638
Epoch 5/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0818 - accuracy: 0.9766 - val_loss: 0.1075 - val_accuracy: 0.9704
Epoch 6/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0698 - accuracy: 0.9795 - val_loss: 0.1109 - val_accuracy: 0.9692
Epoch 7/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0612 - accuracy: 0.9824 - val_loss: 0.1133 - val_accuracy: 0.9688
18334/18334 [==============================] - 0s 25us/sample - loss: 0.1228 - accuracy: 0.9672
[CV] ..................................... n_neurons=87, total=  12.8s
[CV] n_neurons=87 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3372 - accuracy: 0.9052 - val_loss: 0.2042 - val_accuracy: 0.9392
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1700 - accuracy: 0.9510 - val_loss: 0.1414 - val_accuracy: 0.9596
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1243 - accuracy: 0.9641 - val_loss: 0.1212 - val_accuracy: 0.9624
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0998 - accuracy: 0.9708 - val_loss: 0.1039 - val_accuracy: 0.9692
Epoch 5/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0828 - accuracy: 0.9755 - val_loss: 0.1143 - val_accuracy: 0.9692
Epoch 6/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0697 - accuracy: 0.9803 - val_loss: 0.1083 - val_accuracy: 0.9684
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1264 - accuracy: 0.9650
[CV] ..................................... n_neurons=87, total=  11.0s
[CV] n_neurons=87 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3355 - accuracy: 0.9060 - val_loss: 0.1851 - val_accuracy: 0.9454
Epoch 2/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.1720 - accuracy: 0.9501 - val_loss: 0.1465 - val_accuracy: 0.9584
Epoch 3/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1291 - accuracy: 0.9622 - val_loss: 0.1219 - val_accuracy: 0.9648
Epoch 4/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1053 - accuracy: 0.9689 - val_loss: 0.1237 - val_accuracy: 0.9642
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0882 - accuracy: 0.9746 - val_loss: 0.1135 - val_accuracy: 0.9668
Epoch 6/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0752 - accuracy: 0.9783 - val_loss: 0.1177 - val_accuracy: 0.9670
Epoch 7/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0651 - accuracy: 0.9812 - val_loss: 0.1115 - val_accuracy: 0.9696
Epoch 8/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0572 - accuracy: 0.9841 - val_loss: 0.1047 - val_accuracy: 0.9722
Epoch 9/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0509 - accuracy: 0.9861 - val_loss: 0.1103 - val_accuracy: 0.9720
Epoch 10/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0448 - accuracy: 0.9874 - val_loss: 0.1217 - val_accuracy: 0.9684
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1338 - accuracy: 0.9660
[CV] ..................................... n_neurons=87, total=  18.6s
[CV] n_neurons=88 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.3288 - accuracy: 0.9081 - val_loss: 0.1970 - val_accuracy: 0.9484
Epoch 2/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.1656 - accuracy: 0.9505 - val_loss: 0.1505 - val_accuracy: 0.9576
Epoch 3/30
36666/36666 [==============================] - 2s 43us/sample - loss: 0.1226 - accuracy: 0.9639 - val_loss: 0.1207 - val_accuracy: 0.9646
Epoch 4/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0986 - accuracy: 0.9712 - val_loss: 0.1206 - val_accuracy: 0.9636
Epoch 5/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0840 - accuracy: 0.9758 - val_loss: 0.1073 - val_accuracy: 0.9696
Epoch 6/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0715 - accuracy: 0.9795 - val_loss: 0.1101 - val_accuracy: 0.9684
Epoch 7/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.0613 - accuracy: 0.9829 - val_loss: 0.1083 - val_accuracy: 0.9708
18334/18334 [==============================] - 1s 30us/sample - loss: 0.1250 - accuracy: 0.9648
[CV] ..................................... n_neurons=88, total=  13.0s
[CV] n_neurons=88 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 3s 72us/sample - loss: 0.3389 - accuracy: 0.9057 - val_loss: 0.1894 - val_accuracy: 0.9456
Epoch 2/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.1768 - accuracy: 0.9494 - val_loss: 0.1457 - val_accuracy: 0.9596
Epoch 3/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1314 - accuracy: 0.9623 - val_loss: 0.1335 - val_accuracy: 0.9608
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1051 - accuracy: 0.9695 - val_loss: 0.1177 - val_accuracy: 0.9670
Epoch 5/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0881 - accuracy: 0.9745 - val_loss: 0.1272 - val_accuracy: 0.9658
Epoch 6/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0738 - accuracy: 0.9781 - val_loss: 0.1142 - val_accuracy: 0.9700
Epoch 7/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0638 - accuracy: 0.9807 - val_loss: 0.1105 - val_accuracy: 0.9714
Epoch 8/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0550 - accuracy: 0.9846 - val_loss: 0.1128 - val_accuracy: 0.9700
Epoch 9/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0497 - accuracy: 0.9860 - val_loss: 0.1105 - val_accuracy: 0.9702
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1317 - accuracy: 0.9661
[CV] ..................................... n_neurons=88, total=  17.3s
[CV] n_neurons=88 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.3367 - accuracy: 0.9050 - val_loss: 0.1931 - val_accuracy: 0.9452
Epoch 2/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.1699 - accuracy: 0.9501 - val_loss: 0.1445 - val_accuracy: 0.9586
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1259 - accuracy: 0.9632 - val_loss: 0.1187 - val_accuracy: 0.9670
Epoch 4/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1000 - accuracy: 0.9708 - val_loss: 0.1187 - val_accuracy: 0.9662
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0836 - accuracy: 0.9760 - val_loss: 0.1126 - val_accuracy: 0.9686
Epoch 6/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0715 - accuracy: 0.9805 - val_loss: 0.1127 - val_accuracy: 0.9648
Epoch 7/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0607 - accuracy: 0.9827 - val_loss: 0.0990 - val_accuracy: 0.9710
Epoch 8/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0535 - accuracy: 0.9854 - val_loss: 0.1038 - val_accuracy: 0.9728
Epoch 9/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0468 - accuracy: 0.9876 - val_loss: 0.1120 - val_accuracy: 0.9690
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1194 - accuracy: 0.9681
[CV] ..................................... n_neurons=88, total=  16.7s
[CV] n_neurons=89 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 64us/sample - loss: 0.3272 - accuracy: 0.9077 - val_loss: 0.1913 - val_accuracy: 0.9480
Epoch 2/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1633 - accuracy: 0.9527 - val_loss: 0.1445 - val_accuracy: 0.9588
Epoch 3/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1199 - accuracy: 0.9653 - val_loss: 0.1226 - val_accuracy: 0.9646
Epoch 4/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0955 - accuracy: 0.9718 - val_loss: 0.1141 - val_accuracy: 0.9690
Epoch 5/30
36666/36666 [==============================] - 2s 68us/sample - loss: 0.0804 - accuracy: 0.9766 - val_loss: 0.1070 - val_accuracy: 0.9706
Epoch 6/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0685 - accuracy: 0.9804 - val_loss: 0.1098 - val_accuracy: 0.9714
Epoch 7/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0594 - accuracy: 0.9823 - val_loss: 0.1115 - val_accuracy: 0.9716
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1255 - accuracy: 0.9651
[CV] ..................................... n_neurons=89, total=  14.4s
[CV] n_neurons=89 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 3s 77us/sample - loss: 0.3350 - accuracy: 0.9059 - val_loss: 0.1911 - val_accuracy: 0.9444
Epoch 2/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1693 - accuracy: 0.9514 - val_loss: 0.1384 - val_accuracy: 0.9618
Epoch 3/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1244 - accuracy: 0.9640 - val_loss: 0.1226 - val_accuracy: 0.9632
Epoch 4/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.0999 - accuracy: 0.9710 - val_loss: 0.1100 - val_accuracy: 0.9676
Epoch 5/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0825 - accuracy: 0.9762 - val_loss: 0.1215 - val_accuracy: 0.9660
Epoch 6/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.0693 - accuracy: 0.9804 - val_loss: 0.1108 - val_accuracy: 0.9694
18333/18333 [==============================] - 1s 30us/sample - loss: 0.1275 - accuracy: 0.9643
[CV] ..................................... n_neurons=89, total=  13.6s
[CV] n_neurons=89 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.3392 - accuracy: 0.9046 - val_loss: 0.1896 - val_accuracy: 0.9426
Epoch 2/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1696 - accuracy: 0.9505 - val_loss: 0.1419 - val_accuracy: 0.9578
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1234 - accuracy: 0.9637 - val_loss: 0.1167 - val_accuracy: 0.9664
Epoch 4/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.0983 - accuracy: 0.9714 - val_loss: 0.1169 - val_accuracy: 0.9648
Epoch 5/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.0810 - accuracy: 0.9762 - val_loss: 0.1095 - val_accuracy: 0.9682
Epoch 6/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0687 - accuracy: 0.9801 - val_loss: 0.1134 - val_accuracy: 0.9698
Epoch 7/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0588 - accuracy: 0.9830 - val_loss: 0.0978 - val_accuracy: 0.9712
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0518 - accuracy: 0.9860 - val_loss: 0.1003 - val_accuracy: 0.9702
Epoch 9/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.0451 - accuracy: 0.9879 - val_loss: 0.1034 - val_accuracy: 0.9704
18333/18333 [==============================] - 0s 27us/sample - loss: 0.1232 - accuracy: 0.9674s - loss: 0.1207 - accuracy: 0.96
[CV] ..................................... n_neurons=89, total=  17.9s
[CV] n_neurons=90 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 56us/sample - loss: 0.3371 - accuracy: 0.9040 - val_loss: 0.1914 - val_accuracy: 0.9488
Epoch 2/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1652 - accuracy: 0.9513 - val_loss: 0.1471 - val_accuracy: 0.9582
Epoch 3/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1210 - accuracy: 0.9650 - val_loss: 0.1204 - val_accuracy: 0.9650
Epoch 4/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0966 - accuracy: 0.9717 - val_loss: 0.1184 - val_accuracy: 0.9652
Epoch 5/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0814 - accuracy: 0.9765 - val_loss: 0.1114 - val_accuracy: 0.9696
Epoch 6/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0696 - accuracy: 0.9804 - val_loss: 0.1150 - val_accuracy: 0.9688
Epoch 7/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.0599 - accuracy: 0.9825 - val_loss: 0.1182 - val_accuracy: 0.9702
18334/18334 [==============================] - 0s 27us/sample - loss: 0.1260 - accuracy: 0.9663
[CV] ..................................... n_neurons=90, total=  13.5s
[CV] n_neurons=90 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 66us/sample - loss: 0.3344 - accuracy: 0.9059 - val_loss: 0.1984 - val_accuracy: 0.9418
Epoch 2/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.1696 - accuracy: 0.9520 - val_loss: 0.1426 - val_accuracy: 0.9598
Epoch 3/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.1230 - accuracy: 0.9648 - val_loss: 0.1174 - val_accuracy: 0.9662
Epoch 4/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0981 - accuracy: 0.9719 - val_loss: 0.1037 - val_accuracy: 0.9692
Epoch 5/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0813 - accuracy: 0.9767 - val_loss: 0.1130 - val_accuracy: 0.9676
Epoch 6/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0679 - accuracy: 0.9806 - val_loss: 0.1027 - val_accuracy: 0.9712
Epoch 7/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0581 - accuracy: 0.9828 - val_loss: 0.0950 - val_accuracy: 0.9728
Epoch 8/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0497 - accuracy: 0.9852 - val_loss: 0.0983 - val_accuracy: 0.9732
Epoch 9/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0454 - accuracy: 0.9869 - val_loss: 0.0929 - val_accuracy: 0.9730
Epoch 10/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0396 - accuracy: 0.9882 - val_loss: 0.1009 - val_accuracy: 0.9730
Epoch 11/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0338 - accuracy: 0.9905 - val_loss: 0.1040 - val_accuracy: 0.9722
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1171 - accuracy: 0.9713
[CV] ..................................... n_neurons=90, total=  22.4s
[CV] n_neurons=90 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.3365 - accuracy: 0.9047 - val_loss: 0.1898 - val_accuracy: 0.9454
Epoch 2/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1732 - accuracy: 0.9494 - val_loss: 0.1499 - val_accuracy: 0.9562
Epoch 3/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1272 - accuracy: 0.9620 - val_loss: 0.1274 - val_accuracy: 0.9632
Epoch 4/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1019 - accuracy: 0.9705 - val_loss: 0.1169 - val_accuracy: 0.9644
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0847 - accuracy: 0.9758 - val_loss: 0.1130 - val_accuracy: 0.9676
Epoch 6/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0726 - accuracy: 0.9794 - val_loss: 0.1155 - val_accuracy: 0.9690
Epoch 7/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0617 - accuracy: 0.9824 - val_loss: 0.1028 - val_accuracy: 0.9694
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0544 - accuracy: 0.9846 - val_loss: 0.1059 - val_accuracy: 0.9698
Epoch 9/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0483 - accuracy: 0.9866 - val_loss: 0.1053 - val_accuracy: 0.9728
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1233 - accuracy: 0.9659
[CV] ..................................... n_neurons=90, total=  17.2s
[CV] n_neurons=91 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.3242 - accuracy: 0.9084 - val_loss: 0.1895 - val_accuracy: 0.9496
Epoch 2/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1643 - accuracy: 0.9529 - val_loss: 0.1409 - val_accuracy: 0.9580
Epoch 3/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1209 - accuracy: 0.9654 - val_loss: 0.1176 - val_accuracy: 0.9662
Epoch 4/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0962 - accuracy: 0.9726 - val_loss: 0.1203 - val_accuracy: 0.9658
Epoch 5/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0818 - accuracy: 0.9772 - val_loss: 0.1067 - val_accuracy: 0.9702
Epoch 6/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0700 - accuracy: 0.9797 - val_loss: 0.1054 - val_accuracy: 0.9718
Epoch 7/30
36666/36666 [==============================] - 2s 45us/sample - loss: 0.0599 - accuracy: 0.9829 - val_loss: 0.1086 - val_accuracy: 0.9718
Epoch 8/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0540 - accuracy: 0.9849 - val_loss: 0.1100 - val_accuracy: 0.9692
18334/18334 [==============================] - 0s 24us/sample - loss: 0.1267 - accuracy: 0.9668
[CV] ..................................... n_neurons=91, total=  14.5s
[CV] n_neurons=91 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3299 - accuracy: 0.9062 - val_loss: 0.1918 - val_accuracy: 0.9426
Epoch 2/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1702 - accuracy: 0.9511 - val_loss: 0.1414 - val_accuracy: 0.9582
Epoch 3/30
36667/36667 [==============================] - 3s 80us/sample - loss: 0.1249 - accuracy: 0.9635 - val_loss: 0.1201 - val_accuracy: 0.9644
Epoch 4/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1007 - accuracy: 0.9703 - val_loss: 0.1043 - val_accuracy: 0.9696
Epoch 5/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.0842 - accuracy: 0.9753 - val_loss: 0.1105 - val_accuracy: 0.9678
Epoch 6/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0702 - accuracy: 0.9799 - val_loss: 0.1029 - val_accuracy: 0.9696
Epoch 7/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.0612 - accuracy: 0.9818 - val_loss: 0.0957 - val_accuracy: 0.9706
Epoch 8/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0524 - accuracy: 0.9853 - val_loss: 0.0984 - val_accuracy: 0.9742
Epoch 9/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0466 - accuracy: 0.9867 - val_loss: 0.1001 - val_accuracy: 0.9726
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1287 - accuracy: 0.9656
[CV] ..................................... n_neurons=91, total=  18.9s
[CV] n_neurons=91 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.3332 - accuracy: 0.9059 - val_loss: 0.1821 - val_accuracy: 0.9484
Epoch 2/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1608 - accuracy: 0.9535 - val_loss: 0.1371 - val_accuracy: 0.9638
Epoch 3/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1170 - accuracy: 0.9656 - val_loss: 0.1112 - val_accuracy: 0.9692
Epoch 4/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0925 - accuracy: 0.9729 - val_loss: 0.1096 - val_accuracy: 0.9680
Epoch 5/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.0768 - accuracy: 0.9781 - val_loss: 0.1058 - val_accuracy: 0.9710
Epoch 6/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0660 - accuracy: 0.9815 - val_loss: 0.1033 - val_accuracy: 0.9710
Epoch 7/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0561 - accuracy: 0.9841 - val_loss: 0.0935 - val_accuracy: 0.9738
Epoch 8/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0495 - accuracy: 0.9863 - val_loss: 0.0956 - val_accuracy: 0.9736
Epoch 9/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0434 - accuracy: 0.9881 - val_loss: 0.1044 - val_accuracy: 0.9732
18333/18333 [==============================] - 0s 27us/sample - loss: 0.1134 - accuracy: 0.9694
[CV] ..................................... n_neurons=91, total=  17.5s
[CV] n_neurons=92 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 63us/sample - loss: 0.3257 - accuracy: 0.9078 - val_loss: 0.2029 - val_accuracy: 0.9476
Epoch 2/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1683 - accuracy: 0.9507 - val_loss: 0.1499 - val_accuracy: 0.9570
Epoch 3/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.1240 - accuracy: 0.9641 - val_loss: 0.1236 - val_accuracy: 0.9616
Epoch 4/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0991 - accuracy: 0.9709 - val_loss: 0.1228 - val_accuracy: 0.9626
Epoch 5/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0837 - accuracy: 0.9763 - val_loss: 0.1107 - val_accuracy: 0.9680
Epoch 6/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0708 - accuracy: 0.9791 - val_loss: 0.1124 - val_accuracy: 0.9676
Epoch 7/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0608 - accuracy: 0.9818 - val_loss: 0.1191 - val_accuracy: 0.9684
18334/18334 [==============================] - 1s 28us/sample - loss: 0.1272 - accuracy: 0.9653
[CV] ..................................... n_neurons=92, total=  14.1s
[CV] n_neurons=92 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 65us/sample - loss: 0.3364 - accuracy: 0.9055 - val_loss: 0.1888 - val_accuracy: 0.9450
Epoch 2/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1658 - accuracy: 0.9515 - val_loss: 0.1353 - val_accuracy: 0.9614
Epoch 3/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1212 - accuracy: 0.9649 - val_loss: 0.1194 - val_accuracy: 0.9626
Epoch 4/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0962 - accuracy: 0.9720 - val_loss: 0.0991 - val_accuracy: 0.9714
Epoch 5/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.0799 - accuracy: 0.9765 - val_loss: 0.1097 - val_accuracy: 0.9672
Epoch 6/30
36667/36667 [==============================] - 2s 66us/sample - loss: 0.0665 - accuracy: 0.9800 - val_loss: 0.0948 - val_accuracy: 0.9714
Epoch 7/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.0571 - accuracy: 0.9824 - val_loss: 0.0958 - val_accuracy: 0.9726
Epoch 8/30
36667/36667 [==============================] - 3s 70us/sample - loss: 0.0490 - accuracy: 0.9858 - val_loss: 0.1019 - val_accuracy: 0.9720
18333/18333 [==============================] - 1s 36us/sample - loss: 0.1141 - accuracy: 0.9688
[CV] ..................................... n_neurons=92, total=  17.8s
[CV] n_neurons=92 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 3s 87us/sample - loss: 0.3345 - accuracy: 0.9053 - val_loss: 0.1866 - val_accuracy: 0.9460
Epoch 2/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.1670 - accuracy: 0.9511 - val_loss: 0.1428 - val_accuracy: 0.9602
Epoch 3/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.1219 - accuracy: 0.9641 - val_loss: 0.1146 - val_accuracy: 0.9670
Epoch 4/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.0975 - accuracy: 0.9713 - val_loss: 0.1164 - val_accuracy: 0.9656
Epoch 5/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.0794 - accuracy: 0.9770 - val_loss: 0.1045 - val_accuracy: 0.9706
Epoch 6/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.0682 - accuracy: 0.9799 - val_loss: 0.1015 - val_accuracy: 0.9710
Epoch 7/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0970 - val_accuracy: 0.9714
Epoch 8/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.0504 - accuracy: 0.9858 - val_loss: 0.0941 - val_accuracy: 0.9750
Epoch 9/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.0444 - accuracy: 0.9874 - val_loss: 0.0971 - val_accuracy: 0.9740
Epoch 10/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0403 - accuracy: 0.9888 - val_loss: 0.1077 - val_accuracy: 0.9736
18333/18333 [==============================] - 0s 27us/sample - loss: 0.1205 - accuracy: 0.9685
[CV] ..................................... n_neurons=92, total=  22.6s
[CV] n_neurons=93 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 3s 72us/sample - loss: 0.3373 - accuracy: 0.9040 - val_loss: 0.2030 - val_accuracy: 0.9452
Epoch 2/30
36666/36666 [==============================] - 2s 58us/sample - loss: 0.1709 - accuracy: 0.9507 - val_loss: 0.1432 - val_accuracy: 0.9590
Epoch 3/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.1248 - accuracy: 0.9635 - val_loss: 0.1197 - val_accuracy: 0.9644
Epoch 4/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0984 - accuracy: 0.9713 - val_loss: 0.1160 - val_accuracy: 0.9640
Epoch 5/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0834 - accuracy: 0.9762 - val_loss: 0.1109 - val_accuracy: 0.9696
Epoch 6/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0708 - accuracy: 0.9790 - val_loss: 0.1109 - val_accuracy: 0.9698
Epoch 7/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0620 - accuracy: 0.9825 - val_loss: 0.1096 - val_accuracy: 0.9686
Epoch 8/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0539 - accuracy: 0.9848 - val_loss: 0.1074 - val_accuracy: 0.9712
Epoch 9/30
36666/36666 [==============================] - 2s 44us/sample - loss: 0.0476 - accuracy: 0.9866 - val_loss: 0.1077 - val_accuracy: 0.9712
Epoch 10/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0411 - accuracy: 0.9877 - val_loss: 0.1022 - val_accuracy: 0.9732
Epoch 11/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0370 - accuracy: 0.9895 - val_loss: 0.1060 - val_accuracy: 0.9734
Epoch 12/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0322 - accuracy: 0.9909 - val_loss: 0.1093 - val_accuracy: 0.9728
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1283 - accuracy: 0.9691
[CV] ..................................... n_neurons=93, total=  22.8s
[CV] n_neurons=93 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.3360 - accuracy: 0.9058 - val_loss: 0.1889 - val_accuracy: 0.9444
Epoch 2/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1668 - accuracy: 0.9518 - val_loss: 0.1417 - val_accuracy: 0.9604
Epoch 3/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1220 - accuracy: 0.9641 - val_loss: 0.1220 - val_accuracy: 0.9636
Epoch 4/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0967 - accuracy: 0.9725 - val_loss: 0.1038 - val_accuracy: 0.9720
Epoch 5/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0797 - accuracy: 0.9775 - val_loss: 0.1102 - val_accuracy: 0.9692
Epoch 6/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0660 - accuracy: 0.9808 - val_loss: 0.1093 - val_accuracy: 0.9720
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1243 - accuracy: 0.9659
[CV] ..................................... n_neurons=93, total=  12.3s
[CV] n_neurons=93 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 60us/sample - loss: 0.3343 - accuracy: 0.9048 - val_loss: 0.1899 - val_accuracy: 0.9436
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1671 - accuracy: 0.9505 - val_loss: 0.1396 - val_accuracy: 0.9592
Epoch 3/30
36667/36667 [==============================] - 3s 77us/sample - loss: 0.1203 - accuracy: 0.9649 - val_loss: 0.1144 - val_accuracy: 0.9666
Epoch 4/30
36667/36667 [==============================] - 4s 100us/sample - loss: 0.0952 - accuracy: 0.9723 - val_loss: 0.1061 - val_accuracy: 0.9684
Epoch 5/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0778 - accuracy: 0.9774 - val_loss: 0.1014 - val_accuracy: 0.9734
Epoch 6/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0667 - accuracy: 0.9807 - val_loss: 0.1014 - val_accuracy: 0.9706
Epoch 7/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0569 - accuracy: 0.9834 - val_loss: 0.0977 - val_accuracy: 0.9712
Epoch 8/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0498 - accuracy: 0.9858 - val_loss: 0.0963 - val_accuracy: 0.9728
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0439 - accuracy: 0.9879 - val_loss: 0.0981 - val_accuracy: 0.9730
Epoch 10/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0392 - accuracy: 0.9895 - val_loss: 0.1086 - val_accuracy: 0.9722
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1260 - accuracy: 0.9675
[CV] ..................................... n_neurons=93, total=  22.2s
[CV] n_neurons=94 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.3301 - accuracy: 0.9045 - val_loss: 0.1905 - val_accuracy: 0.9506
Epoch 2/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.1651 - accuracy: 0.9517 - val_loss: 0.1372 - val_accuracy: 0.9610
Epoch 3/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1194 - accuracy: 0.9651 - val_loss: 0.1174 - val_accuracy: 0.9666
Epoch 4/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0942 - accuracy: 0.9727 - val_loss: 0.1150 - val_accuracy: 0.9660
Epoch 5/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0788 - accuracy: 0.9768 - val_loss: 0.1081 - val_accuracy: 0.9706
Epoch 6/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0665 - accuracy: 0.9815 - val_loss: 0.1043 - val_accuracy: 0.9720
Epoch 7/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0564 - accuracy: 0.9840 - val_loss: 0.1110 - val_accuracy: 0.9718
Epoch 8/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0499 - accuracy: 0.9857 - val_loss: 0.1083 - val_accuracy: 0.9698
18334/18334 [==============================] - 0s 22us/sample - loss: 0.1259 - accuracy: 0.9667
[CV] ..................................... n_neurons=94, total=  14.4s
[CV] n_neurons=94 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.3330 - accuracy: 0.9075 - val_loss: 0.1858 - val_accuracy: 0.9450
Epoch 2/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.1648 - accuracy: 0.9521 - val_loss: 0.1383 - val_accuracy: 0.9612
Epoch 3/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1201 - accuracy: 0.9652 - val_loss: 0.1210 - val_accuracy: 0.9650
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0957 - accuracy: 0.9723 - val_loss: 0.1029 - val_accuracy: 0.9690
Epoch 5/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0786 - accuracy: 0.9766 - val_loss: 0.1234 - val_accuracy: 0.9658
Epoch 6/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0661 - accuracy: 0.9808 - val_loss: 0.1054 - val_accuracy: 0.9698
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1242 - accuracy: 0.9648
[CV] ..................................... n_neurons=94, total=  11.6s
[CV] n_neurons=94 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.3327 - accuracy: 0.9054 - val_loss: 0.1815 - val_accuracy: 0.9468
Epoch 2/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.1628 - accuracy: 0.9511 - val_loss: 0.1393 - val_accuracy: 0.9568
Epoch 3/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1191 - accuracy: 0.9645 - val_loss: 0.1167 - val_accuracy: 0.9676
Epoch 4/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.0956 - accuracy: 0.9722 - val_loss: 0.1107 - val_accuracy: 0.9672
Epoch 5/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0791 - accuracy: 0.9776 - val_loss: 0.1048 - val_accuracy: 0.9706
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0676 - accuracy: 0.9808 - val_loss: 0.1132 - val_accuracy: 0.9682
Epoch 7/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0577 - accuracy: 0.9836 - val_loss: 0.1039 - val_accuracy: 0.9702
Epoch 8/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0505 - accuracy: 0.9856 - val_loss: 0.1024 - val_accuracy: 0.9724
Epoch 9/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0444 - accuracy: 0.9880 - val_loss: 0.1084 - val_accuracy: 0.9708
Epoch 10/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0395 - accuracy: 0.9893 - val_loss: 0.1153 - val_accuracy: 0.9700
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1284 - accuracy: 0.9679
[CV] ..................................... n_neurons=94, total=  18.7s
[CV] n_neurons=95 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.3279 - accuracy: 0.9065 - val_loss: 0.1984 - val_accuracy: 0.9482
Epoch 2/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1646 - accuracy: 0.9521 - val_loss: 0.1509 - val_accuracy: 0.9572
Epoch 3/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.1205 - accuracy: 0.9651 - val_loss: 0.1226 - val_accuracy: 0.9658
Epoch 4/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0969 - accuracy: 0.9718 - val_loss: 0.1258 - val_accuracy: 0.9656
Epoch 5/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0821 - accuracy: 0.9767 - val_loss: 0.1112 - val_accuracy: 0.9704
Epoch 6/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0703 - accuracy: 0.9793 - val_loss: 0.1192 - val_accuracy: 0.9674
Epoch 7/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0604 - accuracy: 0.9825 - val_loss: 0.1140 - val_accuracy: 0.9704
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1276 - accuracy: 0.9658
[CV] ..................................... n_neurons=95, total=  13.3s
[CV] n_neurons=95 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 58us/sample - loss: 0.3263 - accuracy: 0.9084 - val_loss: 0.1871 - val_accuracy: 0.9444
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1642 - accuracy: 0.9530 - val_loss: 0.1358 - val_accuracy: 0.9602
Epoch 3/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1208 - accuracy: 0.9659 - val_loss: 0.1125 - val_accuracy: 0.9664
Epoch 4/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0968 - accuracy: 0.9717 - val_loss: 0.1000 - val_accuracy: 0.9690
Epoch 5/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0792 - accuracy: 0.9770 - val_loss: 0.1099 - val_accuracy: 0.9696
Epoch 6/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0664 - accuracy: 0.9807 - val_loss: 0.0947 - val_accuracy: 0.9728
Epoch 7/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0574 - accuracy: 0.9828 - val_loss: 0.0983 - val_accuracy: 0.9712
Epoch 8/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0489 - accuracy: 0.9860 - val_loss: 0.1004 - val_accuracy: 0.9716
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1209 - accuracy: 0.9668
[CV] ..................................... n_neurons=95, total=  15.8s
[CV] n_neurons=95 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 62us/sample - loss: 0.3284 - accuracy: 0.9074 - val_loss: 0.1768 - val_accuracy: 0.9484
Epoch 2/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.1578 - accuracy: 0.9540 - val_loss: 0.1366 - val_accuracy: 0.9586
Epoch 3/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1136 - accuracy: 0.9672 - val_loss: 0.1106 - val_accuracy: 0.9678
Epoch 4/30
36667/36667 [==============================] - 2s 63us/sample - loss: 0.0908 - accuracy: 0.9734 - val_loss: 0.1057 - val_accuracy: 0.9684
Epoch 5/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0740 - accuracy: 0.9784 - val_loss: 0.1042 - val_accuracy: 0.9698
Epoch 6/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0621 - accuracy: 0.9828 - val_loss: 0.1072 - val_accuracy: 0.9718
Epoch 7/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0541 - accuracy: 0.9849 - val_loss: 0.0938 - val_accuracy: 0.9728
Epoch 8/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0466 - accuracy: 0.9862 - val_loss: 0.0971 - val_accuracy: 0.9750
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0400 - accuracy: 0.9888 - val_loss: 0.1030 - val_accuracy: 0.9748
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1129 - accuracy: 0.9705s - loss: 0.1225 - accuracy: 
[CV] ..................................... n_neurons=95, total=  17.7s
[CV] n_neurons=96 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.3252 - accuracy: 0.9077 - val_loss: 0.1983 - val_accuracy: 0.9450
Epoch 2/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1652 - accuracy: 0.9517 - val_loss: 0.1394 - val_accuracy: 0.9598
Epoch 3/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1205 - accuracy: 0.9655 - val_loss: 0.1136 - val_accuracy: 0.9658
Epoch 4/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0963 - accuracy: 0.9725 - val_loss: 0.1091 - val_accuracy: 0.9686
Epoch 5/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0813 - accuracy: 0.9768 - val_loss: 0.0978 - val_accuracy: 0.9724
Epoch 6/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0680 - accuracy: 0.9799 - val_loss: 0.0994 - val_accuracy: 0.9710
Epoch 7/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0588 - accuracy: 0.9829 - val_loss: 0.0990 - val_accuracy: 0.9692
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1213 - accuracy: 0.9666
[CV] ..................................... n_neurons=96, total=  13.2s
[CV] n_neurons=96 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 61us/sample - loss: 0.3232 - accuracy: 0.9086 - val_loss: 0.1841 - val_accuracy: 0.9460
Epoch 2/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1620 - accuracy: 0.9532 - val_loss: 0.1386 - val_accuracy: 0.9608
Epoch 3/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1191 - accuracy: 0.9655 - val_loss: 0.1193 - val_accuracy: 0.9658
Epoch 4/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0946 - accuracy: 0.9733 - val_loss: 0.1022 - val_accuracy: 0.9708
Epoch 5/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0783 - accuracy: 0.9769 - val_loss: 0.1216 - val_accuracy: 0.9664
Epoch 6/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0658 - accuracy: 0.9808 - val_loss: 0.1078 - val_accuracy: 0.9716
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1248 - accuracy: 0.9655
[CV] ..................................... n_neurons=96, total=  11.6s
[CV] n_neurons=96 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.3334 - accuracy: 0.9054 - val_loss: 0.1864 - val_accuracy: 0.9464
Epoch 2/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1661 - accuracy: 0.9504 - val_loss: 0.1418 - val_accuracy: 0.9606
Epoch 3/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.1213 - accuracy: 0.9647 - val_loss: 0.1158 - val_accuracy: 0.9678
Epoch 4/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0974 - accuracy: 0.9710 - val_loss: 0.1194 - val_accuracy: 0.9666
Epoch 5/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0798 - accuracy: 0.9765 - val_loss: 0.1119 - val_accuracy: 0.9708
Epoch 6/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0676 - accuracy: 0.9806 - val_loss: 0.1144 - val_accuracy: 0.9700
Epoch 7/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0583 - accuracy: 0.9836 - val_loss: 0.1014 - val_accuracy: 0.9704
Epoch 8/30
36667/36667 [==============================] - 2s 45us/sample - loss: 0.0509 - accuracy: 0.9861 - val_loss: 0.1012 - val_accuracy: 0.9724
Epoch 9/30
36667/36667 [==============================] - 2s 44us/sample - loss: 0.0450 - accuracy: 0.9875 - val_loss: 0.1020 - val_accuracy: 0.9744
Epoch 10/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0395 - accuracy: 0.9885 - val_loss: 0.1118 - val_accuracy: 0.9706
18333/18333 [==============================] - 0s 22us/sample - loss: 0.1310 - accuracy: 0.9665
[CV] ..................................... n_neurons=96, total=  17.3s
[CV] n_neurons=97 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 54us/sample - loss: 0.3279 - accuracy: 0.9056 - val_loss: 0.1986 - val_accuracy: 0.9438
Epoch 2/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1647 - accuracy: 0.9521 - val_loss: 0.1426 - val_accuracy: 0.9576
Epoch 3/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1210 - accuracy: 0.9653 - val_loss: 0.1161 - val_accuracy: 0.9674
Epoch 4/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0959 - accuracy: 0.9717 - val_loss: 0.1187 - val_accuracy: 0.9670
Epoch 5/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0809 - accuracy: 0.9772 - val_loss: 0.1086 - val_accuracy: 0.9704
Epoch 6/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0683 - accuracy: 0.9808 - val_loss: 0.1074 - val_accuracy: 0.9698
Epoch 7/30
36666/36666 [==============================] - 2s 49us/sample - loss: 0.0588 - accuracy: 0.9832 - val_loss: 0.1142 - val_accuracy: 0.9696
Epoch 8/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0518 - accuracy: 0.9852 - val_loss: 0.1112 - val_accuracy: 0.9716
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1287 - accuracy: 0.9657
[CV] ..................................... n_neurons=97, total=  15.0s
[CV] n_neurons=97 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.3255 - accuracy: 0.9073 - val_loss: 0.1913 - val_accuracy: 0.9444
Epoch 2/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1676 - accuracy: 0.9516 - val_loss: 0.1441 - val_accuracy: 0.9594
Epoch 3/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.1231 - accuracy: 0.9642 - val_loss: 0.1287 - val_accuracy: 0.9604
Epoch 4/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0980 - accuracy: 0.9714 - val_loss: 0.1069 - val_accuracy: 0.9676
Epoch 5/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0806 - accuracy: 0.9766 - val_loss: 0.1222 - val_accuracy: 0.9670
Epoch 6/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0675 - accuracy: 0.9795 - val_loss: 0.1091 - val_accuracy: 0.9688
18333/18333 [==============================] - 0s 23us/sample - loss: 0.1276 - accuracy: 0.9654
[CV] ..................................... n_neurons=97, total=  11.8s
[CV] n_neurons=97 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 54us/sample - loss: 0.3336 - accuracy: 0.9066 - val_loss: 0.1838 - val_accuracy: 0.9448
Epoch 2/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1656 - accuracy: 0.9515 - val_loss: 0.1386 - val_accuracy: 0.9592
Epoch 3/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1200 - accuracy: 0.9638 - val_loss: 0.1139 - val_accuracy: 0.9664
Epoch 4/30
36667/36667 [==============================] - 2s 46us/sample - loss: 0.0954 - accuracy: 0.9716 - val_loss: 0.1105 - val_accuracy: 0.9662
Epoch 5/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0780 - accuracy: 0.9776 - val_loss: 0.1075 - val_accuracy: 0.9700
Epoch 6/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0659 - accuracy: 0.9815 - val_loss: 0.1075 - val_accuracy: 0.9692
Epoch 7/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0568 - accuracy: 0.9831 - val_loss: 0.0939 - val_accuracy: 0.9724
Epoch 8/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0491 - accuracy: 0.9859 - val_loss: 0.0979 - val_accuracy: 0.9722
Epoch 9/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0427 - accuracy: 0.9880 - val_loss: 0.1003 - val_accuracy: 0.9724
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1170 - accuracy: 0.9674
[CV] ..................................... n_neurons=97, total=  17.0s
[CV] n_neurons=98 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.3254 - accuracy: 0.9075 - val_loss: 0.1984 - val_accuracy: 0.9452
Epoch 2/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1640 - accuracy: 0.9526 - val_loss: 0.1480 - val_accuracy: 0.9534
Epoch 3/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.1215 - accuracy: 0.9654 - val_loss: 0.1197 - val_accuracy: 0.9632
Epoch 4/30
36666/36666 [==============================] - 2s 51us/sample - loss: 0.0963 - accuracy: 0.9716 - val_loss: 0.1176 - val_accuracy: 0.9636
Epoch 5/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.0808 - accuracy: 0.9776 - val_loss: 0.1065 - val_accuracy: 0.9702
Epoch 6/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.0679 - accuracy: 0.9799 - val_loss: 0.1087 - val_accuracy: 0.9708
Epoch 7/30
36666/36666 [==============================] - 2s 46us/sample - loss: 0.0580 - accuracy: 0.9834 - val_loss: 0.1128 - val_accuracy: 0.9682
18334/18334 [==============================] - 0s 23us/sample - loss: 0.1260 - accuracy: 0.9645
[CV] ..................................... n_neurons=98, total=  13.4s
[CV] n_neurons=98 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.3260 - accuracy: 0.9081 - val_loss: 0.1773 - val_accuracy: 0.9482
Epoch 2/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.1620 - accuracy: 0.9535 - val_loss: 0.1371 - val_accuracy: 0.9620
Epoch 3/30
36667/36667 [==============================] - 2s 63us/sample - loss: 0.1196 - accuracy: 0.9653 - val_loss: 0.1193 - val_accuracy: 0.9654
Epoch 4/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0959 - accuracy: 0.9715 - val_loss: 0.1081 - val_accuracy: 0.9686
Epoch 5/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0794 - accuracy: 0.9776 - val_loss: 0.1227 - val_accuracy: 0.9650
Epoch 6/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0659 - accuracy: 0.9810 - val_loss: 0.1068 - val_accuracy: 0.9702
Epoch 7/30
36667/36667 [==============================] - 2s 66us/sample - loss: 0.0564 - accuracy: 0.9835 - val_loss: 0.1049 - val_accuracy: 0.9700
Epoch 8/30
36667/36667 [==============================] - 3s 69us/sample - loss: 0.0491 - accuracy: 0.9861 - val_loss: 0.1124 - val_accuracy: 0.9704
Epoch 9/30
36667/36667 [==============================] - 3s 82us/sample - loss: 0.0429 - accuracy: 0.9877 - val_loss: 0.1102 - val_accuracy: 0.9704
18333/18333 [==============================] - 1s 37us/sample - loss: 0.1305 - accuracy: 0.9671
[CV] ..................................... n_neurons=98, total=  20.7s
[CV] n_neurons=98 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 3s 74us/sample - loss: 0.3338 - accuracy: 0.9053 - val_loss: 0.1849 - val_accuracy: 0.9484
Epoch 2/30
36667/36667 [==============================] - 2s 64us/sample - loss: 0.1668 - accuracy: 0.9510 - val_loss: 0.1382 - val_accuracy: 0.9594
Epoch 3/30
36667/36667 [==============================] - 3s 73us/sample - loss: 0.1212 - accuracy: 0.9643 - val_loss: 0.1107 - val_accuracy: 0.9702
Epoch 4/30
36667/36667 [==============================] - 2s 67us/sample - loss: 0.0972 - accuracy: 0.9711 - val_loss: 0.1159 - val_accuracy: 0.9652
Epoch 5/30
36667/36667 [==============================] - 3s 70us/sample - loss: 0.0810 - accuracy: 0.9764 - val_loss: 0.0993 - val_accuracy: 0.9716
Epoch 6/30
36667/36667 [==============================] - 2s 63us/sample - loss: 0.0685 - accuracy: 0.9808 - val_loss: 0.1020 - val_accuracy: 0.9724
Epoch 7/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0587 - accuracy: 0.9839 - val_loss: 0.0958 - val_accuracy: 0.9726
Epoch 8/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.0513 - accuracy: 0.9861 - val_loss: 0.0963 - val_accuracy: 0.9738
Epoch 9/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0457 - accuracy: 0.9873 - val_loss: 0.0972 - val_accuracy: 0.9750
18333/18333 [==============================] - 0s 25us/sample - loss: 0.1211 - accuracy: 0.9669
[CV] ..................................... n_neurons=98, total=  21.8s
[CV] n_neurons=99 ....................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.3154 - accuracy: 0.9096 - val_loss: 0.1924 - val_accuracy: 0.9502
Epoch 2/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.1550 - accuracy: 0.9542 - val_loss: 0.1332 - val_accuracy: 0.9618
Epoch 3/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.1121 - accuracy: 0.9673 - val_loss: 0.1076 - val_accuracy: 0.9706
Epoch 4/30
36666/36666 [==============================] - 2s 64us/sample - loss: 0.0889 - accuracy: 0.9741 - val_loss: 0.1129 - val_accuracy: 0.9662
Epoch 5/30
36666/36666 [==============================] - 3s 85us/sample - loss: 0.0745 - accuracy: 0.9787 - val_loss: 0.1005 - val_accuracy: 0.9722
Epoch 6/30
36666/36666 [==============================] - 3s 69us/sample - loss: 0.0625 - accuracy: 0.9816 - val_loss: 0.0992 - val_accuracy: 0.9728
Epoch 7/30
36666/36666 [==============================] - 2s 58us/sample - loss: 0.0540 - accuracy: 0.9843 - val_loss: 0.1072 - val_accuracy: 0.9720
Epoch 8/30
36666/36666 [==============================] - 2s 59us/sample - loss: 0.0475 - accuracy: 0.9867 - val_loss: 0.0982 - val_accuracy: 0.9752
Epoch 9/30
36666/36666 [==============================] - 2s 64us/sample - loss: 0.0417 - accuracy: 0.9878 - val_loss: 0.0990 - val_accuracy: 0.9752
Epoch 10/30
36666/36666 [==============================] - 2s 57us/sample - loss: 0.0354 - accuracy: 0.9896 - val_loss: 0.0972 - val_accuracy: 0.9738
Epoch 11/30
36666/36666 [==============================] - 2s 55us/sample - loss: 0.0311 - accuracy: 0.9913 - val_loss: 0.1034 - val_accuracy: 0.9754
Epoch 12/30
36666/36666 [==============================] - 2s 61us/sample - loss: 0.0272 - accuracy: 0.9925 - val_loss: 0.1001 - val_accuracy: 0.9754
18334/18334 [==============================] - 1s 30us/sample - loss: 0.1250 - accuracy: 0.9698
[CV] ..................................... n_neurons=99, total=  27.8s
[CV] n_neurons=99 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 66us/sample - loss: 0.3213 - accuracy: 0.9108 - val_loss: 0.1803 - val_accuracy: 0.9474
Epoch 2/30
36667/36667 [==============================] - 2s 57us/sample - loss: 0.1609 - accuracy: 0.9536 - val_loss: 0.1326 - val_accuracy: 0.9628
Epoch 3/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1193 - accuracy: 0.9655 - val_loss: 0.1162 - val_accuracy: 0.9652
Epoch 4/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0964 - accuracy: 0.9717 - val_loss: 0.1084 - val_accuracy: 0.9686
Epoch 5/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0806 - accuracy: 0.9762 - val_loss: 0.1166 - val_accuracy: 0.9690
Epoch 6/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0678 - accuracy: 0.9797 - val_loss: 0.1082 - val_accuracy: 0.9700
Epoch 7/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0580 - accuracy: 0.9827 - val_loss: 0.1084 - val_accuracy: 0.9690
Epoch 8/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.0504 - accuracy: 0.9856 - val_loss: 0.1094 - val_accuracy: 0.9722
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1298 - accuracy: 0.9652
[CV] ..................................... n_neurons=99, total=  15.8s
[CV] n_neurons=99 ....................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 56us/sample - loss: 0.3286 - accuracy: 0.9059 - val_loss: 0.1901 - val_accuracy: 0.9458
Epoch 2/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.1636 - accuracy: 0.9517 - val_loss: 0.1378 - val_accuracy: 0.9602
Epoch 3/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.1187 - accuracy: 0.9648 - val_loss: 0.1178 - val_accuracy: 0.9668
Epoch 4/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0949 - accuracy: 0.9719 - val_loss: 0.1257 - val_accuracy: 0.9630
Epoch 5/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0777 - accuracy: 0.9784 - val_loss: 0.1107 - val_accuracy: 0.9696
Epoch 6/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0657 - accuracy: 0.9811 - val_loss: 0.1102 - val_accuracy: 0.9702
Epoch 7/30
36667/36667 [==============================] - 2s 47us/sample - loss: 0.0556 - accuracy: 0.9840 - val_loss: 0.0979 - val_accuracy: 0.9706
Epoch 8/30
36667/36667 [==============================] - 2s 48us/sample - loss: 0.0483 - accuracy: 0.9863 - val_loss: 0.1030 - val_accuracy: 0.9730
Epoch 9/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0419 - accuracy: 0.9884 - val_loss: 0.1073 - val_accuracy: 0.9724
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1213 - accuracy: 0.9674
[CV] ..................................... n_neurons=99, total=  16.7s
[CV] n_neurons=100 ...................................................
Train on 36666 samples, validate on 5000 samples
Epoch 1/30
36666/36666 [==============================] - 2s 56us/sample - loss: 0.3270 - accuracy: 0.9074 - val_loss: 0.1970 - val_accuracy: 0.9476
Epoch 2/30
36666/36666 [==============================] - 2s 48us/sample - loss: 0.1633 - accuracy: 0.9523 - val_loss: 0.1427 - val_accuracy: 0.9586
Epoch 3/30
36666/36666 [==============================] - 2s 47us/sample - loss: 0.1183 - accuracy: 0.9656 - val_loss: 0.1185 - val_accuracy: 0.9646
Epoch 4/30
36666/36666 [==============================] - 2s 50us/sample - loss: 0.0932 - accuracy: 0.9731 - val_loss: 0.1145 - val_accuracy: 0.9658
Epoch 5/30
36666/36666 [==============================] - 2s 65us/sample - loss: 0.0786 - accuracy: 0.9770 - val_loss: 0.1068 - val_accuracy: 0.9702
Epoch 6/30
36666/36666 [==============================] - 3s 74us/sample - loss: 0.0662 - accuracy: 0.9803 - val_loss: 0.1117 - val_accuracy: 0.9676
Epoch 7/30
36666/36666 [==============================] - 2s 57us/sample - loss: 0.0569 - accuracy: 0.9833 - val_loss: 0.1048 - val_accuracy: 0.9718
Epoch 8/30
36666/36666 [==============================] - 2s 53us/sample - loss: 0.0498 - accuracy: 0.9854 - val_loss: 0.1103 - val_accuracy: 0.9694
Epoch 9/30
36666/36666 [==============================] - 2s 52us/sample - loss: 0.0432 - accuracy: 0.9878 - val_loss: 0.1049 - val_accuracy: 0.9706
18334/18334 [==============================] - 2s 83us/sample - loss: 0.1236 - accuracy: 0.9681
[CV] .................................... n_neurons=100, total=  20.1s
[CV] n_neurons=100 ...................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 63us/sample - loss: 0.3258 - accuracy: 0.9079 - val_loss: 0.1829 - val_accuracy: 0.9454
Epoch 2/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.1622 - accuracy: 0.9541 - val_loss: 0.1316 - val_accuracy: 0.9634
Epoch 3/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.1185 - accuracy: 0.9657 - val_loss: 0.1142 - val_accuracy: 0.9654
Epoch 4/30
36667/36667 [==============================] - 2s 55us/sample - loss: 0.0944 - accuracy: 0.9721 - val_loss: 0.0987 - val_accuracy: 0.9696
Epoch 5/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0773 - accuracy: 0.9774 - val_loss: 0.1109 - val_accuracy: 0.9684
Epoch 6/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.0636 - accuracy: 0.9811 - val_loss: 0.1002 - val_accuracy: 0.9712
18333/18333 [==============================] - 0s 26us/sample - loss: 0.1229 - accuracy: 0.9650
[CV] .................................... n_neurons=100, total=  12.6s
[CV] n_neurons=100 ...................................................
Train on 36667 samples, validate on 5000 samples
Epoch 1/30
36667/36667 [==============================] - 2s 59us/sample - loss: 0.3278 - accuracy: 0.9081 - val_loss: 0.1851 - val_accuracy: 0.9496
Epoch 2/30
36667/36667 [==============================] - 2s 49us/sample - loss: 0.1665 - accuracy: 0.9516 - val_loss: 0.1432 - val_accuracy: 0.9572
Epoch 3/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.1222 - accuracy: 0.9645 - val_loss: 0.1188 - val_accuracy: 0.9678
Epoch 4/30
36667/36667 [==============================] - 2s 51us/sample - loss: 0.0982 - accuracy: 0.9724 - val_loss: 0.1140 - val_accuracy: 0.9676
Epoch 5/30
36667/36667 [==============================] - 2s 52us/sample - loss: 0.0807 - accuracy: 0.9772 - val_loss: 0.1008 - val_accuracy: 0.9722
Epoch 6/30
36667/36667 [==============================] - 2s 53us/sample - loss: 0.0686 - accuracy: 0.9802 - val_loss: 0.1150 - val_accuracy: 0.9670
Epoch 7/30
36667/36667 [==============================] - 2s 50us/sample - loss: 0.0596 - accuracy: 0.9831 - val_loss: 0.1027 - val_accuracy: 0.9706
18333/18333 [==============================] - 0s 24us/sample - loss: 0.1286 - accuracy: 0.9653
[CV] .................................... n_neurons=100, total=  14.1s
Train on 55000 samples, validate on 5000 samples
Epoch 1/30
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed: 93.6min finished
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>55000/55000 [==============================] - 3s 50us/sample - loss: 0.2915 - accuracy: 0.9174 - val_loss: 0.1630 - val_accuracy: 0.9546
Epoch 2/30
55000/55000 [==============================] - 2s 43us/sample - loss: 0.1436 - accuracy: 0.9585 - val_loss: 0.1196 - val_accuracy: 0.9674
Epoch 3/30
55000/55000 [==============================] - 2s 44us/sample - loss: 0.1079 - accuracy: 0.9685 - val_loss: 0.1014 - val_accuracy: 0.9706
Epoch 4/30
55000/55000 [==============================] - 3s 50us/sample - loss: 0.0887 - accuracy: 0.9747 - val_loss: 0.0975 - val_accuracy: 0.9726
Epoch 5/30
55000/55000 [==============================] - 3s 46us/sample - loss: 0.0760 - accuracy: 0.9779 - val_loss: 0.0864 - val_accuracy: 0.9758
Epoch 6/30
55000/55000 [==============================] - 2s 44us/sample - loss: 0.0661 - accuracy: 0.9811 - val_loss: 0.0823 - val_accuracy: 0.9774
Epoch 7/30
55000/55000 [==============================] - 2s 44us/sample - loss: 0.0599 - accuracy: 0.9837 - val_loss: 0.0872 - val_accuracy: 0.9782
Epoch 8/30
55000/55000 [==============================] - 3s 54us/sample - loss: 0.0538 - accuracy: 0.9851 - val_loss: 0.0874 - val_accuracy: 0.9762
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[11]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>GridSearchCV(cv=3, error_score=nan,
             estimator=&lt;tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x63225cb50&gt;,
             iid=&#39;deprecated&#39;, n_jobs=None,
             param_grid={&#39;n_neurons&#39;: range(1, 101)}, pre_dispatch=&#39;2*n_jobs&#39;,
             refit=True, return_train_score=False, scoring=None, verbose=2)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Inspecting-the-best-estimator">Inspecting the best estimator<a class="anchor-link" href="#Inspecting-the-best-estimator">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's get the number of neurons that produced the best estimator</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">grid_cv</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[12]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;n_neurons&#39;: 85}</pre>
</div>

</div>

</div>
</div>

</div>Let's get the results to compare the performance of the 100 models.
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">grid_cv</span><span class="o">.</span><span class="n">cv_results_</span>
<span class="n">results</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[13]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;mean_fit_time&#39;: array([ 33.85127203,  24.300723  ,  21.0583969 ,  22.98531063,
         16.42428398,  18.93437139,  12.58610161,  16.98673439,
         12.61866204,  13.65373278,  13.51055614,  13.11565145,
         19.40056364,  17.67243409,  14.37399356,  14.33542069,
         16.62681937,  13.93143868,  17.654447  ,  17.3354423 ,
         15.40284363,  16.5213263 ,  17.17703191,  15.40601937,
         16.59257174,  15.19740232,  16.57461826,  16.86535501,
         18.36507225,  15.82775029,  16.91057833,  17.25118359,
         16.28343376,  15.86417905,  13.9347055 ,  17.32595491,
         15.17376041,  15.52343726,  17.25958435,  16.30760105,
         16.98318108,  18.58442362,  15.83878732,  16.83948302,
         15.21975859,  14.02860109,  15.49221301,  19.68309339,
         17.60358294,  14.7856493 ,  17.07676895,  14.85290567,
         17.2955157 , 113.63663967,  67.9901224 ,  22.592273  ,
         17.58277734,  16.35300366,  20.87893653,  19.8053538 ,
         18.59487931,  16.83426364,  16.30633005,  16.21230443,
         16.42265073,  13.91606967,  13.69098441,  17.74693767,
         16.23989789,  13.62966967,  19.02753735,  18.36217848,
         19.05521401,  18.13494595,  16.19658597,  15.61588693,
         14.68797867,  18.7519606 ,  16.68433897,  15.9854823 ,
         14.17480262,  16.39653055,  16.88446522,  16.7556146 ,
         19.41490173,  15.29065625,  13.70199426,  15.16208235,
         14.75385992,  17.19533587,  16.48121842,  17.57935731,
         18.65696462,  14.44377534,  15.13941161,  13.63276601,
         14.1140391 ,  18.04609426,  19.63797466,  14.77009964]),
 &#39;std_fit_time&#39;: array([  3.53361517,   1.94239253,   1.49269416,   8.36362696,
          4.59094198,   4.52073541,   0.60079967,   5.15441149,
          1.1538155 ,   1.4965692 ,   1.56746836,   1.71395789,
          3.8296373 ,   0.91231575,   1.16505385,   1.33843799,
          3.72785631,   0.79152007,   3.99074462,   4.14526646,
          3.76708724,   3.71357246,   1.18961716,   2.91267905,
          1.16182461,   2.51350821,   1.668494  ,   3.06064002,
          3.41710593,   0.49023444,   1.56866948,   3.81590929,
          0.71466443,   0.42213739,   1.25152278,   2.01685773,
          1.15689779,   2.20789881,   1.19849482,   1.79232229,
          2.48616636,   1.86943674,   1.95474644,   0.30970099,
          2.09248229,   2.13328446,   1.48194884,   1.92859357,
          0.81368775,   1.2919702 ,   1.17226707,   1.72012883,
          1.25603054, 138.19817507,  64.29665412,   1.25106991,
          1.86667529,   3.01211367,   5.08063051,   2.65281524,
          2.94705888,   1.41259418,   1.69271639,   1.62229665,
          1.40863218,   5.28597572,   0.68002782,   1.00373256,
          1.10637579,   2.72734372,   2.56177264,   2.30206821,
          3.32071209,   2.33600111,   2.39610477,   3.7162261 ,
          3.00964133,   2.00211526,   2.04060869,   1.04265593,
          0.39578699,   1.37831982,   1.41891858,   3.84340493,
          1.83797897,   1.72960973,   3.27574311,   1.97096955,
          1.8599815 ,   3.67935876,   1.84233921,   3.4837449 ,
          4.81534048,   2.95173696,   1.77722631,   2.423957  ,
          2.14007355,   3.67177696,   5.3968244 ,   2.77254981]),
 &#39;mean_score_time&#39;: array([0.40829404, 0.41398199, 0.406521  , 0.41109236, 0.40033491,
        0.44207605, 0.4073654 , 0.41235312, 0.41448704, 0.41548856,
        0.42246652, 0.41995899, 0.43607418, 0.41969419, 0.43075172,
        0.42059143, 0.41846546, 0.41133467, 0.41284204, 0.40960463,
        0.43195033, 0.4101007 , 0.40123804, 0.40911865, 0.42168037,
        0.43129794, 0.53997207, 0.42753291, 0.4651498 , 0.42657439,
        0.42364931, 0.4537464 , 0.46404529, 0.43526109, 0.44047713,
        0.44082403, 0.42636983, 0.42881934, 0.4362456 , 0.41920567,
        0.42636418, 0.43316285, 0.43724012, 0.43765775, 0.4669834 ,
        0.45119492, 0.45639133, 0.47246337, 0.45078437, 0.45716476,
        0.44505501, 0.46169138, 0.49394798, 0.57064422, 1.0982999 ,
        0.54367995, 0.47231992, 0.47358139, 0.55792642, 0.60676185,
        0.47557545, 0.44933335, 0.42903733, 0.68234817, 0.42152834,
        0.44218636, 0.42060653, 0.79451934, 0.44703945, 0.43696658,
        0.59305994, 0.48270186, 0.58929404, 0.51552312, 0.46562672,
        0.4998304 , 0.5574472 , 0.47385534, 0.53406636, 0.47277292,
        0.4924678 , 0.47450177, 0.68236605, 0.47653413, 0.49130893,
        0.47151009, 0.45108883, 0.48544502, 0.52879794, 0.47854908,
        0.47518571, 0.57293963, 0.46584837, 0.47992341, 0.45786548,
        0.42615867, 0.45753503, 0.56476212, 0.49484293, 0.83663996]),
 &#39;std_score_time&#39;: array([0.01209661, 0.00692188, 0.01249815, 0.00817917, 0.0055548 ,
        0.0606841 , 0.00537203, 0.00795588, 0.00480269, 0.018045  ,
        0.01737434, 0.01417935, 0.03061117, 0.01432099, 0.01243306,
        0.01350499, 0.01620816, 0.00243286, 0.0091    , 0.01361717,
        0.0325911 , 0.00852538, 0.00202833, 0.00702423, 0.01425232,
        0.0071444 , 0.18814633, 0.01576784, 0.03712978, 0.02232218,
        0.02205068, 0.01910499, 0.01617372, 0.00224678, 0.03266342,
        0.01842802, 0.0109737 , 0.01105157, 0.00517025, 0.00759368,
        0.01194349, 0.0142576 , 0.01088892, 0.01490304, 0.04190918,
        0.01679165, 0.0088882 , 0.01694092, 0.00761896, 0.01402644,
        0.01164981, 0.02927897, 0.0344043 , 0.09790802, 0.77203275,
        0.03462655, 0.01614592, 0.02105474, 0.08902748, 0.09988824,
        0.00851301, 0.00751263, 0.0115552 , 0.26421882, 0.00115486,
        0.01309084, 0.0013724 , 0.44871828, 0.01614917, 0.00852296,
        0.0985885 , 0.03828361, 0.07148289, 0.02584369, 0.01111038,
        0.03763938, 0.05714398, 0.02988051, 0.07236914, 0.03857594,
        0.04962175, 0.01343043, 0.25441896, 0.0160427 , 0.06778239,
        0.0210183 , 0.02225835, 0.07925372, 0.04767077, 0.02797869,
        0.02798629, 0.07434253, 0.01783648, 0.04324498, 0.02560097,
        0.00850071, 0.01475911, 0.10499547, 0.05004732, 0.50055156]),
 &#39;param_n_neurons&#39;: masked_array(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
                    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,
                    31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44,
                    45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58,
                    59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,
                    73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86,
                    87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,
                    100],
              mask=[False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False, False, False, False, False,
                    False, False, False, False],
        fill_value=&#39;?&#39;,
             dtype=object),
 &#39;params&#39;: [{&#39;n_neurons&#39;: 1},
  {&#39;n_neurons&#39;: 2},
  {&#39;n_neurons&#39;: 3},
  {&#39;n_neurons&#39;: 4},
  {&#39;n_neurons&#39;: 5},
  {&#39;n_neurons&#39;: 6},
  {&#39;n_neurons&#39;: 7},
  {&#39;n_neurons&#39;: 8},
  {&#39;n_neurons&#39;: 9},
  {&#39;n_neurons&#39;: 10},
  {&#39;n_neurons&#39;: 11},
  {&#39;n_neurons&#39;: 12},
  {&#39;n_neurons&#39;: 13},
  {&#39;n_neurons&#39;: 14},
  {&#39;n_neurons&#39;: 15},
  {&#39;n_neurons&#39;: 16},
  {&#39;n_neurons&#39;: 17},
  {&#39;n_neurons&#39;: 18},
  {&#39;n_neurons&#39;: 19},
  {&#39;n_neurons&#39;: 20},
  {&#39;n_neurons&#39;: 21},
  {&#39;n_neurons&#39;: 22},
  {&#39;n_neurons&#39;: 23},
  {&#39;n_neurons&#39;: 24},
  {&#39;n_neurons&#39;: 25},
  {&#39;n_neurons&#39;: 26},
  {&#39;n_neurons&#39;: 27},
  {&#39;n_neurons&#39;: 28},
  {&#39;n_neurons&#39;: 29},
  {&#39;n_neurons&#39;: 30},
  {&#39;n_neurons&#39;: 31},
  {&#39;n_neurons&#39;: 32},
  {&#39;n_neurons&#39;: 33},
  {&#39;n_neurons&#39;: 34},
  {&#39;n_neurons&#39;: 35},
  {&#39;n_neurons&#39;: 36},
  {&#39;n_neurons&#39;: 37},
  {&#39;n_neurons&#39;: 38},
  {&#39;n_neurons&#39;: 39},
  {&#39;n_neurons&#39;: 40},
  {&#39;n_neurons&#39;: 41},
  {&#39;n_neurons&#39;: 42},
  {&#39;n_neurons&#39;: 43},
  {&#39;n_neurons&#39;: 44},
  {&#39;n_neurons&#39;: 45},
  {&#39;n_neurons&#39;: 46},
  {&#39;n_neurons&#39;: 47},
  {&#39;n_neurons&#39;: 48},
  {&#39;n_neurons&#39;: 49},
  {&#39;n_neurons&#39;: 50},
  {&#39;n_neurons&#39;: 51},
  {&#39;n_neurons&#39;: 52},
  {&#39;n_neurons&#39;: 53},
  {&#39;n_neurons&#39;: 54},
  {&#39;n_neurons&#39;: 55},
  {&#39;n_neurons&#39;: 56},
  {&#39;n_neurons&#39;: 57},
  {&#39;n_neurons&#39;: 58},
  {&#39;n_neurons&#39;: 59},
  {&#39;n_neurons&#39;: 60},
  {&#39;n_neurons&#39;: 61},
  {&#39;n_neurons&#39;: 62},
  {&#39;n_neurons&#39;: 63},
  {&#39;n_neurons&#39;: 64},
  {&#39;n_neurons&#39;: 65},
  {&#39;n_neurons&#39;: 66},
  {&#39;n_neurons&#39;: 67},
  {&#39;n_neurons&#39;: 68},
  {&#39;n_neurons&#39;: 69},
  {&#39;n_neurons&#39;: 70},
  {&#39;n_neurons&#39;: 71},
  {&#39;n_neurons&#39;: 72},
  {&#39;n_neurons&#39;: 73},
  {&#39;n_neurons&#39;: 74},
  {&#39;n_neurons&#39;: 75},
  {&#39;n_neurons&#39;: 76},
  {&#39;n_neurons&#39;: 77},
  {&#39;n_neurons&#39;: 78},
  {&#39;n_neurons&#39;: 79},
  {&#39;n_neurons&#39;: 80},
  {&#39;n_neurons&#39;: 81},
  {&#39;n_neurons&#39;: 82},
  {&#39;n_neurons&#39;: 83},
  {&#39;n_neurons&#39;: 84},
  {&#39;n_neurons&#39;: 85},
  {&#39;n_neurons&#39;: 86},
  {&#39;n_neurons&#39;: 87},
  {&#39;n_neurons&#39;: 88},
  {&#39;n_neurons&#39;: 89},
  {&#39;n_neurons&#39;: 90},
  {&#39;n_neurons&#39;: 91},
  {&#39;n_neurons&#39;: 92},
  {&#39;n_neurons&#39;: 93},
  {&#39;n_neurons&#39;: 94},
  {&#39;n_neurons&#39;: 95},
  {&#39;n_neurons&#39;: 96},
  {&#39;n_neurons&#39;: 97},
  {&#39;n_neurons&#39;: 98},
  {&#39;n_neurons&#39;: 99},
  {&#39;n_neurons&#39;: 100}],
 &#39;split0_test_score&#39;: array([0.37754992, 0.64923096, 0.80380714, 0.86658669, 0.8869859 ,
        0.90874875, 0.90683973, 0.91589397, 0.91562122, 0.92429364,
        0.92309368, 0.9242391 , 0.93585688, 0.93078434, 0.93634778,
        0.94480199, 0.94109303, 0.93722045, 0.94687468, 0.9412567 ,
        0.94180214, 0.94534743, 0.95123816, 0.9505291 , 0.95663792,
        0.9480201 , 0.95385623, 0.95069271, 0.95554709, 0.95609248,
        0.95260173, 0.95581979, 0.95669246, 0.95511073, 0.95952874,
        0.95903784, 0.95658338, 0.95778334, 0.96160138, 0.96001965,
        0.96405584, 0.96061963, 0.96160138, 0.96018326, 0.9547289 ,
        0.95631069, 0.95925605, 0.96067417, 0.96269226, 0.96078324,
        0.96307409, 0.9622559 , 0.96127415, 0.95881969, 0.96252865,
        0.96432859, 0.96765572, 0.96231043, 0.96378314, 0.96340132,
        0.96454674, 0.9670012 , 0.96421951, 0.96629214, 0.96754664,
        0.96269226, 0.96307409, 0.9657467 , 0.96547401, 0.96192867,
        0.96394676, 0.96640122, 0.96596485, 0.9627468 , 0.96241957,
        0.96961927, 0.96476489, 0.96765572, 0.96416491, 0.96378314,
        0.96460128, 0.96536487, 0.96612853, 0.96792841, 0.96885568,
        0.96438313, 0.96721935, 0.96481949, 0.96509218, 0.96634668,
        0.96678305, 0.96531034, 0.96912837, 0.96672851, 0.96580124,
        0.96661937, 0.96569216, 0.9644922 , 0.96983749, 0.96809208]),
 &#39;split1_test_score&#39;: array([0.39955273, 0.68204874, 0.76883215, 0.83646977, 0.88343424,
        0.89919817, 0.90814376, 0.91998035, 0.91676211, 0.9239077 ,
        0.92750776, 0.92630774, 0.93628973, 0.93688977, 0.9364534 ,
        0.92794412, 0.93787158, 0.94354445, 0.93858069, 0.95019907,
        0.94332623, 0.95003545, 0.94812632, 0.95238096, 0.95003545,
        0.94987184, 0.95401734, 0.95336276, 0.95287186, 0.95810831,
        0.95183551, 0.95614463, 0.95647192, 0.9558174 , 0.95554465,
        0.96018112, 0.9571265 , 0.95794469, 0.96149021, 0.96045381,
        0.96007198, 0.96083564, 0.96378118, 0.9613266 , 0.96034473,
        0.95865381, 0.96230841, 0.96389025, 0.96323568, 0.96116292,
        0.96454483, 0.96476299, 0.96552664, 0.96503574, 0.96285385,
        0.96329021, 0.96345389, 0.96165383, 0.96623576, 0.96569031,
        0.96350843, 0.96667212, 0.96639937, 0.9654721 , 0.96694487,
        0.96034473, 0.96683574, 0.96149021, 0.9649812 , 0.96509027,
        0.96890855, 0.96378118, 0.9618175 , 0.96405387, 0.96852672,
        0.96694487, 0.96825397, 0.96481752, 0.96705395, 0.96601754,
        0.9667812 , 0.96847218, 0.96459937, 0.96034473, 0.97021765,
        0.96645391, 0.9649812 , 0.96612668, 0.96432662, 0.97125405,
        0.96558118, 0.96879941, 0.96585393, 0.96476299, 0.9667812 ,
        0.9654721 , 0.96541756, 0.96710849, 0.96519935, 0.9649812 ]),
 &#39;split2_test_score&#39;: array([0.35591558, 0.63344789, 0.78595972, 0.86461574, 0.8940708 ,
        0.90705287, 0.90492553, 0.92543501, 0.92188948, 0.92892599,
        0.93236238, 0.93214422, 0.94043529, 0.93956256, 0.940108  ,
        0.94348991, 0.94687176, 0.94305348, 0.95118093, 0.94725358,
        0.94365352, 0.9479627 , 0.95025367, 0.95167184, 0.95238096,
        0.95123547, 0.95734465, 0.95499921, 0.95167184, 0.95456278,
        0.95876288, 0.95614463, 0.961272  , 0.95849013, 0.95341736,
        0.96236295, 0.96149021, 0.95685375, 0.96034473, 0.95958108,
        0.96449023, 0.96279931, 0.95985383, 0.96454483, 0.96454483,
        0.96258116, 0.95936292, 0.96176296, 0.96421754, 0.96329021,
        0.96329021, 0.96383572, 0.96383572, 0.96454483, 0.96449023,
        0.96334481, 0.96427208, 0.9626357 , 0.96329021, 0.96410847,
        0.96759939, 0.96405387, 0.96258116, 0.96274477, 0.96509027,
        0.96710849, 0.9649812 , 0.96470845, 0.96765393, 0.96710849,
        0.96607208, 0.96830851, 0.96650851, 0.96574485, 0.96645391,
        0.96165383, 0.96219933, 0.96569031, 0.96623576, 0.9673267 ,
        0.96607208, 0.96618122, 0.96645391, 0.96601754, 0.96552664,
        0.96874487, 0.96596301, 0.96809036, 0.96738124, 0.96590847,
        0.96939945, 0.96852672, 0.96754485, 0.96792668, 0.97054493,
        0.96650851, 0.96738124, 0.96689034, 0.96738124, 0.96525389]),
 &#39;mean_test_score&#39;: array([0.37767274, 0.65490919, 0.78619967, 0.85589073, 0.88816365,
        0.90499993, 0.90663634, 0.92043644, 0.91809094, 0.92570911,
        0.9276546 , 0.92756369, 0.9375273 , 0.93574556, 0.9376364 ,
        0.93874534, 0.94194545, 0.9412728 , 0.94554543, 0.94623645,
        0.9429273 , 0.94778186, 0.94987271, 0.9515273 , 0.95301811,
        0.94970914, 0.95507274, 0.95301823, 0.9533636 , 0.95625452,
        0.95440004, 0.95603635, 0.95814546, 0.95647275, 0.95616359,
        0.9605273 , 0.95840003, 0.95752726, 0.96114544, 0.96001818,
        0.96287268, 0.96141819, 0.96174546, 0.96201823, 0.95987282,
        0.95918188, 0.96030913, 0.96210913, 0.96338183, 0.96174546,
        0.96363638, 0.9636182 , 0.9635455 , 0.96280009, 0.96329091,
        0.96365454, 0.96512723, 0.96219999, 0.96443637, 0.96440003,
        0.96521819, 0.96590906, 0.96440001, 0.96483634, 0.96652726,
        0.96338183, 0.96496367, 0.96398179, 0.96603638, 0.96470914,
        0.96630913, 0.96616364, 0.96476362, 0.96418184, 0.96580007,
        0.96607266, 0.96507273, 0.96605452, 0.96581821, 0.96570913,
        0.96581819, 0.96667276, 0.96572727, 0.96476356, 0.96819999,
        0.9665273 , 0.96605452, 0.96634551, 0.96560001, 0.9678364 ,
        0.96725456, 0.96754549, 0.96750905, 0.96647273, 0.96770912,
        0.96619999, 0.96616366, 0.96616367, 0.96747269, 0.96610906]),
 &#39;std_test_score&#39;: array([0.01781501, 0.02024339, 0.01427949, 0.01375625, 0.0044215 ,
        0.00416047, 0.00132169, 0.00390844, 0.00272606, 0.00228013,
        0.00378536, 0.00334722, 0.00206384, 0.00367389, 0.00174822,
        0.00765637, 0.00372342, 0.00287244, 0.00522919, 0.00372088,
        0.00080675, 0.00191815, 0.00129865, 0.00076289, 0.00273284,
        0.0013177 , 0.00160783, 0.00177492, 0.00161983, 0.00145198,
        0.00310081, 0.00015313, 0.00221263, 0.00145538, 0.00253306,
        0.00137936, 0.00219631, 0.00048078, 0.000568  , 0.00035629,
        0.00198832, 0.00098057, 0.00160657, 0.00184654, 0.00402121,
        0.002587  , 0.00141438, 0.00133558, 0.00063121, 0.00110325,
        0.00064841, 0.00103501, 0.00174816, 0.00282169, 0.00085838,
        0.00047715, 0.00181885, 0.00040838, 0.00128817, 0.00095695,
        0.0017363 , 0.00131868, 0.001564  , 0.00151638, 0.00104538,
        0.00280401, 0.00153574, 0.00181208, 0.00116134, 0.00213176,
        0.00203256, 0.00185589, 0.00209501, 0.00122729, 0.00253574,
        0.00330985, 0.00248136, 0.00118696, 0.00121584, 0.001463  ,
        0.00090788, 0.00131531, 0.00080853, 0.0032205 , 0.00197042,
        0.00178143, 0.00091601, 0.00134426, 0.00129771, 0.00242326,
        0.00159406, 0.00158441, 0.00133702, 0.00130418, 0.00204474,
        0.00051668, 0.00086823, 0.00118526, 0.00189461, 0.00140662]),
 &#39;rank_test_score&#39;: array([100,  99,  98,  97,  96,  95,  94,  92,  93,  91,  89,  90,  87,
         88,  86,  85,  83,  84,  81,  80,  82,  79,  77,  76,  75,  78,
         71,  74,  73,  68,  72,  70,  65,  67,  69,  59,  64,  66,  58,
         61,  50,  57,  55,  54,  62,  63,  60,  53,  47,  55,  44,  45,
         46,  51,  49,  43,  31,  52,  38,  39,  30,  23,  40,  34,  10,
         47,  33,  42,  22,  37,  13,  17,  35,  41,  26,  19,  32,  20,
         24,  28,  25,   8,  27,  36,   1,   9,  20,  12,  29,   2,   7,
          4,   5,  11,   3,  14,  16,  15,   6,  18], dtype=int32)}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">],</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;rank_test_score&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[14]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>([{&#39;n_neurons&#39;: 1},
  {&#39;n_neurons&#39;: 2},
  {&#39;n_neurons&#39;: 3},
  {&#39;n_neurons&#39;: 4},
  {&#39;n_neurons&#39;: 5},
  {&#39;n_neurons&#39;: 6},
  {&#39;n_neurons&#39;: 7},
  {&#39;n_neurons&#39;: 8},
  {&#39;n_neurons&#39;: 9},
  {&#39;n_neurons&#39;: 10},
  {&#39;n_neurons&#39;: 11},
  {&#39;n_neurons&#39;: 12},
  {&#39;n_neurons&#39;: 13},
  {&#39;n_neurons&#39;: 14},
  {&#39;n_neurons&#39;: 15},
  {&#39;n_neurons&#39;: 16},
  {&#39;n_neurons&#39;: 17},
  {&#39;n_neurons&#39;: 18},
  {&#39;n_neurons&#39;: 19},
  {&#39;n_neurons&#39;: 20},
  {&#39;n_neurons&#39;: 21},
  {&#39;n_neurons&#39;: 22},
  {&#39;n_neurons&#39;: 23},
  {&#39;n_neurons&#39;: 24},
  {&#39;n_neurons&#39;: 25},
  {&#39;n_neurons&#39;: 26},
  {&#39;n_neurons&#39;: 27},
  {&#39;n_neurons&#39;: 28},
  {&#39;n_neurons&#39;: 29},
  {&#39;n_neurons&#39;: 30},
  {&#39;n_neurons&#39;: 31},
  {&#39;n_neurons&#39;: 32},
  {&#39;n_neurons&#39;: 33},
  {&#39;n_neurons&#39;: 34},
  {&#39;n_neurons&#39;: 35},
  {&#39;n_neurons&#39;: 36},
  {&#39;n_neurons&#39;: 37},
  {&#39;n_neurons&#39;: 38},
  {&#39;n_neurons&#39;: 39},
  {&#39;n_neurons&#39;: 40},
  {&#39;n_neurons&#39;: 41},
  {&#39;n_neurons&#39;: 42},
  {&#39;n_neurons&#39;: 43},
  {&#39;n_neurons&#39;: 44},
  {&#39;n_neurons&#39;: 45},
  {&#39;n_neurons&#39;: 46},
  {&#39;n_neurons&#39;: 47},
  {&#39;n_neurons&#39;: 48},
  {&#39;n_neurons&#39;: 49},
  {&#39;n_neurons&#39;: 50},
  {&#39;n_neurons&#39;: 51},
  {&#39;n_neurons&#39;: 52},
  {&#39;n_neurons&#39;: 53},
  {&#39;n_neurons&#39;: 54},
  {&#39;n_neurons&#39;: 55},
  {&#39;n_neurons&#39;: 56},
  {&#39;n_neurons&#39;: 57},
  {&#39;n_neurons&#39;: 58},
  {&#39;n_neurons&#39;: 59},
  {&#39;n_neurons&#39;: 60},
  {&#39;n_neurons&#39;: 61},
  {&#39;n_neurons&#39;: 62},
  {&#39;n_neurons&#39;: 63},
  {&#39;n_neurons&#39;: 64},
  {&#39;n_neurons&#39;: 65},
  {&#39;n_neurons&#39;: 66},
  {&#39;n_neurons&#39;: 67},
  {&#39;n_neurons&#39;: 68},
  {&#39;n_neurons&#39;: 69},
  {&#39;n_neurons&#39;: 70},
  {&#39;n_neurons&#39;: 71},
  {&#39;n_neurons&#39;: 72},
  {&#39;n_neurons&#39;: 73},
  {&#39;n_neurons&#39;: 74},
  {&#39;n_neurons&#39;: 75},
  {&#39;n_neurons&#39;: 76},
  {&#39;n_neurons&#39;: 77},
  {&#39;n_neurons&#39;: 78},
  {&#39;n_neurons&#39;: 79},
  {&#39;n_neurons&#39;: 80},
  {&#39;n_neurons&#39;: 81},
  {&#39;n_neurons&#39;: 82},
  {&#39;n_neurons&#39;: 83},
  {&#39;n_neurons&#39;: 84},
  {&#39;n_neurons&#39;: 85},
  {&#39;n_neurons&#39;: 86},
  {&#39;n_neurons&#39;: 87},
  {&#39;n_neurons&#39;: 88},
  {&#39;n_neurons&#39;: 89},
  {&#39;n_neurons&#39;: 90},
  {&#39;n_neurons&#39;: 91},
  {&#39;n_neurons&#39;: 92},
  {&#39;n_neurons&#39;: 93},
  {&#39;n_neurons&#39;: 94},
  {&#39;n_neurons&#39;: 95},
  {&#39;n_neurons&#39;: 96},
  {&#39;n_neurons&#39;: 97},
  {&#39;n_neurons&#39;: 98},
  {&#39;n_neurons&#39;: 99},
  {&#39;n_neurons&#39;: 100}],
 array([100,  99,  98,  97,  96,  95,  94,  92,  93,  91,  89,  90,  87,
         88,  86,  85,  83,  84,  81,  80,  82,  79,  77,  76,  75,  78,
         71,  74,  73,  68,  72,  70,  65,  67,  69,  59,  64,  66,  58,
         61,  50,  57,  55,  54,  62,  63,  60,  53,  47,  55,  44,  45,
         46,  51,  49,  43,  31,  52,  38,  39,  30,  23,  40,  34,  10,
         47,  33,  42,  22,  37,  13,  17,  35,  41,  26,  19,  32,  20,
         24,  28,  25,   8,  27,  36,   1,   9,  20,  12,  29,   2,   7,
          4,   5,  11,   3,  14,  16,  15,   6,  18], dtype=int32))</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_cv</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">model</span>
<span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">best_model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>10000/10000 [==============================] - 0s 33us/sample - loss: 0.0978 - accuracy: 0.9748
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[16]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[0.09776723790270044, 0.9748]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Further-explortion...">Further explortion...<a class="anchor-link" href="#Further-explortion...">&#182;</a></h2><p>Try the following grid parameters:</p>
<div class="highlight"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neurons&#39;</span><span class="p">:</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">900</span><span class="p">,</span><span class="mi">100</span><span class="p">)}</span>
<span class="n">param_grid</span>
</pre></div>
<p>Were you able to get accuracies over 99 percent? How many epochs are needed to train the model before it starts overfitting?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Saving-the-best-model">Saving the best model<a class="anchor-link" href="#Saving-the-best-model">&#182;</a></h2><p><strong>N.B</strong>: Note that I ran the code three times with a different numbr of nodes each and time with higher test accuracy (~99.6%).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_model</span> <span class="o">=</span> <span class="n">grid_cv</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">model</span>
<span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">plot_model</span><span class="p">(</span><span class="n">best_model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;mnist_model_best.h5&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluating-the-model">Evaluating the model<a class="anchor-link" href="#Evaluating-the-model">&#182;</a></h2><p>The model with 500 neurons gets a test accuracy of 98%</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">best_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_images</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>10000/10000 [==============================] - 0s 25us/sample - loss: 0.0978 - accuracy: 0.9748
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[19]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[0.09776723790270044, 0.9748]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Plotting-the-performance">Plotting the performance<a class="anchor-link" href="#Plotting-the-performance">&#182;</a></h2><p>We use Matplotlib to create 2 plots--displaying the training and validation loss (resp. accuracy) for each (training) epoch side by side.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">history_dict</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">history</span>
<span class="n">history_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[22]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>dict_keys([&#39;loss&#39;, &#39;accuracy&#39;, &#39;val_loss&#39;, &#39;val_accuracy&#39;])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;val_accuracy&#39;</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># &quot;bo&quot; is for &quot;blue dot&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
<span class="c1"># b is for &quot;solid blue line&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation loss&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5xVVf3/8deHiyB3BEwFuampgAg4In5R8JZh5iUjBcFbGF7T8lvJVy0Vo7x9lSh+fiWTLoyQYSqZSpYYWQkMVwUlEAFHEGEQBEFh4PP7Y+0ZDsOe4czM2XPmMO/n47Ef5+zr+ZwzcD5nrbXXWubuiIiIlFUv2wGIiEjtpAQhIiKxlCBERCSWEoSIiMRSghARkVhKECIiEksJQmqEmdU3s61m1jGTx2aTmR1tZhm/T9zMzjGzlSnrS83s9HSOrcJrPWFmd1T1/Aqu+2Mz+3Wmrys1q0G2A5Daycy2pqw2AT4HdkXr17l7fmWu5+67gGaZPrYucPdjM3EdM7sWGO7uZ6Rc+9pMXFsOTEoQEsvdS7+go1+o17r7X8s73swauHtxTcQmIjVDVUxSJVEVwu/NbLKZbQGGm9mpZvaGmW0ys7VmNs7MGkbHNzAzN7PO0fqkaP9LZrbFzP5tZl0qe2y0/zwz+4+ZbTazn5vZP83s6nLiTifG68xsuZl9bGbjUs6tb2aPmlmRmb0LDKrg87nLzKaU2TbezB6Jnl9rZm9H7+fd6Nd9edcqNLMzoudNzOx3UWyLgZNiXndFdN3FZnZhtP0E4BfA6VH13YaUz/aelPOvj957kZk9Z2aHp/PZ7I+ZXRzFs8nMXjWzY1P23WFma8zsEzN7J+W99jOzedH2dWb2ULqvJxni7lq0VLgAK4Fzymz7MbADuIDwQ+Ng4GTgFELJtCvwH+Dm6PgGgAOdo/VJwAYgD2gI/B6YVIVjDwW2ABdF+24DdgJXl/Ne0onxeaAl0BnYWPLegZuBxUAHoA0wM/wXin2drsBWoGnKtT8C8qL1C6JjDDgL2A70jPadA6xMuVYhcEb0/GHgNaA10AlYUubYS4HDo7/J5VEMX4j2XQu8VibOScA90fNzoxh7AY2B/we8ms5nE/P+fwz8Onp+fBTHWdHf6I7oc28IdAdWAYdFx3YBukbP5wBDo+fNgVOy/X+hri0qQUh1vO7uf3L33e6+3d3nuPssdy929xXABGBgBedPdfcCd98J5BO+mCp77FeBBe7+fLTvUUIyiZVmjD91983uvpLwZVzyWpcCj7p7obsXAfdX8DorgLcIiQvgS8Amdy+I9v/J3Vd48CrwNyC2IbqMS4Efu/vH7r6KUCpIfd2n3X1t9Dd5ipDc89K4LsAw4Al3X+DunwGjgIFm1iHlmPI+m4oMAaa5+6vR3+h+oAUhURcTklH3qJryveizg5DojzGzNu6+xd1npfk+JEOUIKQ63k9dMbPjzOzPZvahmX0CjAbaVnD+hynPt1Fxw3R5xx6RGoe7O+EXd6w0Y0zrtQi/fCvyFDA0en45IbGVxPFVM5tlZhvNbBPh13tFn1WJwyuKwcyuNrOFUVXOJuC4NK8L4f2VXs/dPwE+BtqnHFOZv1l5191N+Bu1d/elwH8T/g4fRVWWh0WHXgN0A5aa2Wwz+0qa70MyRAlCqqPsLZ6PE341H+3uLYAfEapQkrSWUOUDgJkZe3+hlVWdGNcCR6as7+823N8D50S/wC8iJAzM7GBgKvBTQvVPK+AvacbxYXkxmFlX4DHgBqBNdN13Uq67v1ty1xCqrUqu15xQlfVBGnFV5rr1CH+zDwDcfZK79ydUL9UnfC64+1J3H0KoRvxf4Bkza1zNWKQSlCAkk5oDm4FPzex44LoaeM0XgD5mdoGZNQBuBdolFOPTwHfMrL2ZtQFur+hgd18HvA5MBJa6+7JoVyPgIGA9sMvMvgqcXYkY7jCzVhb6idycsq8ZIQmsJ+TKawkliBLrgA4ljfIxJgMjzKynmTUifFH/w93LLZFVIuYLzeyM6LW/T2g3mmVmx5vZmdHrbY+WXYQ3cIWZtY1KHJuj97a7mrFIJShBSCb9N3AV4T//44Rf0ImKvoQvAx4BioCjgPmEfhuZjvExQlvBm4QG1KlpnPMUodH5qZSYNwHfBZ4lNPQOJiS6dNxNKMmsBF4Cfpty3UXAOGB2dMxxQGq9/SvAMmCdmaVWFZWc/zKhqufZ6PyOhHaJanH3xYTP/DFC8hoEXBi1RzQCHiS0G31IKLHcFZ36FeBtC3fJPQxc5u47qhuPpM9Cla3IgcHM6hOqNAa7+z+yHY9ILlMJQnKemQ0ys5ZRNcUPCXfGzM5yWCI5TwlCDgSnASsI1RSDgIvdvbwqJhFJk6qYREQklkoQIiISK9HB+sxsEPAzwr3NT7j7/WX2Xw/cRLitbSsw0t2XRPv+BxgR7bvF3adX9Fpt27b1zp07Z/w9iIgcyObOnbvB3WNvDU+siim6m+Q/hCEGCtkzrsqSlGNaRL01iQYVu9HdB5lZN8I92X0JvTD/CnzRwzDQsfLy8rygoCCR9yIicqAys7nuHjscS5JVTH2B5dF4MzuAKewZlwYo7cpfoil7enpeBExx98/d/T1geXQ9ERGpIUlWMbVn7zFjCgmDc+3FzG4ijMB5EGG0x5Jz3yhz7j7DJ5jZSGAkQMeOtXryMRGRnJNkCSJuXJl96rPcfby7H0UYtqCkB2W6505w9zx3z2vXrqLRFUREpLKSLEEUsvegYh0IPVzLM4XQFb8q54pIDdu5cyeFhYV89tln2Q5F0tC4cWM6dOhAw4blDcW1ryQTxBzCWO5dCKM2DiEMeVzKzI5JGcDsfMI4MQDTgKei2beOAI5BPWNFapXCwkKaN29O586dCYPoSm3l7hQVFVFYWEiXLl32f0IksSomD/MT3wxMB94Gnnb3xWY2umQaRODmaBrCBYR2iKuicxcTRoBcArwM3FTRHUzVkZ8PnTtDvXrhMT9/f2eICMBnn31GmzZtlBxygJnRpk2bSpf2Eu0H4e4vAi+W2fajlOe3VnDuGGBMctGFZDByJGzbFtZXrQrrAMOqPYalyIFPySF3VOVvVad7Ut95557kUGLbtrBdRKSuq9MJYvXqym0XkdqjqKiIXr160atXLw477DDat29fur5jR3rTRlxzzTUsXbq0wmPGjx9Pfobqnk877TQWLFiQkWvVhESrmGq7jh1DtVLcdhHJrPz8UDpfvTr8HxszpnpVuW3atCn9sr3nnnto1qwZ3/ve9/Y6xt1xd+rVi/8tPHHixP2+zk033VT1IHNcnS5BjBkDTZrsva1Jk7BdRDKnpL1v1Spw39Pel8RNIcuXL6dHjx5cf/319OnTh7Vr1zJy5Ejy8vLo3r07o0ePLj225Bd9cXExrVq1YtSoUZx44omceuqpfPTRRwDcddddjB07tvT4UaNG0bdvX4499lj+9a9/AfDpp5/y9a9/nRNPPJGhQ4eSl5e335LCpEmTOOGEE+jRowd33HEHAMXFxVxxxRWl28eNGwfAo48+Srdu3TjxxBMZPnx4xj+z8tTpBDFsGEyYAJ06gVl4nDBBDdQimVbT7X1LlixhxIgRzJ8/n/bt23P//fdTUFDAwoULeeWVV1iyZMk+52zevJmBAweycOFCTj31VJ588snYa7s7s2fP5qGHHipNNj//+c857LDDWLhwIaNGjWL+/PkVxldYWMhdd93FjBkzmD9/Pv/85z954YUXmDt3Lhs2bODNN9/krbfe4sorrwTgwQcfZMGCBSxcuJBf/OIX1fx00lenEwSEZLByJezeHR6VHEQyr6bb+4466ihOPvnk0vXJkyfTp08f+vTpw9tvvx2bIA4++GDOO+88AE466SRWrlwZe+1LLrlkn2Nef/11hgwZAsCJJ55I9+7dK4xv1qxZnHXWWbRt25aGDRty+eWXM3PmTI4++miWLl3KrbfeyvTp02nZsiUA3bt3Z/jw4eTn51eqo1t11fkEISLJK69dL6n2vqZNm5Y+X7ZsGT/72c949dVXWbRoEYMGDYrtD3DQQQeVPq9fvz7FxcWx127UqNE+x1R2VOzyjm/Tpg2LFi3itNNOY9y4cVx33XUATJ8+neuvv57Zs2eTl5fHrl2JdAvbhxKEiCQum+19n3zyCc2bN6dFixasXbuW6dMrnFqmSk477TSefvppAN58883YEkqqfv36MWPGDIqKiiguLmbKlCkMHDiQ9evX4+584xvf4N5772XevHns2rWLwsJCzjrrLB566CHWr1/PtrL1dQmp03cxiUjNKKm6zeRdTOnq06cP3bp1o0ePHnTt2pX+/ftn/DW+/e1vc+WVV9KzZ0/69OlDjx49SquH4nTo0IHRo0dzxhln4O5ccMEFnH/++cybN48RI0bg7pgZDzzwAMXFxVx++eVs2bKF3bt3c/vtt9O8efOMv4c4B8yc1JowSKRmvf322xx//PHZDqNWKC4upri4mMaNG7Ns2TLOPfdcli1bRoMGtes3eNzfrKIJg2pX9CIiOWjr1q2cffbZFBcX4+48/vjjtS45VEXuvwMRkSxr1aoVc+fOzXYYGadGahERiaUEISIisZQgREQklhKEiIjEUoIQkZx0xhln7NPpbezYsdx4440VntesWTMA1qxZw+DBg8u99v5umx87duxeHda+8pWvsGnTpnRCr9A999zDww8/XO3rZIIShIjkpKFDhzJlypS9tk2ZMoWhQ4emdf4RRxzB1KlTq/z6ZRPEiy++SKtWrap8vdpICUJEctLgwYN54YUX+PzzzwFYuXIla9as4bTTTivtl9CnTx9OOOEEnn/++X3OX7lyJT169ABg+/btDBkyhJ49e3LZZZexffv20uNuuOGG0qHC7777bgDGjRvHmjVrOPPMMznzzDMB6Ny5Mxs2bADgkUceoUePHvTo0aN0qPCVK1dy/PHH861vfYvu3btz7rnn7vU6cRYsWEC/fv3o2bMnX/va1/j4449LX79bt2707NmzdJDAv//976UTJvXu3ZstW7ZU+bMtoX4QIlJt3/kOZHqitF69IPpujdWmTRv69u3Lyy+/zEUXXcSUKVO47LLLMDMaN27Ms88+S4sWLdiwYQP9+vXjwgsvLHde5scee4wmTZqwaNEiFi1aRJ8+fUr3jRkzhkMOOYRdu3Zx9tlns2jRIm655RYeeeQRZsyYQdu2bfe61ty5c5k4cSKzZs3C3TnllFMYOHAgrVu3ZtmyZUyePJlf/vKXXHrppTzzzDMVzu9w5ZVX8vOf/5yBAwfyox/9iHvvvZexY8dy//33895779GoUaPSaq2HH36Y8ePH079/f7Zu3Urjxo0r8WnHUwlCRHJWajVTavWSu3PHHXfQs2dPzjnnHD744APWrVtX7nVmzpxZ+kXds2dPevbsWbrv6aefpk+fPvTu3ZvFixfvdyC+119/na997Ws0bdqUZs2acckll/CPf/wDgC5dutCrVy+g4iHFIcxPsWnTJgYOHAjAVVddxcyZM0tjHDZsGJMmTSrtsd2/f39uu+02xo0bx6ZNmzLSk1slCBGptop+6Sfp4osv5rbbbmPevHls37699Jd/fn4+69evZ+7cuTRs2JDOnTvHDvGdKq508d577/Hwww8zZ84cWrduzdVXX73f61Q0vl3JUOEQhgvfXxVTef785z8zc+ZMpk2bxn333cfixYsZNWoU559/Pi+++CL9+vXjr3/9K8cdd1yVrl9CJQgRyVnNmjXjjDPO4Jvf/OZejdObN2/m0EMPpWHDhsyYMYNVcZPPpxgwYAD50fynb731FosWLQLCUOFNmzalZcuWrFu3jpdeeqn0nObNm8fW8w8YMIDnnnuObdu28emnn/Lss89y+umnV/q9tWzZktatW5eWPn73u98xcOBAdu/ezfvvv8+ZZ57Jgw8+yKZNm9i6dSvvvvsuJ5xwArfffjt5eXm88847lX7NslSCEJGcNnToUC655JK97mgaNmwYF1xwAXl5efTq1Wu/v6RvuOEGrrnmGnr27EmvXr3o27cvEGaH6927N927d99nqPCRI0dy3nnncfjhhzNjxozS7X369OHqq68uvca1115L7969K6xOKs9vfvMbrr/+erZt20bXrl2ZOHEiu3btYvjw4WzevBl357vf/S6tWrXihz/8ITNmzKB+/fp069atdHa86tBw3yJSJRruO/dUdrhvVTGJiEgsJQgREYmlBCEiVXagVFHXBVX5WylBiEiVNG7cmKKiIiWJHODuFBUVVbrznO5iEpEq6dChA4WFhaxfvz7boUgaGjduTIcOHSp1jhKEiFRJw4YN6dKlS7bDkASpiklERGIpQYiISKxEE4SZDTKzpWa23MxGxey/zcyWmNkiM/ubmXVK2bfLzBZEy7Qk4xQRkX0l1gZhZvWB8cCXgEJgjplNc/fUoRDnA3nuvs3MbgAeBC6L9m13915JxSciIhVLsgTRF1ju7ivcfQcwBbgo9QB3n+HuJVMyvQFUroldREQSk2SCaA+8n7JeGG0rzwjgpZT1xmZWYGZvmNnFcSeY2cjomALdaicikllJ3uYaN3VTbI8aMxsO5AEDUzZ3dPc1ZtYVeNXM3nT3d/e6mPsEYAKEwfoyE7aIiECyJYhC4MiU9Q7AmrIHmdk5wJ3Ahe7+ecl2d18TPa4AXgN6JxiriIiUkWSCmAMcY2ZdzOwgYAiw191IZtYbeJyQHD5K2d7azBpFz9sC/YGK5/kTEZGMSqyKyd2LzexmYDpQH3jS3Reb2WigwN2nAQ8BzYA/RNP9rXb3C4HjgcfNbDchid1f5u4nERFJmCYMEhGpwzRhkIiIVJoShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmlBCEiIrESTRBmNsjMlprZcjMbFbP/NjNbYmaLzOxvZtYpZd9VZrYsWq5KMk4REdlXYgnCzOoD44HzgG7AUDPrVuaw+UCeu/cEpgIPRuceAtwNnAL0Be42s9ZJxSoiIvtKsgTRF1ju7ivcfQcwBbgo9QB3n+Hu26LVN4AO0fMvA6+4+0Z3/xh4BRiUYKwiIlJGkgmiPfB+ynphtK08I4CXKnOumY00swIzK1i/fn01wxURkVRJJgiL2eaxB5oNB/KAhypzrrtPcPc8d89r165dlQMVEZF9JZkgCoEjU9Y7AGvKHmRm5wB3Ahe6++eVOVdERJKTZIKYAxxjZl3M7CBgCDAt9QAz6w08TkgOH6Xsmg6ca2ato8bpc6NtIiJSQxokdWF3Lzazmwlf7PWBJ919sZmNBgrcfRqhSqkZ8AczA1jt7he6+0Yzu4+QZABGu/vGpGIVEZF9mXtss0DOycvL84KCgmyHISKSU8xsrrvnxe1TT2oREYmlBCEiIrGUIEREJJYShIiIxFKCEBGRWEoQIiISSwlCRERiKUGIiEgsJQgREYmVVoIws6PMrFH0/Awzu8XMWiUbmoiIZFO6JYhngF1mdjTwK6AL8FRiUYmISNalmyB2u3sx8DVgrLt/Fzg8ubBERCTb0k0QO81sKHAV8EK0rWEyIYmISG2QboK4BjgVGOPu75lZF2BScmGJiEi2pTUfhLsvAW4BiCbwae7u9ycZmIiIZFe6dzG9ZmYtzOwQYCEw0cweSTY0ERHJpnSrmFq6+yfAJcBEdz8JOCe5sEREJNvSTRANzOxw4FL2NFKLiMgBLN0EMZowt/S77j7HzLoCy5ILS0REsi3dRuo/AH9IWV8BfD2poEREJPvSbaTuYGbPmtlHZrbOzJ4xsw5JByciItmTbhXTRGAacATQHvhTtE1ERA5Q6SaIdu4+0d2Lo+XXQLsE4xIRkSxLN0FsMLPhZlY/WoYDRUkGJiIi2ZVugvgm4RbXD4G1wGDC8Bs5b+dOuOYaeOutbEciIlK7pJUg3H21u1/o7u3c/VB3v5jQaS7nrV4N06fDqafCn/6U7WhERGqP6swod1vGosiio46COXPguOPgoovgwQfBPdtRiYhkX3UShGUsiixr3x7+/ne49FK4/Xa46ir47LNsRyUikl3VSRAH1O/sJk1g8mS47z743e/gzDPhww+zHZWISPZUmCDMbIuZfRKzbCH0iTigmMFdd8HUqbBoEfTtC/PnZzsqEZHsqDBBuHtzd28RszR397SG6chFX/86vP56eH7aafDMM9mNR0QkG6pTxXRA690bZs+Gnj1h8OBQ9aTGaxGpS5QgKnDYYTBjBlxxBfzoRzB0KGzfnu2oRERqRqIJwswGmdlSM1tuZqNi9g8ws3lmVmxmg8vs22VmC6JlWpJxVqRxY/jNb+CBB+Dpp2HAAPjgg2xFIyJScxJLEGZWHxgPnAd0A4aaWbcyh60GrgaeirnEdnfvFS0XJhVnOszgBz+A55+Hd96Bk08OfSdERA5kSZYg+gLL3X2Fu+8ApgAXpR7g7ivdfRGwO8E4MuaCC+Df/4ZGjUJJYvLkbEckIpKcJBNEe+D9lPXCaFu6GptZgZm9YWYXxx1gZiOjYwrWr19fnVjT1qNHaLw++WS4/HL44Q9hd06kNxGRykkyQcT1tK7MfUAd3T0PuBwYa2ZH7XMx9wnunufuee3a1dzo4+3awV//CiNGwI9/DN/4Bnz6aY29vIhIjUgyQRQCR6asdwDWpHuyu6+JHlcArwG9MxlcdR10EPzylzB2LDz3HPTvHwb+S1J+PnTuDPXqhcf8/GRfT0TqtiQTxBzgGDPrYmYHAUMIs9Ltl5m1NrNG0fO2QH9gSWKRVpEZ3Hor/PnP8N57odrpX/9K5rXy82HkSFi1KvTHWLUqrCtJiEhSEksQ7l4M3AxMB94Gnnb3xWY22swuBDCzk82sEPgG8LiZLY5OPx4oMLOFwAzgfnevdQmixKBB8MYb0KJFGMPpt7/N/GvceSds27b3tm3bwnYRkSSYHyDdg/Py8rygoCCrMWzcGNojXn013Bb7k59A/fqZuXa9evE9uc3USC4iVWdmc6P23n2oJ3UGHXIIvPwy3HhjmFfi4ovhk08yc+2OHSu3XUSkupQgMqxhQxg/PiwvvQT/9V+wYkX1rztmTBiSPFWTJmG7iEgSlCAScuONYSrTNWvCsOEzZ1bvesOGwYQJ0KlTqFbq1CmsDxuWmXhFRMpSgkjQ2WfDrFnQtm14/sQT1bvesGGwcmVoc1i5UslBRJKlBJGwY44JdzidfTZ861vwne9AcXG2oxIR2T8liBrQqhW88EJIDj/7GXz1q7BpU7ajEhGpmBJEDWnQAB59NPS+fvVV6NcPli3LdlQiIuVTgqhh114bxnEqKoJTTgnPRURqIyWILBgwIIwIe8QRoRf2+PHZjkhEZF9KEFnSpUsYt+m88+Dmm8NtsTt3ZjsqEZE9lCCyqEWLMBLs7bfDY4+F0sTGjdmOSkQkUILIsvr14f77w7zXr78eOtW9/Xa2oxIRUYKoNa68El57DbZsCXc4vfRStiMSkbpOCaIWOfVUmDMHunYNfSUefTR+BFcRkZqgBFHLdOwYqpouvhhuuy30vt6xI9tRiUhdpARRCzVtCn/4A/zwh/CrX8E558D69dmOSkTqGiWIWqpePRg9GiZPDtVOJ58Mb76Z7ahEpC5RgqjlhgwJQ4Xv2BHmlpiW1qzeIiLVpwSRA04+OZQijjsutE3ccQe8/362oxKRA50SRI5o3z6UJC6/HH7609CYffrpoYOd2idEJAlKEDnk4INh0qQwCux994Ve1zfeCIcfHobs+O1vMzcHtoiIEkQOOvpouOsueOstWLgQvv99eOcduOoq+MIXYPBgeOYZ+OyzbEcqIrlMCSKHmUHPnqHKacWKMPjft74V+lEMHgyHHhqSxssvaxY7Eak8JYgDhFnoiT1uHBQWwl/+EpLE88+H6qcjjoCbbgrJY/fubEcrIrlACeIA1KABfOlL8OSTsG4dPPssnHkmTJwYGrY7d4Yf/ADmz9dQHiJSPiWIA1yjRuHW2N//PiSLSZNCtdSjj0KfPtCtG9x7L/znP9mOVERqGyWIOqR5cxg2DF54AT78EP7v/0Kj9r33wrHHQl4e/O//hiqqTMjPD6WVevXCY35+Zq4rIjVDCaKOatMGrrsuDDH+/vshMZjB974X+lgMHBgSyIYNVbt+fj6MHAmrVoVqrFWrwrqShEjuMD9AKqHz8vK8oKAg22HkvGXLYMoUeOqpcOtsSXvG0KGhqqp58/Su07lzSApldeoEK1dmMmIRqQ4zm+vuebH7lCAkjnvoYzF5ckgYq1dD48ZwwQUhWZx3XlgvT7168Q3gZrqLSqQ2qShBqIpJYplBr17wwAPw3nvh9tgRI0KV1CWXhLaLa64Jt9PG9bHo2DH+uuVtF5HaRwlC9qtePejfH37xC1izBqZPD0nij3+EL385jBN1882ho15JqWHMGGjSZO/rNGkStotIbkg0QZjZIDNbambLzWxUzP4BZjbPzIrNbHCZfVeZ2bJouSrJOCV9DRrAueeGPhXr1oUhPQYMCBMb9e8PXbrAqFHQowc8/nhoczALjxMmhLuoRCQ3JNYGYWb1gf8AXwIKgTnAUHdfknJMZ6AF8D1gmrtPjbYfAhQAeYADc4GT3P3j8l5PbRDZ9cknodf25Mmh2mnXLjj++DCfxXHHhbumUpeypQsRyY6K2iAaJPi6fYHl7r4iCmIKcBFQmiDcfWW0r2yz5ZeBV9x9Y7T/FWAQMDnBeKUaWrSAK64Iy4YNMHVqSBZ33x1/fOPG+yaN/S2tWkH9+jX7vkTqsiQTRHsgdVqbQuCUapzbPkNxScLatoXrrw9LURGsXRseK1reeis8btwYSh9xzKB168onloMPrtn3L3KgSDJBWMy2dOuz0jrXzEYCIwE66vaYWqnkSzpdu3eH6qr9JZSiotBg/uab4fmnn5Z/zYMPTj+ZdOwYBjYUkWQTRCFwZMp6B2BNJc49o8y5r5U9yN0nABMgtEFUJbEc8G0AAAuYSURBVEipXerVC1VJrVrBUUelf97nn6eXVIqKYNGiPaWVuD4Z7dtD375wyinhMS8v/Q6CIgeSJBPEHOAYM+sCfAAMAS5P89zpwE/MrHW0fi7wP5kPUQ4UjRqFX/6V+fW/ezds3rx38li2DGbPhlmzwii4EKq2unXbkzBOOSXcpdUgyf89IrVAYv/E3b3YzG4mfNnXB55098VmNhoocPdpZnYy8CzQGrjAzO519+7uvtHM7iMkGYDRJQ3WIplSr15o02jdOszSV1ZRUUgWJQnj+efDEOoQqq1OOmnvkkbJLb0iBwoNtSGSJvcwc19Jwpg9G+bNC9VbEGbwS00YffuGqjKR2ixbt7mKHFDMQrvIUUeF8agAduwIDeUlCWPWrDCceokvfjEkjJKkceKJcNBB2YlfpLJUghAhDEN+551hUMKOHcOQIFXt9b15M8yZsydhzJoVep1DSA69e+/dnnHUUaqakuzRaK4iFSiZu2Lbtj3bmjTJ3NAg7mHOjdSqqYKCPa93yCF7qqRKEkfbttV/XZF0KEGIVCAbc1cUF8OSJXtXTS1evOe2265d904YvXurw58kQwlCpAK1Ze6KrVth7ty9SxrvR+MJNGgQ5hJPrZo69tgQu1SNe/jMP/wwLGvX7nn+0UchibtXbSm5fk0tPXqEATOrQo3UIhXo2DG+BFHTnfObNQtTvQ4cuGfb2rV7J4z8fHjssbCvefPQqa9t2/SWFi3qRlvHzp3hC77sl37qUrI9tVqxRIMG0K4dNGwYPq+qLFD1c+OWevUq3p9UR06VIKTOS7oNIpN274alS0PCmDcvfMlt2LD3snNn/LkNGqSfTEqWJk1qR1Jxh02b4n/tl91W3jzqrVvDYYfB4YeHx9Qlddshh9StkpmqmET2I5N3MWWTO2zZsm/SqGgpKiq/Kq1x48onlUaN0o/3s8/CHV77+9L/8MNwS3FZjRqV/6Wfuv0LX6hcXHWJEoSIlGv37vDrvDJJ5eNyZ2YJVWVxicNs3y/+TZv2Pd8sHJ/Or/2WLWtHCSeXqQ1CRMpVr16oVjnkkNCxLx3FxWGww3SSydKl4XH37j1f7D16wDnnxH/xl9T/S/YpQYhIpTVoEIYWOfTQbEciSapDTTEiB478/NB/o1698Jifn+2I5ECkEoRIjil719WqVWEdcrNhXWovlSBEcsydd+57//62bWG7SCYpQYjkmNWrK7ddpKqUIERyTHk9vDUtu2SaEoRIjhkzJvRwTtWkSdgukklKECI5ZtiwMAxIyRSnnTrVzmFBJPfpLiaRHDRsmBKCJE8lCBERiaUEISKJUqe+3KUqJhFJjDr15TaVIEQkMerUl9uUIEQkMerUl9uUIEQkMerUl9uUIEQkMbnYqU+N6nsoQYhIYnKtU19Jo/qqVWH61pJG9bqaJDTlqIhIpHPnkBTK6tQJVq6s6WhqRkVTjqoEISISUaP63pQgREQialTfmxKEiEgk1xrVk25QV4IQEYnkUqN6TTSoq5FaRCQHZapBPWuN1GY2yMyWmtlyMxsVs7+Rmf0+2j/LzDpH2zub2XYzWxAt/5dknCIiuaYmGtQTG6zPzOoD44EvAYXAHDOb5u5LUg4bAXzs7keb2RDgAeCyaN+77t4rqfhERHJZx47xJYhMNqgnWYLoCyx39xXuvgOYAlxU5piLgN9Ez6cCZ5uZJRiTiMgBoSYa1JNMEO2B91PWC6Ntsce4ezGwGWgT7etiZvPN7O9mdnrcC5jZSDMrMLOC9evXZzZ6EZFarCYa1JOcDyKuJFC2Rby8Y9YCHd29yMxOAp4zs+7u/sleB7pPACZAaKTOQMwiIjkj6alnkyxBFAJHpqx3ANaUd4yZNQBaAhvd/XN3LwJw97nAu8AXE4xVRETKSDJBzAGOMbMuZnYQMASYVuaYacBV0fPBwKvu7mbWLmrkxsy6AscAKxKMVUREykisisndi83sZmA6UB940t0Xm9looMDdpwG/An5nZsuBjYQkAjAAGG1mxcAu4Hp335hUrCIisi91lBMRqcM0mquIiFTaAVOCMLP1QEy3kbS1BTZkKJyk5VKskFvx5lKskFvx5lKskFvxVifWTu7eLm7HAZMgqsvMCsorZtU2uRQr5Fa8uRQr5Fa8uRQr5Fa8ScWqKiYREYmlBCEiIrGUIPaYkO0AKiGXYoXcijeXYoXcijeXYoXcijeRWNUGISIisVSCEBGRWEoQIiISq84nCDN70sw+MrO3sh3L/pjZkWY2w8zeNrPFZnZrtmMqj5k1NrPZZrYwivXebMe0P2ZWPxpi/oVsx7I/ZrbSzN6MZlys9UMImFkrM5tqZu9E/35PzXZMcczs2JSZLBeY2Sdm9p1sx1URM/tu9H/sLTObbGaNM3btut4GYWYDgK3Ab929R7bjqYiZHQ4c7u7zzKw5MBe4uMwsfbVCNPFTU3ffamYNgdeBW939jSyHVi4zuw3IA1q4+1ezHU9FzGwlkOfuOdGRy8x+A/zD3Z+IBu9s4u6bsh1XRaIBQz8ATnH36nTCTYyZtSf83+rm7tvN7GngRXf/dSauX+dLEO4+kzBQYK3n7mvdfV70fAvwNvtOwlQreLA1Wm0YLbX214iZdQDOB57IdiwHGjNrQRiA81cA7r6jtieHyNmEqY9rZXJI0QA4OJoyoQn7TqtQZXU+QeQqM+sM9AZmZTeS8kVVNguAj4BX3L3WxgqMBX4A7M52IGly4C9mNtfMRmY7mP3oCqwHJkZVeE+YWdNsB5WGIcDkbAdREXf/AHgYWE2YaG2zu/8lU9dXgshBZtYMeAb4TtlZ9moTd9/l7r0Ik0X1NbNaWYVnZl8FPoomp8oV/d29D3AecFNUVVpbNQD6AI+5e2/gU2BUdkOqWFQNdiHwh2zHUhEzaw1cBHQBjgCamtnwTF1fCSLHRPX5zwD57v7HbMeTjqg64TVgUJZDKU9/4MKoXn8KcJaZTcpuSBVz9zXR40fAs0Df7EZUoUKgMKUEOZWQMGqz84B57r4u24HsxznAe+6+3t13An8E/itTF1eCyCFRw++vgLfd/ZFsx1ORaFbAVtHzgwn/kN/JblTx3P1/3L2Du3cmVCu86u4Z+xWWaWbWNLpJgaiq5lyg1t6F5+4fAu+b2bHRprOBWndjRRlDqeXVS5HVQD8zaxJ9P5xNaJvMiDqfIMxsMvBv4FgzKzSzEdmOqQL9gSsIv3BLbsP7SraDKsfhwAwzW0SYfvYVd6/1t4/miC8Ar5vZQmA28Gd3fznLMe3Pt4H86N9DL+AnWY6nXGbWBPgS4dd4rRaVyqYC84A3Cd/pGRt2o87f5ioiIvHqfAlCRETiKUGIiEgsJQgREYmlBCEiIrGUIEREJJYShMh+mNmuMiN8ZqwXsJl1zoWRhKVuapDtAERywPZoyBCROkUlCJEqiuZkeCCa92K2mR0dbe9kZn8zs0XRY8do+xfM7NlojoyFZlYyJEJ9M/tlNKb/X6Ke55jZLWa2JLrOlCy9TanDlCBE9u/gMlVMl6Xs+8Td+wK/IIwIS/T8t+7eE8gHxkXbxwF/d/cTCWMRLY62HwOMd/fuwCbg69H2UUDv6DrXJ/XmRMqjntQi+2FmW929Wcz2lcBZ7r4iGkTxQ3dvY2YbCBM77Yy2r3X3tma2Hujg7p+nXKMzYRiSY6L124GG7v5jM3uZMJnVc8BzKfNriNQIlSBEqsfLeV7eMXE+T3m+iz1tg+cD44GTgLnRhDAiNUYJQqR6Lkt5/Hf0/F+EUWEBhhGmhAT4G3ADlE6m1KK8i5pZPeBId59BmMioFbBPKUYkSfpFIrJ/B0cz45V42d1LbnVtZGazCD+2hkbbbgGeNLPvE2ZSuybafiswIRoxeBchWawt5zXrA5PMrCVgwKM5Mk2nHEDUBiFSRVEbRJ67b8h2LCJJUBWTiIjEUglCRERiqQQhIiKxlCBERCSWEoSIiMRSghARkVhKECIiEuv/A/ytXwcxrQFoAAAAAElFTkSuQmCC
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>   <span class="c1"># clear figure</span>
<span class="n">acc_values</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
<span class="c1"># val_acc_values = history_dict[&#39;val_acc&#39;]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation accuracy&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Confusion-matrix">Confusion matrix<a class="anchor-link" href="#Confusion-matrix">&#182;</a></h2><p>Let us see what the confusion matrix looks like. Using both <code>sklearn.metrics</code> and <code>tensorflow</code>. Then we visualize the confusion matrix and see what that tells us.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get the predicted classes:</span>
<span class="n">pred_classes</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict_classes</span><span class="p">(</span><span class="n">train_images</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">conf_max</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span><span class="n">pred_classes</span><span class="p">)</span>
<span class="n">conf_max</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[37]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>array([[5409,    0,    3,    3,    3,    9,    4,    1,   10,    2],
       [   1, 6123,   13,    5,   10,    0,    4,    5,   14,    4],
       [  10,   11, 5368,   24,   11,    6,    4,    6,   24,    6],
       [   2,    0,   14, 5549,    0,   21,    0,    4,   38,   10],
       [   2,    2,    3,    1, 5241,    3,   10,    3,    7,   35],
       [   5,    2,    4,   23,    3, 4905,   16,    1,   21,    7],
       [  11,    4,    2,    2,    8,    8, 5376,    0,    6,    0],
       [   2,   16,   20,   11,   13,   10,    0, 5591,   13,   39],
       [   5,    9,    8,    6,    2,   10,    4,    0, 5335,   10],
       [   9,    2,    0,   15,   27,   23,    1,   14,   11, 5352]])</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">row_sums</span> <span class="o">=</span> <span class="n">conf_max</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">norm_conf_max</span> <span class="o">=</span> <span class="n">conf_max</span> <span class="o">/</span> <span class="n">row_sums</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Looks like there were 38 threes confused as eights. You can see this in the heat map for the error below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Visualizing-the-confusion-matrix">Visualizing the confusion matrix<a class="anchor-link" href="#Visualizing-the-confusion-matrix">&#182;</a></h2><p>We use code from chapter 3 of Hands on Machine Learning (A. Geron) (cf. <a href="https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb">https://github.com/ageron/handson-ml2/blob/master/03_classification.ipynb</a>) to display a "heat map" of the confusion matrix. Then we normalize the confusion matrix so we can compare error rates.</p>
<p>See <a href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter">https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch03.html#classification_chapter</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">matrix</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;If you prefer color and a colorbar&quot;&quot;&quot;</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
    <span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cax</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[60]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">conf_max</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Classes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual Classes&quot;</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;confusion_matrix_plot_mnist&quot;, tight_layout=False)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>The confusion matrix looks better than the one in in the previous experiments, doesn' it? Let us normalize the confusion matrix to get the error rates.
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">row_sums</span> <span class="o">=</span> <span class="n">conf_max</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">norm_conf_max</span> <span class="o">=</span> <span class="n">conf_max</span> <span class="o">/</span> <span class="n">row_sums</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[61]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">norm_conf_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">norm_conf_max</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Classes&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Actual Classes&quot;</span><span class="p">)</span>
<span class="c1"># plt.savefig(&quot;confusion_matrix_errors_plot_mnist_val&quot;, tight_layout=False)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>Looks like a lot of digits were still misclassified as 8's. Let's plot examples of 3's and 8's to try to determine what went wrong here.
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[48]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">plot_digits</span><span class="p">(</span><span class="n">instances</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">images_per_row</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">28</span>
    <span class="n">images_per_row</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">instances</span><span class="p">),</span> <span class="n">images_per_row</span><span class="p">)</span>
    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">instance</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">instance</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">]</span>
    <span class="n">n_rows</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">images_per_row</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">row_images</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_empty</span> <span class="o">=</span> <span class="n">n_rows</span> <span class="o">*</span> <span class="n">images_per_row</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>
    <span class="n">images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span> <span class="o">*</span> <span class="n">n_empty</span><span class="p">)))</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rows</span><span class="p">):</span>
        <span class="n">rimages</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="n">row</span> <span class="o">*</span> <span class="n">images_per_row</span> <span class="p">:</span> <span class="p">(</span><span class="n">row</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">images_per_row</span><span class="p">]</span>
        <span class="n">row_images</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">rimages</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">row_images</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pos</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s1">&#39;binary&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">options</span><span class="p">)</span>
    <span class="n">pos</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cl_a</span><span class="p">,</span> <span class="n">cl_b</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="mi">8</span>
<span class="n">X_aa</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[(</span><span class="n">train_labels</span> <span class="o">==</span> <span class="n">cl_a</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">pred_classes</span> <span class="o">==</span> <span class="n">cl_a</span><span class="p">)]</span>
<span class="n">X_ab</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[(</span><span class="n">train_labels</span> <span class="o">==</span> <span class="n">cl_a</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">pred_classes</span> <span class="o">==</span> <span class="n">cl_b</span><span class="p">)]</span>
<span class="n">X_ba</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[(</span><span class="n">train_labels</span> <span class="o">==</span> <span class="n">cl_b</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">pred_classes</span> <span class="o">==</span> <span class="n">cl_a</span><span class="p">)]</span>
<span class="n">X_bb</span> <span class="o">=</span> <span class="n">train_images</span><span class="p">[(</span><span class="n">train_labels</span> <span class="o">==</span> <span class="n">cl_b</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">pred_classes</span> <span class="o">==</span> <span class="n">cl_b</span><span class="p">)]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>
<span class="n">p4</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>

<span class="n">plot_digits</span><span class="p">(</span><span class="n">X_aa</span><span class="p">[:</span><span class="mi">25</span><span class="p">],</span> <span class="n">p1</span><span class="p">,</span> <span class="n">images_per_row</span><span class="o">=</span><span class="mi">5</span><span class="p">);</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">X_ab</span><span class="p">[:</span><span class="mi">25</span><span class="p">],</span> <span class="n">p2</span><span class="p">,</span> <span class="n">images_per_row</span><span class="o">=</span><span class="mi">5</span><span class="p">);</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">X_ba</span><span class="p">[:</span><span class="mi">25</span><span class="p">],</span> <span class="n">p3</span><span class="p">,</span> <span class="n">images_per_row</span><span class="o">=</span><span class="mi">5</span><span class="p">);</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">X_bb</span><span class="p">[:</span><span class="mi">25</span><span class="p">],</span> <span class="n">p4</span><span class="p">,</span> <span class="n">images_per_row</span><span class="o">=</span><span class="mi">5</span><span class="p">);</span>


<span class="n">p1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{cl_a}</span><span class="s2">&#39;s classified as </span><span class="si">{cl_a}</span><span class="s2">&#39;s&quot;</span><span class="p">)</span>
<span class="n">p2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{cl_a}</span><span class="s2">&#39;s classified as </span><span class="si">{cl_b}</span><span class="s2">&#39;s&quot;</span><span class="p">)</span>
<span class="n">p3</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{cl_b}</span><span class="s2">&#39;s classified as </span><span class="si">{cl_a}</span><span class="s2">&#39;s&quot;</span><span class="p">)</span>
<span class="n">p4</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{cl_b}</span><span class="s2">&#39;s classified as </span><span class="si">{cl_b}</span><span class="s2">&#39;s&quot;</span><span class="p">)</span>

<span class="c1"># plt.savefig(&quot;error_analysis_digits_plot_EXP1_valid&quot;)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[55]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># display 50 of the 3&#39;s classified correctly</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">X_aa</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span><span class="n">plt</span><span class="p">,</span><span class="n">images_per_row</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[56]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># display all the 3&#39;s classified as 8&#39;s</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">X_ab</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">))</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">X_ab</span><span class="p">[:</span><span class="n">num</span><span class="p">],</span><span class="n">plt</span><span class="p">,</span> <span class="n">images_per_row</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[57]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># display 50 of the 8&#39;s classified correctly</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">X_bb</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span><span class="n">plt</span><span class="p">,</span> <span class="n">images_per_row</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[59]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># display all the 8&#39;s classified as 3&#39;s</span>
<span class="n">num</span> <span class="o">=</span> <span class="n">X_ba</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plot_digits</span><span class="p">(</span><span class="n">X_ba</span><span class="p">[:</span><span class="n">num</span><span class="p">],</span><span class="n">plt</span><span class="p">,</span> <span class="n">images_per_row</span> <span class="o">=</span> <span class="mi">30</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
